From 4d330b7b55402dbbc9387012776f4a56b2f7355d Mon Sep 17 00:00:00 2001
From: gxw <guxiwei-hf@loongson.cn>
Date: Mon, 30 May 2022 17:52:54 +0800
Subject: [PATCH v2] LoongArch-Migrate-patches-from-0.155.2917_git0a84d98

---
 Makefile                               |   42 +
 common/cpu.c                           |   64 +-
 common/dct.c                           |   21 +
 common/deblock.c                       |   13 +
 common/loongarch/asm.S                 |  181 ++
 common/loongarch/dct-c.c               |  724 +++++
 common/loongarch/dct.h                 |   57 +
 common/loongarch/deblock-c.c           |  908 ++++++
 common/loongarch/deblock.h             |   41 +
 common/loongarch/loongson_intrinsics.h | 1965 ++++++++++++
 common/loongarch/mc-a.S                |  136 +
 common/loongarch/mc-c.c                | 3935 ++++++++++++++++++++++++
 common/loongarch/mc.h                  |   33 +
 common/loongarch/pixel-c.c             | 3204 +++++++++++++++++++
 common/loongarch/pixel.h               |  232 ++
 common/loongarch/predict-c.c           |  586 ++++
 common/loongarch/predict.h             |   61 +
 common/loongarch/quant-c.c             |  280 ++
 common/loongarch/quant.h               |   44 +
 common/loongarch/sad-a.S               |  322 ++
 common/mc.c                            |    6 +
 common/mips/dct-c.c                    |   93 +
 common/mips/dct.h                      |    6 +
 common/mips/mc-c.c                     |    8 +-
 common/mips/pixel-c.c                  |  146 +-
 common/mips/pixel.h                    |   10 +-
 common/pixel.c                         |   38 +-
 common/predict.c                       |    8 +-
 common/quant.c                         |   15 +
 config.guess                           |    3 +
 config.sub                             |    4 +-
 configure                              |   35 +-
 tools/checkasm.c                       |   10 +-
 x264.h                                 |    4 +
 34 files changed, 13132 insertions(+), 103 deletions(-)
 create mode 100644 common/loongarch/asm.S
 create mode 100644 common/loongarch/dct-c.c
 create mode 100644 common/loongarch/dct.h
 create mode 100644 common/loongarch/deblock-c.c
 create mode 100644 common/loongarch/deblock.h
 create mode 100644 common/loongarch/loongson_intrinsics.h
 create mode 100644 common/loongarch/mc-a.S
 create mode 100644 common/loongarch/mc-c.c
 create mode 100644 common/loongarch/mc.h
 create mode 100644 common/loongarch/pixel-c.c
 create mode 100644 common/loongarch/pixel.h
 create mode 100644 common/loongarch/predict-c.c
 create mode 100644 common/loongarch/predict.h
 create mode 100644 common/loongarch/quant-c.c
 create mode 100644 common/loongarch/quant.h
 create mode 100644 common/loongarch/sad-a.S

diff --git a/Makefile b/Makefile
index efc863f..b40c1ff 100644
--- a/Makefile
+++ b/Makefile
@@ -196,6 +196,30 @@ SRCS_X += common/mips/dct-c.c \
 endif
 endif
 
+# LOONGARCH optimization
+ifeq ($(SYS_ARCH),LOONGARCH)
+ifneq ($(findstring HAVE_LASX 1, $(CONFIG)),)
+SRCASM_X = common/loongarch/mc-a.S \
+           common/loongarch/sad-a.S
+
+SRCS_X += common/loongarch/pixel-c.c \
+          common/loongarch/predict-c.c \
+          common/loongarch/quant-c.c \
+          common/loongarch/dct-c.c \
+          common/loongarch/mc-c.c \
+          common/loongarch/deblock-c.c
+
+OBJASM +=
+ifneq ($(findstring HAVE_BITDEPTH8 1, $(CONFIG)),)
+OBJASM += $(SRCASM_X:%.S=%-8.o)
+endif
+ifneq ($(findstring HAVE_BITDEPTH10 1, $(CONFIG)),)
+OBJASM += $(SRCASM_X:%.S=%-10.o)
+endif
+
+endif
+endif
+
 endif
 
 ifneq ($(HAVE_GETOPT_LONG),1)
@@ -285,6 +309,24 @@ $(OBJS) $(OBJASM) $(OBJSO) $(OBJCLI) $(OBJCHK) $(OBJCHK_8) $(OBJCHK_10) $(OBJEXA
 %-10.o: %.c
 	$(CC) $(CFLAGS) -c $< $(CC_O) -DHIGH_BIT_DEPTH=1 -DBIT_DEPTH=10
 
+common/mips/%-c.o: common/mips/%-c.c
+	$(CC) $(CFLAGS) $(MSA_CFLAGS) -c -o $@ $<
+
+common/mips/%-c-8.o: common/mips/%-c.c
+	$(CC) $(CFLAGS) $(MSA_CFLAGS) -c -o $@ $< -DHIGH_BIT_DEPTH=0 -DBIT_DEPTH=8
+
+common/mips/%-c-10.o: common/mips/%-c.c
+	$(CC) $(CFLAGS) $(MSA_CFLAGS) -c -o $@ $< -DHIGH_BIT_DEPTH=1 -DBIT_DEPTH=10
+
+common/loongarch/%-c.o: common/loongarch/%-c.c
+	$(CC) $(CFLAGS) $(LSX_CFLAGS) $(LASX_CFLAGS) -c -o $@ $<
+
+common/loongarch/%-c-8.o: common/loongarch/%-c.c
+	$(CC) $(CFLAGS) $(LSX_CFLAGS) $(LASX_CFLAGS) -c -o $@ $< -DHIGH_BIT_DEPTH=0 -DBIT_DEPTH=8
+
+common/loongarch/%-c-10.o: common/loongarch/%-c.c
+	$(CC) $(CFLAGS) $(LSX_CFLAGS) $(LASX_CFLAGS) -c -o $@ $< -DHIGH_BIT_DEPTH=1 -DBIT_DEPTH=10
+
 %.o: %.asm common/x86/x86inc.asm common/x86/x86util.asm
 	$(AS) $(ASFLAGS) -o $@ $<
 	-@ $(if $(STRIP), $(STRIP) -x $@) # delete local/anonymous symbols, so they don't show up in oprofile
diff --git a/common/cpu.c b/common/cpu.c
index 0cb7ac2..c95c337 100644
--- a/common/cpu.c
+++ b/common/cpu.c
@@ -93,6 +93,9 @@ const x264_cpu_name_t x264_cpu_names[] =
     {"NEON",            X264_CPU_NEON},
 #elif ARCH_MIPS
     {"MSA",             X264_CPU_MSA},
+#elif ARCH_LOONGARCH
+    {"LSX",             X264_CPU_LSX},
+    {"LASX",            X264_CPU_LASX},
 #endif
     {"", 0},
 };
@@ -413,7 +416,66 @@ uint32_t x264_cpu_detect( void )
 
 uint32_t x264_cpu_detect( void )
 {
-    return X264_CPU_MSA;
+    uint32_t flags = 0;
+    return flags;
+    char buf[1024];
+
+# ifdef __linux__
+    FILE* fp = fopen("/proc/cpuinfo", "r");
+    if (!fp)
+        return flags;
+
+    memset(buf, 0, sizeof(buf));
+    while (fgets(buf, sizeof(buf), fp)) {
+        if (!strncmp(buf, "cpu model", strlen("cpu model"))) {
+            /* In case of some kernel havn't add loongson extention in ASEs implemented,
+             * analysis cpu model is still needed.
+             */
+            if (strstr(buf, "Loongson-2K")) {
+                flags |= X264_CPU_MSA;
+            }
+        }
+        if (!strncmp(buf, "ASEs implemented", strlen("ASEs implemented"))) {
+            if (strstr(buf, "msa")) {
+                flags |= X264_CPU_MSA;
+            }
+            break;
+        }
+    }
+
+    fclose(fp);
+# endif
+     return flags;
+}
+
+#elif ARCH_LOONGARCH
+
+#define LOONGARCH_CFG2    0x02
+#define LOONGARCH_LSX     ( 1 << 6 )
+#define LOONGARCH_LASX    ( 1 << 7 )
+
+uint32_t x264_cpu_detect( void )
+{
+    uint32_t flags = 0;
+    uint32_t cfg = LOONGARCH_CFG2;
+    uint32_t reg;
+
+    __asm__ volatile(
+        "cpucfg %0, %1 \n\t"
+        : "+&r"(reg)
+        : "r"(cfg)
+    );
+
+#if HAVE_LSX
+    if ( reg & LOONGARCH_LSX )
+        flags |= X264_CPU_LSX;
+#endif
+#if HAVE_LASX
+    if ( reg & LOONGARCH_LASX )
+        flags |= X264_CPU_LASX;
+#endif
+
+    return flags;
 }
 
 #else
diff --git a/common/dct.c b/common/dct.c
index 3cc290e..b5cf287 100644
--- a/common/dct.c
+++ b/common/dct.c
@@ -41,6 +41,9 @@
 #if HAVE_MSA
 #   include "mips/dct.h"
 #endif
+#if ARCH_LOONGARCH
+#   include "loongarch/dct.h"
+#endif
 
 static void dct4x4dc( dctcoef d[16] )
 {
@@ -724,6 +727,24 @@ void x264_dct_init( uint32_t cpu, x264_dct_function_t *dctf )
         dctf->add16x16_idct_dc = x264_add16x16_idct_dc_msa;
         dctf->add8x8_idct8     = x264_add8x8_idct8_msa;
         dctf->add16x16_idct8   = x264_add16x16_idct8_msa;
+        dctf->sub8x8_dct8      = x264_sub8x8_dct8_msa;
+        dctf->sub16x16_dct8    = x264_sub16x16_dct8_msa;
+    }
+#endif
+
+#if HAVE_LASX
+    if( cpu&X264_CPU_LASX )
+    {
+        dctf->sub4x4_dct       = x264_sub4x4_dct_lasx;
+        dctf->sub8x8_dct       = x264_sub8x8_dct_lasx;
+        dctf->sub16x16_dct     = x264_sub16x16_dct_lasx;
+        dctf->add4x4_idct      = x264_add4x4_idct_lasx;
+        dctf->add8x8_idct      = x264_add8x8_idct_lasx;
+        dctf->add8x8_idct8     = x264_add8x8_idct8_lasx;
+        dctf->add16x16_idct    = x264_add16x16_idct_lasx;
+        dctf->sub8x8_dct8      = x264_sub8x8_dct8_lasx;
+        dctf->sub16x16_dct8    = x264_sub16x16_dct8_lasx;
+        dctf->add8x8_idct_dc   = x264_add8x8_idct_dc_lasx;
     }
 #endif
 
diff --git a/common/deblock.c b/common/deblock.c
index f1f2a37..24b47de 100644
--- a/common/deblock.c
+++ b/common/deblock.c
@@ -680,6 +680,9 @@ void x264_macroblock_deblock( x264_t *h )
 #if HAVE_MSA
 #include "mips/deblock.h"
 #endif
+#if HAVE_LASX
+#include "loongarch/deblock.h"
+#endif
 
 void x264_deblock_init( uint32_t cpu, x264_deblock_function_t *pf, int b_mbaff )
 {
@@ -816,6 +819,16 @@ void x264_deblock_init( uint32_t cpu, x264_deblock_function_t *pf, int b_mbaff )
         pf->deblock_strength = x264_deblock_strength_msa;
     }
 #endif
+
+#if HAVE_LASX
+    if( cpu&X264_CPU_LASX )
+    {
+        pf->deblock_luma[1] = x264_deblock_v_luma_lasx;
+        pf->deblock_luma[0] = x264_deblock_h_luma_lasx;
+        pf->deblock_strength = x264_deblock_strength_lasx;
+    }
+#endif
+
 #endif // !HIGH_BIT_DEPTH
 
     /* These functions are equivalent, so don't duplicate them. */
diff --git a/common/loongarch/asm.S b/common/loongarch/asm.S
new file mode 100644
index 0000000..ba98024
--- /dev/null
+++ b/common/loongarch/asm.S
@@ -0,0 +1,181 @@
+/*****************************************************************************
+ * asm.S: LoongArch utility macros
+ *****************************************************************************
+ * Copyright (C) 2015-2022 x264 project
+ * Copyright (C) 2022 Loongson Technology Corporation Limited
+ *
+ * Authors: gxw <guxiwei-hf@loongson.cn>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02111, USA.
+ *
+ * This program is also available under a commercial proprietary license.
+ * For more information, contact us at licensing@x264.com.
+ *****************************************************************************/
+#include "config.h"
+
+#define GLUE(a, b) a ## b
+#define JOIN(a, b) GLUE(a, b)
+
+#ifdef BIT_DEPTH
+#define  ASM_PREF  JOIN(JOIN(x264_, BIT_DEPTH), _)
+#else
+#define  ASM_PREF  x264_
+#endif  /* BIT_DEPTH */
+
+#define DEFAULT_ALIGN    5
+#define FENC_STRIDE      16
+#define FDEC_STRIDE      32
+
+.macro   function name, align=DEFAULT_ALIGN
+    .macro   endfunc
+    jirl     $r0, $r1, 0x0
+    .size    ASM_PREF\name, . - ASM_PREF\name
+    .purgem  endfunc
+    .endm
+.text
+.align    \align
+.globl    ASM_PREF\name
+.type     ASM_PREF\name, @function
+ASM_PREF\name:
+.endm
+
+.macro    const name, align=DEFAULT_ALIGN
+    .macro endconst
+    .size  \name, . - \name
+    .purgem endconst
+    .endm
+.section .rodata
+.align   \align
+\name:
+.endm
+
+/**********************************************
+* LoongArch register alias
+***********************************************/
+#define a0 $a0
+#define a1 $a1
+#define a2 $a2
+#define a3 $a3
+#define a4 $a4
+#define a5 $a5
+#define a6 $a6
+#define a7 $a7
+
+#define t0 $t0
+#define t1 $t1
+#define t2 $t2
+#define t3 $t3
+#define t4 $t4
+#define t5 $t5
+#define t6 $t6
+#define t7 $t7
+#define t8 $t8
+
+#define v0 $v0
+#define v1 $v1
+
+#define zero $zero
+#define sp   $sp
+
+#define vr0 $vr0
+#define vr1 $vr1
+#define vr2 $vr2
+#define vr3 $vr3
+#define vr4 $vr4
+#define vr5 $vr5
+#define vr6 $vr6
+#define vr7 $vr7
+#define vr8 $vr8
+#define vr9 $vr9
+#define vr10 $vr10
+#define vr11 $vr11
+#define vr12 $vr12
+#define vr13 $vr13
+#define vr14 $vr14
+#define vr15 $vr15
+#define vr16 $vr16
+#define vr17 $vr17
+#define vr18 $vr18
+#define vr19 $vr19
+#define vr20 $vr20
+#define vr21 $vr21
+#define vr22 $vr22
+#define vr23 $vr23
+#define vr24 $vr24
+#define vr25 $vr25
+#define vr26 $vr26
+#define vr27 $vr27
+#define vr28 $vr28
+#define vr29 $vr29
+#define vr30 $vr30
+#define vr31 $vr31
+
+#define xr0 $xr0
+#define xr1 $xr1
+#define xr2 $xr2
+#define xr3 $xr3
+#define xr4 $xr4
+#define xr5 $xr5
+#define xr6 $xr6
+#define xr7 $xr7
+#define xr8 $xr8
+#define xr9 $xr9
+#define xr10 $xr10
+#define xr11 $xr11
+#define xr12 $xr12
+#define xr13 $xr13
+#define xr14 $xr14
+#define xr15 $xr15
+#define xr16 $xr16
+#define xr17 $xr17
+#define xr18 $xr18
+#define xr19 $xr19
+#define xr20 $xr20
+#define xr21 $xr21
+#define xr22 $xr22
+#define xr23 $xr23
+#define xr24 $xr24
+#define xr25 $xr25
+#define xr26 $xr26
+#define xr27 $xr27
+#define xr28 $xr28
+#define xr29 $xr29
+#define xr30 $xr30
+#define xr31 $xr31
+
+/**********************************************
+* LSX/LASX custom macros
+***********************************************/
+/* Attention: a cannot be both input and output
+ * TODO: Use native dp2 ins instead of macro
+ */
+.macro xvdp2.h.bu a, b, c
+    xvmulwev.h.bu    \a,    \b,    \c
+    xvmaddwod.h.bu   \a,    \b,    \c
+.endm
+
+.macro LASX_LOADX_4 src, stride, stride2, stride3, out0, out1, out2, out3
+    xvld    \out0,    \src,    0
+    xvldx   \out1,    \src,    \stride
+    xvldx   \out2,    \src,    \stride2
+    xvldx   \out3,    \src,    \stride3
+.endm
+
+.macro LSX_LOADX_4 src, stride, stride2, stride3, out0, out1, out2, out3
+    vld     \out0,    \src,    0
+    vldx    \out1,    \src,    \stride
+    vldx    \out2,    \src,    \stride2
+    vldx    \out3,    \src,    \stride3
+.endm
diff --git a/common/loongarch/dct-c.c b/common/loongarch/dct-c.c
new file mode 100644
index 0000000..3863196
--- /dev/null
+++ b/common/loongarch/dct-c.c
@@ -0,0 +1,724 @@
+/*****************************************************************************
+ * dct-c.c: loongarch transform and zigzag
+ *****************************************************************************
+ * Copyright (C) 2015-2018 x264 project
+ * Copyright (C) 2020 Loongson Technology Corporation Limited
+ *
+ * Authors: zhou peng <zhoupeng@loongson.cn>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02111, USA.
+ *
+ * This program is also available under a commercial proprietary license.
+ * For more information, contact us at licensing@x264.com.
+ *****************************************************************************/
+#include "common/common.h"
+#include "loongson_intrinsics.h"
+#include "dct.h"
+
+#if !HIGH_BIT_DEPTH
+
+#define LASX_LD4x4( p_src, out0, out1, out2, out3 )     \
+{                                                       \
+    out0 = __lasx_xvld( p_src, 0 );                     \
+    out1 = __lasx_xvpermi_d( out0, 0x55 );              \
+    out2 = __lasx_xvpermi_d( out0, 0xAA );              \
+    out3 = __lasx_xvpermi_d( out0, 0xFF );              \
+}
+
+#define LASX_ITRANS_H( in0, in1, in2, in3, out0, out1, out2, out3 )         \
+{                                                                           \
+    __m256i tmp0_m, tmp1_m, tmp2_m, tmp3_m;                                 \
+                                                                            \
+    tmp0_m = __lasx_xvadd_h( in0, in2 );                                    \
+    tmp1_m = __lasx_xvsub_h( in0, in2 );                                    \
+    tmp2_m = __lasx_xvsrai_h( in1, 1 );                                     \
+    tmp2_m = __lasx_xvsub_h( tmp2_m, in3 );                                 \
+    tmp3_m = __lasx_xvsrai_h( in3, 1 );                                     \
+    tmp3_m = __lasx_xvadd_h( in1, tmp3_m );                                 \
+                                                                            \
+    LASX_BUTTERFLY_4_H( tmp0_m, tmp1_m, tmp2_m, tmp3_m,                     \
+                        out0, out1, out2, out3 );                           \
+}
+
+#define LASX_ADDBLK_ST4x4_128SV( in0, in1, in2, in3, p_dst, stride )        \
+{                                                                           \
+    uint32_t src0_m, src1_m, src2_m, src3_m;                                \
+    uint8_t *p_dst0;                                                        \
+    __m256i inp0_m, inp1_m, res0_m, res1_m;                                 \
+    __m256i dst0_m = __lasx_xvldi( 0 );                                     \
+    __m256i dst1_m = __lasx_xvldi( 0 );                                     \
+    __m256i zero_m = __lasx_xvldi( 0 );                                     \
+                                                                            \
+    DUP2_ARG2( __lasx_xvilvl_d, in1, in0, in3, in2, inp0_m, inp1_m );       \
+    src0_m = *( uint32_t* )( p_dst );                                       \
+    p_dst0 = p_dst + stride;                                                \
+    src1_m = *( uint32_t* )( p_dst0 );                                      \
+    p_dst0 += stride;                                                       \
+    src2_m = *( uint32_t* )( p_dst0 );                                      \
+    p_dst0 += stride;                                                       \
+    src3_m = *( uint32_t* )( p_dst0 );                                      \
+    dst0_m = __lasx_xvinsgr2vr_w( dst0_m, src0_m, 0 );                      \
+    dst0_m = __lasx_xvinsgr2vr_w( dst0_m, src1_m, 1 );                      \
+    dst1_m = __lasx_xvinsgr2vr_w( dst1_m, src2_m, 0 );                      \
+    dst1_m = __lasx_xvinsgr2vr_w( dst1_m, src3_m, 1 );                      \
+    DUP2_ARG2( __lasx_xvilvl_b, zero_m, dst0_m, zero_m, dst1_m, res0_m,     \
+               res1_m );                                                    \
+    res0_m = __lasx_xvadd_h( res0_m, inp0_m );                              \
+    res1_m = __lasx_xvadd_h( res1_m, inp1_m );                              \
+    DUP2_ARG1( __lasx_xvclip255_h, res0_m, res1_m, res0_m, res1_m );        \
+    DUP2_ARG2( __lasx_xvpickev_b, res0_m, res0_m, res1_m, res1_m, dst0_m,   \
+               dst1_m );                                                    \
+                                                                            \
+    __lasx_xvstelm_w( dst0_m, p_dst, 0, 0 );                                \
+    p_dst0 = p_dst + stride;                                                \
+    __lasx_xvstelm_w( dst0_m, p_dst0, 0, 1 );                               \
+    p_dst0 += stride;                                                       \
+    __lasx_xvstelm_w( dst1_m, p_dst0, 0, 0 );                               \
+    p_dst0 += stride;                                                       \
+    __lasx_xvstelm_w( dst1_m, p_dst0, 0, 1 );                               \
+}
+
+static void avc_sub4x4_dct_lasx( uint8_t *p_src, int32_t i_src_stride,
+                                 uint8_t *p_ref, int32_t i_dst_stride,
+                                 int16_t *p_dst )
+{
+    __m256i src0, src1, src2, src3;
+    __m256i ref0, ref1, ref2, ref3;
+    __m256i inp0, inp1, tmp;
+    __m256i diff0, diff1, diff2, diff3;
+    __m256i temp0, temp1, temp2, temp3;
+
+    src0 = __lasx_xvldrepl_w( p_src, 0 );
+    p_src += i_src_stride;
+    src1 = __lasx_xvldrepl_w( p_src, 0 );
+    p_src += i_src_stride;
+    src2 = __lasx_xvldrepl_w( p_src, 0 );
+    p_src += i_src_stride;
+    src3 = __lasx_xvldrepl_w( p_src, 0 );
+    src0 = __lasx_xvpackev_w( src1, src0 );
+    src1 = __lasx_xvpackev_w( src3, src2 );
+    src0 = __lasx_xvpackev_d( src1, src0 );
+
+    ref0 = __lasx_xvldrepl_w( p_ref, 0 );
+    p_ref += i_dst_stride;
+    ref1 = __lasx_xvldrepl_w( p_ref, 0 );
+    p_ref += i_dst_stride;
+    ref2 = __lasx_xvldrepl_w( p_ref, 0 );
+    p_ref += i_dst_stride;
+    ref3 = __lasx_xvldrepl_w( p_ref, 0 );
+    ref0 = __lasx_xvpackev_w( ref1, ref0 );
+    ref1 = __lasx_xvpackev_w( ref3, ref2 );
+    ref0 = __lasx_xvpackev_d( ref1, ref0 );
+
+    inp0 = __lasx_xvilvl_b( src0, ref0 );
+    inp1 = __lasx_xvilvh_b( src0, ref0 );
+    DUP2_ARG2( __lasx_xvhsubw_hu_bu, inp0, inp0, inp1, inp1, diff0, diff2 );
+    DUP2_ARG2( __lasx_xvilvh_d, diff0, diff0, diff2, diff2, diff1, diff3 );
+
+    LASX_BUTTERFLY_4_H( diff0, diff1, diff2, diff3, temp0, temp1, temp2, temp3 );
+
+    diff0 = __lasx_xvadd_h( temp0, temp1);
+    tmp = __lasx_xvslli_h( temp3, 1);
+    diff1 = __lasx_xvadd_h( tmp, temp2);
+    diff2 = __lasx_xvsub_h( temp0, temp1 );
+    tmp = __lasx_xvslli_h( temp2, 1);
+    diff3 = __lasx_xvsub_h( temp3, tmp );
+
+    LASX_TRANSPOSE4x4_H( diff0, diff1, diff2, diff3, temp0, temp1, temp2, temp3 );
+    LASX_BUTTERFLY_4_H( temp0, temp1, temp2, temp3, diff0, diff1, diff2, diff3 );
+
+    temp0 = __lasx_xvadd_h( diff0, diff1);
+    tmp = __lasx_xvslli_h( diff3, 1);
+    temp1 = __lasx_xvadd_h( tmp, diff2);
+    temp2 = __lasx_xvsub_h( diff0, diff1 );
+    tmp = __lasx_xvslli_h( diff2, 1);
+    temp3 = __lasx_xvsub_h( diff3, tmp );
+
+    DUP2_ARG2( __lasx_xvilvl_d, temp1, temp0, temp3, temp2, inp0, inp1 );
+    inp0 = __lasx_xvpermi_q(inp1, inp0, 0x20);
+    __lasx_xvst( inp0, p_dst, 0 );
+}
+
+void x264_sub4x4_dct_lasx( int16_t p_dst[16], uint8_t *p_src,
+                           uint8_t *p_ref )
+{
+    avc_sub4x4_dct_lasx( p_src, FENC_STRIDE, p_ref, FDEC_STRIDE, p_dst );
+}
+
+void x264_sub8x8_dct_lasx( int16_t p_dst[4][16], uint8_t *p_src,
+                           uint8_t *p_ref )
+{
+    avc_sub4x4_dct_lasx( &p_src[0], FENC_STRIDE,
+                         &p_ref[0], FDEC_STRIDE, p_dst[0] );
+    avc_sub4x4_dct_lasx( &p_src[4], FENC_STRIDE, &p_ref[4],
+                         FDEC_STRIDE, p_dst[1] );
+    avc_sub4x4_dct_lasx( &p_src[4 * FENC_STRIDE + 0],
+                         FENC_STRIDE, &p_ref[4 * FDEC_STRIDE + 0],
+                         FDEC_STRIDE, p_dst[2] );
+    avc_sub4x4_dct_lasx( &p_src[4 * FENC_STRIDE + 4],
+                         FENC_STRIDE, &p_ref[4 * FDEC_STRIDE + 4],
+                         FDEC_STRIDE, p_dst[3] );
+}
+
+void x264_sub16x16_dct_lasx( int16_t p_dst[16][16],
+                             uint8_t *p_src,
+                             uint8_t *p_ref )
+{
+    x264_sub8x8_dct_lasx( &p_dst[ 0], &p_src[0], &p_ref[0] );
+    x264_sub8x8_dct_lasx( &p_dst[ 4], &p_src[8], &p_ref[8] );
+    x264_sub8x8_dct_lasx( &p_dst[ 8], &p_src[8 * FENC_STRIDE + 0],
+                          &p_ref[8*FDEC_STRIDE+0] );
+    x264_sub8x8_dct_lasx( &p_dst[12], &p_src[8 * FENC_STRIDE + 8],
+                          &p_ref[8*FDEC_STRIDE+8] );
+}
+
+static void avc_idct4x4_addblk_lasx( uint8_t *p_dst, int16_t *p_src,
+                                     int32_t i_dst_stride )
+{
+    __m256i src0, src1, src2, src3;
+    __m256i hres0, hres1, hres2, hres3;
+    __m256i vres0, vres1, vres2, vres3;
+
+    LASX_LD4x4( p_src, src0, src1, src2, src3 );
+    LASX_ITRANS_H( src0, src1, src2, src3, hres0, hres1, hres2, hres3 );
+    LASX_TRANSPOSE4x4_H( hres0, hres1, hres2, hres3, hres0, hres1, hres2, hres3 );
+    LASX_ITRANS_H( hres0, hres1, hres2, hres3, vres0, vres1, vres2, vres3 );
+
+    DUP4_ARG2( __lasx_xvsrari_h, vres0, 6, vres1, 6, vres2, 6, vres3, 6,
+               vres0, vres1, vres2, vres3 );
+    LASX_ADDBLK_ST4x4_128SV( vres0, vres1, vres2, vres3, p_dst, i_dst_stride );
+}
+
+void x264_add4x4_idct_lasx( uint8_t *p_dst, int16_t pi_dct[16] )
+{
+    avc_idct4x4_addblk_lasx( p_dst, pi_dct, FDEC_STRIDE );
+}
+
+void x264_add8x8_idct8_lasx( uint8_t *dst, int16_t dct[64] )
+{
+    int32_t stride2, stride3, stride4;
+    uint8_t* dst_tmp;
+    __m256i src0, src1, src2, src3, src4, src5, src6, src7;
+    __m256i tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7;
+    __m256i reg0, reg1, reg2, reg3, reg4, reg5, reg6, reg7;
+    __m256i shift = {0x0000000400000000, 0x0000000500000001,
+                     0x0000000600000002, 0x0000000700000003};
+
+    dct[0] += 32;
+    stride2 = FDEC_STRIDE << 1;
+    stride3 = FDEC_STRIDE + stride2;
+    stride4 = stride2 << 1;
+    dst_tmp = dst + stride4;
+
+    src0 = __lasx_xvld(dct, 0);
+    src2 = __lasx_xvld(dct, 32);
+    src4 = __lasx_xvld(dct, 64);
+    src6 = __lasx_xvld(dct, 96);
+    src1 = __lasx_xvpermi_d(src0, 0x4E);
+    src3 = __lasx_xvpermi_d(src2, 0x4E);
+    src5 = __lasx_xvpermi_d(src4, 0x4E);
+    src7 = __lasx_xvpermi_d(src6, 0x4E);
+
+    src0 = __lasx_vext2xv_w_h(src0);
+    src1 = __lasx_vext2xv_w_h(src1);
+    src2 = __lasx_vext2xv_w_h(src2);
+    src3 = __lasx_vext2xv_w_h(src3);
+    src4 = __lasx_vext2xv_w_h(src4);
+    src5 = __lasx_vext2xv_w_h(src5);
+    src6 = __lasx_vext2xv_w_h(src6);
+    src7 = __lasx_vext2xv_w_h(src7);
+
+    tmp0 = __lasx_xvadd_w(src0, src4);
+    tmp2 = __lasx_xvsub_w(src0, src4);
+    tmp4 = __lasx_xvsrai_w(src2, 1);
+    tmp4 = __lasx_xvsub_w(tmp4, src6);
+    tmp6 = __lasx_xvsrai_w(src6, 1);
+    tmp6 = __lasx_xvadd_w(tmp6, src2);
+    reg7 = __lasx_xvsrai_w(src7, 1);
+    reg3 = __lasx_xvsrai_w(src3, 1);
+    reg5 = __lasx_xvsrai_w(src5, 1);
+    reg1 = __lasx_xvsrai_w(src1, 1);
+    tmp1 = __lasx_xvsub_w(src5, src3);
+    tmp3 = __lasx_xvadd_w(src1, src7);
+    tmp5 = __lasx_xvsub_w(src7, src1);
+    tmp7 = __lasx_xvadd_w(src3, src5);
+    reg7 = __lasx_xvadd_w(src7, reg7);
+    reg3 = __lasx_xvadd_w(src3, reg3);
+    reg5 = __lasx_xvadd_w(reg5, src5);
+    reg1 = __lasx_xvadd_w(reg1, src1);
+    tmp1 = __lasx_xvsub_w(tmp1, reg7);
+    tmp3 = __lasx_xvsub_w(tmp3, reg3);
+    tmp5 = __lasx_xvadd_w(reg5, tmp5);
+    tmp7 = __lasx_xvadd_w(tmp7, reg1);
+    reg0 = __lasx_xvadd_w(tmp0, tmp6);
+    reg2 = __lasx_xvadd_w(tmp2, tmp4);
+    reg4 = __lasx_xvsub_w(tmp2, tmp4);
+    reg6 = __lasx_xvsub_w(tmp0, tmp6);
+    reg1 = __lasx_xvsrai_w(tmp7, 2);
+    reg3 = __lasx_xvsrai_w(tmp5, 2);
+    reg5 = __lasx_xvsrai_w(tmp3, 2);
+    reg7 = __lasx_xvsrai_w(tmp1, 2);
+    reg1 = __lasx_xvadd_w(tmp1, reg1);
+    reg3 = __lasx_xvadd_w(tmp3, reg3);
+    reg5 = __lasx_xvsub_w(reg5, tmp5);
+    reg7 = __lasx_xvsub_w(tmp7, reg7);
+
+    src0 = __lasx_xvadd_w(reg0, reg7);
+    src1 = __lasx_xvadd_w(reg2, reg5);
+    src2 = __lasx_xvadd_w(reg4, reg3);
+    src3 = __lasx_xvadd_w(reg6, reg1);
+    src4 = __lasx_xvsub_w(reg6, reg1);
+    src5 = __lasx_xvsub_w(reg4, reg3);
+    src6 = __lasx_xvsub_w(reg2, reg5);
+    src7 = __lasx_xvsub_w(reg0, reg7);
+
+    LASX_TRANSPOSE8x8_W(src0, src1, src2, src3, src4, src5, src6, src7,
+                        src0, src1, src2, src3, src4, src5, src6, src7);
+
+    tmp0 = __lasx_xvaddwev_w_h(src0, src4);
+    tmp2 = __lasx_xvsubwev_w_h(src0, src4);
+    tmp4 = __lasx_xvsrai_h(src2, 1);
+    tmp4 = __lasx_xvsubwev_w_h(tmp4, src6);
+    tmp6 = __lasx_xvsrai_h(src6, 1);
+    tmp6 = __lasx_xvaddwev_w_h(tmp6, src2);
+    reg7 = __lasx_xvsrai_h(src7, 1);
+    reg3 = __lasx_xvsrai_h(src3, 1);
+    reg5 = __lasx_xvsrai_h(src5, 1);
+    reg1 = __lasx_xvsrai_h(src1, 1);
+    tmp1 = __lasx_xvsubwev_w_h(src5, src3);
+    tmp3 = __lasx_xvaddwev_w_h(src1, src7);
+    tmp5 = __lasx_xvsubwev_w_h(src7, src1);
+    tmp7 = __lasx_xvaddwev_w_h(src3, src5);
+    reg7 = __lasx_xvaddwev_w_h(src7, reg7);
+    reg3 = __lasx_xvaddwev_w_h(src3, reg3);
+    reg5 = __lasx_xvaddwev_w_h(reg5, src5);
+    reg1 = __lasx_xvaddwev_w_h(reg1, src1);
+
+    tmp1 = __lasx_xvsub_w(tmp1, reg7);
+    tmp3 = __lasx_xvsub_w(tmp3, reg3);
+    tmp5 = __lasx_xvadd_w(reg5, tmp5);
+    tmp7 = __lasx_xvadd_w(tmp7, reg1);
+    reg0 = __lasx_xvadd_w(tmp0, tmp6);
+    reg2 = __lasx_xvadd_w(tmp2, tmp4);
+    reg4 = __lasx_xvsub_w(tmp2, tmp4);
+    reg6 = __lasx_xvsub_w(tmp0, tmp6);
+    reg1 = __lasx_xvsrai_w(tmp7, 2);
+    reg3 = __lasx_xvsrai_w(tmp5, 2);
+    reg5 = __lasx_xvsrai_w(tmp3, 2);
+    reg7 = __lasx_xvsrai_w(tmp1, 2);
+    reg1 = __lasx_xvadd_w(tmp1, reg1);
+    reg3 = __lasx_xvadd_w(tmp3, reg3);
+    reg5 = __lasx_xvsub_w(reg5, tmp5);
+    reg7 = __lasx_xvsub_w(tmp7, reg7);
+    src0 = __lasx_xvadd_w(reg0, reg7);
+    src1 = __lasx_xvadd_w(reg2, reg5);
+    src2 = __lasx_xvadd_w(reg4, reg3);
+    src3 = __lasx_xvadd_w(reg6, reg1);
+    src4 = __lasx_xvsub_w(reg6, reg1);
+    src5 = __lasx_xvsub_w(reg4, reg3);
+    src6 = __lasx_xvsub_w(reg2, reg5);
+    src7 = __lasx_xvsub_w(reg0, reg7);
+
+    src0 = __lasx_xvsrai_w(src0, 6);
+    src1 = __lasx_xvsrai_w(src1, 6);
+    src2 = __lasx_xvsrai_w(src2, 6);
+    src3 = __lasx_xvsrai_w(src3, 6);
+    src4 = __lasx_xvsrai_w(src4, 6);
+    src5 = __lasx_xvsrai_w(src5, 6);
+    src6 = __lasx_xvsrai_w(src6, 6);
+    src7 = __lasx_xvsrai_w(src7, 6);
+
+    reg0 = __lasx_xvld(dst, 0);
+    reg1 = __lasx_xvld(dst, FDEC_STRIDE);
+    reg2 = __lasx_xvldx(dst, stride2);
+    reg3 = __lasx_xvldx(dst, stride3);
+    reg4 = __lasx_xvld(dst_tmp, 0);
+    reg5 = __lasx_xvld(dst_tmp, FDEC_STRIDE);
+    reg6 = __lasx_xvldx(dst_tmp, stride2);
+    reg7 = __lasx_xvldx(dst_tmp, stride3);
+
+    reg0 = __lasx_vext2xv_wu_bu(reg0);
+    reg1 = __lasx_vext2xv_wu_bu(reg1);
+    reg2 = __lasx_vext2xv_wu_bu(reg2);
+    reg3 = __lasx_vext2xv_wu_bu(reg3);
+    reg4 = __lasx_vext2xv_wu_bu(reg4);
+    reg5 = __lasx_vext2xv_wu_bu(reg5);
+    reg6 = __lasx_vext2xv_wu_bu(reg6);
+    reg7 = __lasx_vext2xv_wu_bu(reg7);
+    reg0 = __lasx_xvadd_w(reg0, src0);
+    reg1 = __lasx_xvadd_w(reg1, src1);
+    reg2 = __lasx_xvadd_w(reg2, src2);
+    reg3 = __lasx_xvadd_w(reg3, src3);
+    reg4 = __lasx_xvadd_w(reg4, src4);
+    reg5 = __lasx_xvadd_w(reg5, src5);
+    reg6 = __lasx_xvadd_w(reg6, src6);
+    reg7 = __lasx_xvadd_w(reg7, src7);
+
+    reg0 = __lasx_xvmaxi_w(reg0, 0);
+    reg1 = __lasx_xvmaxi_w(reg1, 0);
+    reg2 = __lasx_xvmaxi_w(reg2, 0);
+    reg3 = __lasx_xvmaxi_w(reg3, 0);
+    reg4 = __lasx_xvmaxi_w(reg4, 0);
+    reg5 = __lasx_xvmaxi_w(reg5, 0);
+    reg6 = __lasx_xvmaxi_w(reg6, 0);
+    reg7 = __lasx_xvmaxi_w(reg7, 0);
+    src0 = __lasx_xvssrlni_hu_w(reg1, reg0, 0);
+    src1 = __lasx_xvssrlni_hu_w(reg3, reg2, 0);
+    src2 = __lasx_xvssrlni_hu_w(reg5, reg4, 0);
+    src3 = __lasx_xvssrlni_hu_w(reg7, reg6, 0);
+    src0 = __lasx_xvssrlni_bu_h(src1, src0, 0);
+    src1 = __lasx_xvssrlni_bu_h(src3, src2, 0);
+    src0 = __lasx_xvperm_w(src0, shift);
+    src1 = __lasx_xvperm_w(src1, shift);
+    __lasx_xvstelm_d(src0, dst, 0, 0);
+    dst += FDEC_STRIDE;
+    __lasx_xvstelm_d(src0, dst, 0, 1);
+    dst += FDEC_STRIDE;
+    __lasx_xvstelm_d(src0, dst, 0, 2);
+    dst += FDEC_STRIDE;
+    __lasx_xvstelm_d(src0, dst, 0, 3);
+    dst += FDEC_STRIDE;
+    __lasx_xvstelm_d(src1, dst, 0, 0);
+    dst += FDEC_STRIDE;
+    __lasx_xvstelm_d(src1, dst, 0, 1);
+    dst += FDEC_STRIDE;
+    __lasx_xvstelm_d(src1, dst, 0, 2);
+    dst += FDEC_STRIDE;
+    __lasx_xvstelm_d(src1, dst, 0, 3);
+}
+
+void x264_add8x8_idct_lasx( uint8_t *p_dst, int16_t pi_dct[4][16] )
+{
+    avc_idct4x4_addblk_lasx( &p_dst[0], &pi_dct[0][0], FDEC_STRIDE );
+    avc_idct4x4_addblk_lasx( &p_dst[4], &pi_dct[1][0], FDEC_STRIDE );
+    avc_idct4x4_addblk_lasx( &p_dst[4 * FDEC_STRIDE + 0],
+                            &pi_dct[2][0], FDEC_STRIDE );
+    avc_idct4x4_addblk_lasx( &p_dst[4 * FDEC_STRIDE + 4],
+                            &pi_dct[3][0], FDEC_STRIDE );
+}
+
+void x264_add16x16_idct_lasx( uint8_t *p_dst, int16_t pi_dct[16][16] )
+{
+    x264_add8x8_idct_lasx( &p_dst[0], &pi_dct[0] );
+    x264_add8x8_idct_lasx( &p_dst[8], &pi_dct[4] );
+    x264_add8x8_idct_lasx( &p_dst[8 * FDEC_STRIDE + 0], &pi_dct[8] );
+    x264_add8x8_idct_lasx( &p_dst[8 * FDEC_STRIDE + 8], &pi_dct[12] );
+}
+
+void x264_add8x8_idct_dc_lasx( uint8_t *pdst, int16_t dct[4] )
+{
+    int32_t stride2 = FDEC_STRIDE << 1;
+    int32_t stride3 = FDEC_STRIDE + stride2;
+    int32_t stride4 = stride2 << 1;
+    uint8_t *pdst_tmp = pdst + stride4;
+    __m256i vec_dct, vec_dct0, vec_dct1;
+    __m256i src0, src1, src2, src3, src4, src5, src6, src7;
+    __m256i const_32 = __lasx_xvldi(0x420);
+
+    vec_dct = __lasx_xvldrepl_d(dct, 0);
+    vec_dct = __lasx_xvadd_h(vec_dct, const_32);
+    vec_dct = __lasx_xvsrai_h(vec_dct, 6);
+    vec_dct = __lasx_xvilvl_h(vec_dct, vec_dct);
+    vec_dct0 = __lasx_xvilvl_w(vec_dct, vec_dct);
+    vec_dct1 = __lasx_xvilvh_w(vec_dct, vec_dct);
+
+    src0 = __lasx_xvld(pdst, 0);
+    src1 = __lasx_xvld(pdst, FDEC_STRIDE);
+    src2 = __lasx_xvldx(pdst, stride2);
+    src3 = __lasx_xvldx(pdst, stride3);
+    src4 = __lasx_xvld(pdst_tmp, 0);
+    src5 = __lasx_xvld(pdst_tmp, FDEC_STRIDE);
+    src6 = __lasx_xvldx(pdst_tmp, stride2);
+    src7 = __lasx_xvldx(pdst_tmp, stride3);
+
+    src0 = __lasx_xvilvl_d(src1, src0);
+    src1 = __lasx_xvilvl_d(src3, src2);
+    src2 = __lasx_xvilvl_d(src5, src4);
+    src3 = __lasx_xvilvl_d(src7, src6);
+
+    src0 = __lasx_vext2xv_hu_bu(src0);
+    src1 = __lasx_vext2xv_hu_bu(src1);
+    src2 = __lasx_vext2xv_hu_bu(src2);
+    src3 = __lasx_vext2xv_hu_bu(src3);
+
+    src0 = __lasx_xvadd_h(src0, vec_dct0);
+    src1 = __lasx_xvadd_h(src1, vec_dct0);
+    src2 = __lasx_xvadd_h(src2, vec_dct1);
+    src3 = __lasx_xvadd_h(src3, vec_dct1);
+
+    src0 = __lasx_xvmaxi_h(src0, 0);
+    src1 = __lasx_xvmaxi_h(src1, 0);
+    src2 = __lasx_xvmaxi_h(src2, 0);
+    src3 = __lasx_xvmaxi_h(src3, 0);
+    src0 = __lasx_xvssrlni_bu_h(src1, src0, 0);
+    src1 = __lasx_xvssrlni_bu_h(src3, src2, 0);
+    __lasx_xvstelm_d(src0, pdst, 0, 0);
+    pdst += FDEC_STRIDE;
+    __lasx_xvstelm_d(src0, pdst, 0, 2);
+    pdst += FDEC_STRIDE;
+    __lasx_xvstelm_d(src0, pdst, 0, 1);
+    pdst += FDEC_STRIDE;
+    __lasx_xvstelm_d(src0, pdst, 0, 3);
+    __lasx_xvstelm_d(src1, pdst_tmp, 0, 0);
+    pdst_tmp += FDEC_STRIDE;
+    __lasx_xvstelm_d(src1, pdst_tmp, 0, 2);
+    pdst_tmp += FDEC_STRIDE;
+    __lasx_xvstelm_d(src1, pdst_tmp, 0, 1);
+    pdst_tmp += FDEC_STRIDE;
+    __lasx_xvstelm_d(src1, pdst_tmp, 0, 3);
+}
+
+/****************************************************************************
+ * 8x8 transform:
+ ****************************************************************************/
+
+void x264_sub8x8_dct8_lasx( int16_t pi_dct[64], uint8_t *p_pix1,
+                            uint8_t *p_pix2 )
+{
+    __m256i src0, src1, src2, src3;
+    __m256i tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7;
+    __m256i temp, temp1;
+    __m256i zero = {0};
+    __m256i s07, s16, s25, s34, d07, d16, d25, d34;
+    __m256i a0, a1, a2, a3, a4, a5, a6, a7;
+
+#define LOAD_PIX_DATA_2(data1, data2)                                     \
+    DUP2_ARG2( __lasx_xvld, p_pix1, 0, p_pix1, FENC_STRIDE, src0, src1 ); \
+    DUP2_ARG2( __lasx_xvld, p_pix2, 0, p_pix2, FDEC_STRIDE, src2, src3 ); \
+    DUP4_ARG2( __lasx_xvilvl_b, zero, src0, zero, src1, zero, src2, zero, \
+               src3, src0, src1, src2, src3 );                            \
+    data1 = __lasx_xvsub_h( src0, src2 );                                 \
+    data2 = __lasx_xvsub_h( src1, src3 );                                 \
+    p_pix1 += ( FENC_STRIDE << 1 );                                       \
+    p_pix2 += ( FDEC_STRIDE << 1 );
+
+    LOAD_PIX_DATA_2(tmp0, tmp1);
+    LOAD_PIX_DATA_2(tmp2, tmp3);
+    LOAD_PIX_DATA_2(tmp4, tmp5);
+    LOAD_PIX_DATA_2(tmp6, tmp7);
+
+#undef LOAD_PIX_DATA_2
+
+#define LASX_DCT8_1D                      \
+    s07 = __lasx_xvadd_h( tmp0, tmp7 );   \
+    s16 = __lasx_xvadd_h( tmp1, tmp6 );   \
+    s25 = __lasx_xvadd_h( tmp2, tmp5 );   \
+    s34 = __lasx_xvadd_h( tmp3, tmp4 );   \
+    a0 = __lasx_xvadd_h( s07, s34 );      \
+    a1 = __lasx_xvadd_h( s16, s25 );      \
+    a2 = __lasx_xvsub_h( s07, s34 );      \
+    a3 = __lasx_xvsub_h( s16, s25 );      \
+                                          \
+    d07 = __lasx_xvsub_h( tmp0, tmp7 );   \
+    d16 = __lasx_xvsub_h( tmp1, tmp6 );   \
+    d25 = __lasx_xvsub_h( tmp2, tmp5 );   \
+    d34 = __lasx_xvsub_h( tmp3, tmp4 );   \
+                                          \
+    temp = __lasx_xvsrai_h( d07, 1 );     \
+    temp = __lasx_xvadd_h( temp, d16 );   \
+    temp = __lasx_xvadd_h( temp, d25 );   \
+    a4 = __lasx_xvadd_h( temp, d07 );     \
+                                          \
+    temp = __lasx_xvsrai_h( d25, 1 );     \
+    temp1 = __lasx_xvsub_h( d07, d34 );   \
+    temp1 = __lasx_xvsub_h( temp1, d25 ); \
+    a5 = __lasx_xvsub_h( temp1, temp );   \
+                                          \
+    temp = __lasx_xvsrai_h( d16, 1 );     \
+    temp1 = __lasx_xvadd_h( d07, d34 );   \
+    temp1 = __lasx_xvsub_h( temp1, d16 ); \
+    a6 = __lasx_xvsub_h( temp1, temp );   \
+                                          \
+    temp = __lasx_xvsrai_h( d34, 1 );     \
+    temp1 = __lasx_xvsub_h( d16, d25 );   \
+    temp1 = __lasx_xvadd_h( temp1, d34 ); \
+    a7 = __lasx_xvadd_h( temp1, temp );   \
+                                          \
+    tmp0 = __lasx_xvadd_h( a0, a1 );      \
+    temp = __lasx_xvsrai_h( a7, 2 );      \
+    tmp1 = __lasx_xvadd_h( a4, temp );    \
+    temp = __lasx_xvsrai_h( a3, 1 );      \
+    tmp2 = __lasx_xvadd_h( a2, temp );    \
+    temp = __lasx_xvsrai_h( a6, 2 );      \
+    tmp3 = __lasx_xvadd_h( a5, temp );    \
+    tmp4 = __lasx_xvsub_h( a0, a1 );      \
+    temp = __lasx_xvsrai_h( a5, 2 );      \
+    tmp5 = __lasx_xvsub_h( a6, temp );    \
+    temp = __lasx_xvsrai_h( a2, 1 );      \
+    tmp6 = __lasx_xvsub_h( temp, a3 );    \
+    temp = __lasx_xvsrai_h( a4, 2 );      \
+    tmp7 = __lasx_xvsub_h( temp, a7 );
+
+    LASX_DCT8_1D;
+    LASX_TRANSPOSE8x8_H( tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7,
+                         tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7);
+    LASX_DCT8_1D;
+
+#undef LASX_DCT8_1D
+
+    __lasx_xvstelm_d( tmp0, &pi_dct[0], 0, 0 );
+    __lasx_xvstelm_d( tmp0, &pi_dct[4], 0, 1 );
+    __lasx_xvstelm_d( tmp1, &pi_dct[8], 0, 0 );
+    __lasx_xvstelm_d( tmp1, &pi_dct[12], 0, 1 );
+    __lasx_xvstelm_d( tmp2, &pi_dct[16], 0, 0 );
+    __lasx_xvstelm_d( tmp2, &pi_dct[20], 0, 1 );
+    __lasx_xvstelm_d( tmp3, &pi_dct[24], 0, 0 );
+    __lasx_xvstelm_d( tmp3, &pi_dct[28], 0, 1 );
+    __lasx_xvstelm_d( tmp4, &pi_dct[32], 0, 0 );
+    __lasx_xvstelm_d( tmp4, &pi_dct[36], 0, 1 );
+    __lasx_xvstelm_d( tmp5, &pi_dct[40], 0, 0 );
+    __lasx_xvstelm_d( tmp5, &pi_dct[44], 0, 1 );
+    __lasx_xvstelm_d( tmp6, &pi_dct[48], 0, 0 );
+    __lasx_xvstelm_d( tmp6, &pi_dct[52], 0, 1 );
+    __lasx_xvstelm_d( tmp7, &pi_dct[56], 0, 0 );
+    __lasx_xvstelm_d( tmp7, &pi_dct[60], 0, 1 );
+}
+
+static void x264_sub8x8_dct8_ext_lasx( int16_t pi_dct1[64],
+                                       uint8_t *p_pix1, uint8_t *p_pix2,
+                                       int16_t pi_dct2[64] )
+{
+    __m256i src0, src1, src2, src3;
+    __m256i tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7;
+    __m256i temp, temp1;
+    __m256i zero = {0};
+    __m256i s07, s16, s25, s34, d07, d16, d25, d34;
+    __m256i a0, a1, a2, a3, a4, a5, a6, a7;
+
+#define LOAD_PIX_DATA_2_EXT(data1, data2)                                 \
+    DUP2_ARG2( __lasx_xvld, p_pix1, 0, p_pix1, FENC_STRIDE, src0, src1 ); \
+    DUP2_ARG2( __lasx_xvld, p_pix2, 0, p_pix2, FDEC_STRIDE, src2, src3 ); \
+    src0 = __lasx_xvpermi_d( src0, 0x50 );                                \
+    src1 = __lasx_xvpermi_d( src1, 0x50 );                                \
+    src2 = __lasx_xvpermi_d( src2, 0x50 );                                \
+    src3 = __lasx_xvpermi_d( src3, 0x50 );                                \
+                                                                          \
+    DUP4_ARG2( __lasx_xvilvl_b, zero, src0, zero, src1, zero, src2, zero, \
+               src3, src0, src1, src2, src3 );                            \
+    data1 = __lasx_xvsub_h( src0, src2 );                                 \
+    data2 = __lasx_xvsub_h( src1, src3 );                                 \
+    p_pix1 += ( FENC_STRIDE << 1 );                                       \
+    p_pix2 += ( FDEC_STRIDE << 1 );
+
+    LOAD_PIX_DATA_2_EXT(tmp0, tmp1);
+    LOAD_PIX_DATA_2_EXT(tmp2, tmp3);
+    LOAD_PIX_DATA_2_EXT(tmp4, tmp5);
+    LOAD_PIX_DATA_2_EXT(tmp6, tmp7);
+
+#undef LOAD_PIX_DATA_2_EXT
+
+#define LASX_DCT8_1D_EXT                  \
+    s07 = __lasx_xvadd_h( tmp0, tmp7 );   \
+    s16 = __lasx_xvadd_h( tmp1, tmp6 );   \
+    s25 = __lasx_xvadd_h( tmp2, tmp5 );   \
+    s34 = __lasx_xvadd_h( tmp3, tmp4 );   \
+    a0 = __lasx_xvadd_h( s07, s34 );      \
+    a1 = __lasx_xvadd_h( s16, s25 );      \
+    a2 = __lasx_xvsub_h( s07, s34 );      \
+    a3 = __lasx_xvsub_h( s16, s25 );      \
+                                          \
+    d07 = __lasx_xvsub_h( tmp0, tmp7 );   \
+    d16 = __lasx_xvsub_h( tmp1, tmp6 );   \
+    d25 = __lasx_xvsub_h( tmp2, tmp5 );   \
+    d34 = __lasx_xvsub_h( tmp3, tmp4 );   \
+                                          \
+    temp = __lasx_xvsrai_h( d07, 1 );     \
+    temp = __lasx_xvadd_h( temp, d16 );   \
+    temp = __lasx_xvadd_h( temp, d25 );   \
+    a4 = __lasx_xvadd_h( temp, d07 );     \
+                                          \
+    temp = __lasx_xvsrai_h( d25, 1 );     \
+    temp1 = __lasx_xvsub_h( d07, d34 );   \
+    temp1 = __lasx_xvsub_h( temp1, d25 ); \
+    a5 = __lasx_xvsub_h( temp1, temp );   \
+                                          \
+    temp = __lasx_xvsrai_h( d16, 1 );     \
+    temp1 = __lasx_xvadd_h( d07, d34 );   \
+    temp1 = __lasx_xvsub_h( temp1, d16 ); \
+    a6 = __lasx_xvsub_h( temp1, temp );   \
+                                          \
+    temp = __lasx_xvsrai_h( d34, 1 );     \
+    temp1 = __lasx_xvsub_h( d16, d25 );   \
+    temp1 = __lasx_xvadd_h( temp1, d34 ); \
+    a7 = __lasx_xvadd_h( temp1, temp );   \
+                                          \
+    tmp0 = __lasx_xvadd_h( a0, a1 );      \
+    temp = __lasx_xvsrai_h( a7, 2 );      \
+    tmp1 = __lasx_xvadd_h( a4, temp );    \
+    temp = __lasx_xvsrai_h( a3, 1 );      \
+    tmp2 = __lasx_xvadd_h( a2, temp );    \
+    temp = __lasx_xvsrai_h( a6, 2 );      \
+    tmp3 = __lasx_xvadd_h( a5, temp );    \
+    tmp4 = __lasx_xvsub_h( a0, a1 );      \
+    temp = __lasx_xvsrai_h( a5, 2 );      \
+    tmp5 = __lasx_xvsub_h( a6, temp );    \
+    temp = __lasx_xvsrai_h( a2, 1 );      \
+    tmp6 = __lasx_xvsub_h( temp, a3 );    \
+    temp = __lasx_xvsrai_h( a4, 2 );      \
+    tmp7 = __lasx_xvsub_h( temp, a7 );
+
+    LASX_DCT8_1D_EXT;
+    LASX_TRANSPOSE8x8_H( tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7,
+                         tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7);
+    LASX_DCT8_1D_EXT;
+
+#undef LASX_DCT8_1D_EXT
+
+    __lasx_xvstelm_d( tmp0, &pi_dct1[0], 0, 0 );
+    __lasx_xvstelm_d( tmp0, &pi_dct1[4], 0, 1 );
+    __lasx_xvstelm_d( tmp1, &pi_dct1[8], 0, 0 );
+    __lasx_xvstelm_d( tmp1, &pi_dct1[12], 0, 1 );
+    __lasx_xvstelm_d( tmp2, &pi_dct1[16], 0, 0 );
+    __lasx_xvstelm_d( tmp2, &pi_dct1[20], 0, 1 );
+    __lasx_xvstelm_d( tmp3, &pi_dct1[24], 0, 0 );
+    __lasx_xvstelm_d( tmp3, &pi_dct1[28], 0, 1 );
+    __lasx_xvstelm_d( tmp4, &pi_dct1[32], 0, 0 );
+    __lasx_xvstelm_d( tmp4, &pi_dct1[36], 0, 1 );
+    __lasx_xvstelm_d( tmp5, &pi_dct1[40], 0, 0 );
+    __lasx_xvstelm_d( tmp5, &pi_dct1[44], 0, 1 );
+    __lasx_xvstelm_d( tmp6, &pi_dct1[48], 0, 0 );
+    __lasx_xvstelm_d( tmp6, &pi_dct1[52], 0, 1 );
+    __lasx_xvstelm_d( tmp7, &pi_dct1[56], 0, 0 );
+    __lasx_xvstelm_d( tmp7, &pi_dct1[60], 0, 1 );
+
+    __lasx_xvstelm_d( tmp0, &pi_dct2[0], 0, 2 );
+    __lasx_xvstelm_d( tmp0, &pi_dct2[4], 0, 3 );
+    __lasx_xvstelm_d( tmp1, &pi_dct2[8], 0, 2 );
+    __lasx_xvstelm_d( tmp1, &pi_dct2[12], 0, 3 );
+    __lasx_xvstelm_d( tmp2, &pi_dct2[16], 0, 2 );
+    __lasx_xvstelm_d( tmp2, &pi_dct2[20], 0, 3 );
+    __lasx_xvstelm_d( tmp3, &pi_dct2[24], 0, 2 );
+    __lasx_xvstelm_d( tmp3, &pi_dct2[28], 0, 3 );
+    __lasx_xvstelm_d( tmp4, &pi_dct2[32], 0, 2 );
+    __lasx_xvstelm_d( tmp4, &pi_dct2[36], 0, 3 );
+    __lasx_xvstelm_d( tmp5, &pi_dct2[40], 0, 2 );
+    __lasx_xvstelm_d( tmp5, &pi_dct2[44], 0, 3 );
+    __lasx_xvstelm_d( tmp6, &pi_dct2[48], 0, 2 );
+    __lasx_xvstelm_d( tmp6, &pi_dct2[52], 0, 3 );
+    __lasx_xvstelm_d( tmp7, &pi_dct2[56], 0, 2 );
+    __lasx_xvstelm_d( tmp7, &pi_dct2[60], 0, 3 );
+
+}
+
+void x264_sub16x16_dct8_lasx( int16_t pi_dct[4][64], uint8_t *p_pix1,
+                              uint8_t *p_pix2 )
+{
+    x264_sub8x8_dct8_ext_lasx( pi_dct[0], &p_pix1[0], &p_pix2[0],
+                               pi_dct[1] );
+    x264_sub8x8_dct8_ext_lasx( pi_dct[2], &p_pix1[8 * FENC_STRIDE + 0],
+                               &p_pix2[8*FDEC_STRIDE+0], pi_dct[3] );
+}
+
+#endif
diff --git a/common/loongarch/dct.h b/common/loongarch/dct.h
new file mode 100644
index 0000000..8a0991e
--- /dev/null
+++ b/common/loongarch/dct.h
@@ -0,0 +1,57 @@
+/*****************************************************************************
+ * dct.h: loongarch transform and zigzag
+ *****************************************************************************
+ * Copyright (C) 2015-2018 x264 project
+ * Copyright (C) 2020 Loongson Technology Corporation Limited
+ *
+ * Authors: zhou peng <zhoupeng@loongson.cn>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02111, USA.
+ *
+ * This program is also available under a commercial proprietary license.
+ * For more information, contact us at licensing@x264.com.
+ *****************************************************************************/
+
+#ifndef X264_LOONGARCH_DCT_H
+#define X264_LOONGARCH_DCT_H
+
+#define x264_sub4x4_dct_lasx x264_template(sub4x4_dct_lasx)
+void x264_sub4x4_dct_lasx( int16_t p_dst[16], uint8_t *p_src, uint8_t *p_ref );
+#define x264_sub8x8_dct_lasx x264_template(sub8x8_dct_lasx)
+void x264_sub8x8_dct_lasx( int16_t p_dst[4][16], uint8_t *p_src,
+                           uint8_t *p_ref );
+#define x264_sub16x16_dct_lasx x264_template(sub16x16_dct_lasx)
+void x264_sub16x16_dct_lasx( int16_t p_dst[16][16], uint8_t *p_src,
+                             uint8_t *p_ref );
+
+#define x264_sub8x8_dct8_lasx x264_template(sub8x8_dct8_lasx)
+void x264_sub8x8_dct8_lasx( int16_t pi_dct[64], uint8_t *p_pix1,
+                            uint8_t *p_pix2 );
+#define x264_sub16x16_dct8_lasx x264_template(sub16x16_dct8_lasx)
+void x264_sub16x16_dct8_lasx( int16_t pi_dct[4][64], uint8_t *p_pix1,
+                              uint8_t *p_pix2 );
+
+#define x264_add4x4_idct_lasx x264_template(add4x4_idct_lasx)
+void x264_add4x4_idct_lasx( uint8_t *p_dst, int16_t pi_dct[16] );
+#define x264_add8x8_idct_lasx x264_template(add8x8_idct_lasx)
+void x264_add8x8_idct_lasx( uint8_t *p_dst, int16_t pi_dct[4][16] );
+#define x264_add16x16_idct_lasx x264_template(add16x16_idct_lasx)
+void x264_add16x16_idct_lasx( uint8_t *p_dst, int16_t pi_dct[16][16] );
+#define x264_add8x8_idct8_lasx x264_template(add8x8_idct8_lasx)
+void x264_add8x8_idct8_lasx( uint8_t *p_dst, int16_t pi_dct[64] );
+#define x264_add8x8_idct_dc_lasx x264_template(add8x8_idct_dc_lasx)
+void x264_add8x8_idct_dc_lasx( uint8_t *p_dst, int16_t dct[4] );
+
+#endif
diff --git a/common/loongarch/deblock-c.c b/common/loongarch/deblock-c.c
new file mode 100644
index 0000000..b691266
--- /dev/null
+++ b/common/loongarch/deblock-c.c
@@ -0,0 +1,908 @@
+/*****************************************************************************
+ * deblock-c.c: loongarch deblocking
+ *****************************************************************************
+ * Copyright (C) 2015-2018 x264 project
+ * Copyright (C) 2020 Loongson Technology Corporation Limited
+ *
+ * Authors: zhou peng <zhoupeng@loongson.cn>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02111, USA.
+ *
+ * This program is also available under a commercial proprietary license.
+ * For more information, contact us at licensing@x264.com.
+ *****************************************************************************/
+
+#include "common/common.h"
+#include "loongson_intrinsics.h"
+#include "deblock.h"
+
+#if !HIGH_BIT_DEPTH
+
+#define LASX_LPF_P1_OR_Q1( p0_or_q0_org_in, q0_or_p0_org_in,         \
+                           p1_or_q1_org_in, p2_or_q2_org_in,         \
+                           negate_tc_in, tc_in, p1_or_q1_out )       \
+{                                                                    \
+    __m256i clip0, temp;                                             \
+                                                                     \
+    clip0 = __lasx_xvavgr_hu( p0_or_q0_org_in, q0_or_p0_org_in );    \
+    temp = __lasx_xvslli_h( p1_or_q1_org_in, 1 );                    \
+    clip0 = __lasx_xvsub_h( clip0, temp );                           \
+    clip0 = __lasx_xvavg_h( p2_or_q2_org_in, clip0 );                \
+    clip0 = __lasx_xvclip_h( clip0, negate_tc_in, tc_in );           \
+    p1_or_q1_out = __lasx_xvadd_h( p1_or_q1_org_in, clip0 );         \
+}
+
+#define LASX_LPF_P0Q0( q0_or_p0_org_in, p0_or_q0_org_in,             \
+                       p1_or_q1_org_in, q1_or_p1_org_in,             \
+                       negate_threshold_in, threshold_in,            \
+                       p0_or_q0_out, q0_or_p0_out )                  \
+{                                                                    \
+    __m256i q0_sub_p0, p1_sub_q1, delta;                             \
+                                                                     \
+    q0_sub_p0 = __lasx_xvsub_h( q0_or_p0_org_in, p0_or_q0_org_in );  \
+    p1_sub_q1 = __lasx_xvsub_h( p1_or_q1_org_in, q1_or_p1_org_in );  \
+    q0_sub_p0 = __lasx_xvslli_h( q0_sub_p0, 2 );                     \
+    p1_sub_q1 = __lasx_xvaddi_hu( p1_sub_q1, 4 );                    \
+    delta = __lasx_xvadd_h( q0_sub_p0, p1_sub_q1 );                  \
+    delta = __lasx_xvsrai_h( delta, 3 );                             \
+                                                                     \
+    delta = __lasx_xvclip_h(delta, negate_threshold_in,              \
+            threshold_in);                                           \
+                                                                     \
+    p0_or_q0_out = __lasx_xvadd_h( p0_or_q0_org_in, delta );         \
+    q0_or_p0_out = __lasx_xvsub_h( q0_or_p0_org_in, delta );         \
+                                                                     \
+    DUP2_ARG1( __lasx_xvclip255_h, p0_or_q0_out, q0_or_p0_out,       \
+               p0_or_q0_out, q0_or_p0_out );                         \
+}
+
+void x264_deblock_h_luma_lasx( uint8_t *p_pix, intptr_t i_stride,
+                               int32_t i_alpha, int32_t i_beta, int8_t *p_tc0 )
+{
+    uint8_t *p_src;
+    intptr_t i_stride_2x = ( i_stride << 1 );
+    intptr_t i_stride_4x = ( i_stride << 2 );
+    intptr_t i_stride_3x = i_stride_2x + i_stride;
+    __m256i beta, bs, tc;
+    __m256i zero = __lasx_xvldi( 0 );
+
+    tc = __lasx_xvld( p_tc0, 0 );
+    tc = __lasx_xvilvl_b( tc, tc );
+    tc = __lasx_xvilvl_h( tc, tc );
+
+    beta = __lasx_xvsle_w( zero, tc );
+    bs = __lasx_xvandi_b( beta, 0x01 );
+
+    if( !__lasx_xbz_v( bs ) )
+    {
+        __m256i is_less_than, is_less_than_beta, is_bs_greater_than0;
+        __m256i src0, src1, src2, src3, src4, src5, src6, src7;
+        __m256i p3_org, p2_org, p1_org, p0_org, q0_org, q1_org, q2_org, q3_org;
+        __m256i p2_org_l, p1_org_l, p0_org_l, q0_org_l, q1_org_l, q2_org_l;
+        __m256i p2_org_h, p1_org_h, p0_org_h, q0_org_h, q1_org_h, q2_org_h;
+        __m256i tc_l, tc_h;
+        __m256i mask_l = { 0, 2, 0, 2 };
+        __m256i mask_h = { 3, 0, 3, 0 };
+
+        is_bs_greater_than0 = __lasx_xvslt_bu( zero, bs );
+
+        {
+            p_src = p_pix - 4;
+            DUP4_ARG2( __lasx_xvldx, p_src, 0, p_src, i_stride, p_src, i_stride_2x, p_src,
+                       i_stride_3x, src0, src1, src2, src3 );
+            p_src += i_stride_4x;
+            DUP4_ARG2( __lasx_xvldx, p_src, 0, p_src, i_stride, p_src, i_stride_2x, p_src,
+                       i_stride_3x, src4, src5, src6, src7 );
+            p_src += i_stride_4x;
+            DUP4_ARG2( __lasx_xvldx, p_src, 0, p_src, i_stride, p_src, i_stride_2x, p_src,
+                       i_stride_3x, p2_org_l, p1_org_l, p0_org_l, q0_org_l );
+            p_src += i_stride_4x;
+            DUP4_ARG2( __lasx_xvldx, p_src, 0, p_src, i_stride, p_src, i_stride_2x, p_src,
+                       i_stride_3x, q1_org_l, q2_org_l, p2_org_h, p1_org_h );
+            p_src -= i_stride_4x;
+
+            LASX_TRANSPOSE16x8_B( src0, src1, src2, src3,
+                                  src4, src5, src6, src7,
+                                  p2_org_l, p1_org_l, p0_org_l, q0_org_l,
+                                  q1_org_l, q2_org_l, p2_org_h, p1_org_h,
+                                  p3_org, p2_org, p1_org, p0_org,
+                                  q0_org, q1_org, q2_org, q3_org );
+        }
+        {
+            src0 = __lasx_xvabsd_bu( p0_org, q0_org );
+            src1 = __lasx_xvabsd_bu( p1_org, p0_org );
+            src2 = __lasx_xvabsd_bu( q1_org, q0_org );
+
+            src3 = __lasx_xvreplgr2vr_b( i_alpha );
+            beta = __lasx_xvreplgr2vr_b( i_beta );
+
+            src4 = __lasx_xvslt_bu( src0, src3 );
+            is_less_than_beta = __lasx_xvslt_bu( src1, beta );
+            is_less_than = __lasx_xvand_v( is_less_than_beta, src4 );
+            is_less_than_beta = __lasx_xvslt_bu( src2, beta );
+            is_less_than = __lasx_xvand_v( is_less_than_beta,
+                                           is_less_than );
+            is_less_than = __lasx_xvand_v( is_less_than,
+                                           is_bs_greater_than0 );
+        }
+        if( !__lasx_xbz_v( is_less_than ) )
+        {
+            __m256i negate_tc, sign_negate_tc;
+            __m256i negate_tc_l, i16_negatetc_h;
+
+            negate_tc = __lasx_xvsub_b( zero, tc );
+            sign_negate_tc = __lasx_xvslti_b( negate_tc, 0 );
+
+            negate_tc_l = __lasx_xvilvl_b( sign_negate_tc, negate_tc );
+            i16_negatetc_h = __lasx_xvilvh_b( sign_negate_tc, negate_tc );
+
+            tc_l = __lasx_xvilvl_b( zero, tc );
+            tc_h = __lasx_xvilvh_b( zero, tc );
+            p1_org_l = __lasx_xvilvl_b( zero, p1_org );
+            p1_org_h = __lasx_xvilvh_b( zero, p1_org );
+            p0_org_l = __lasx_xvilvl_b( zero, p0_org );
+            p0_org_h = __lasx_xvilvh_b( zero, p0_org );
+            q0_org_l = __lasx_xvilvl_b( zero, q0_org );
+            q0_org_h = __lasx_xvilvh_b( zero, q0_org );
+
+            {
+                __m256i p2_asub_p0;
+                __m256i is_less_than_beta_l, is_less_than_beta_h;
+
+                p2_asub_p0 = __lasx_xvabsd_bu( p2_org, p0_org );
+                is_less_than_beta = __lasx_xvslt_bu( p2_asub_p0, beta );
+                is_less_than_beta = __lasx_xvand_v( is_less_than_beta,
+                                                    is_less_than );
+
+                is_less_than_beta_l = __lasx_xvshuf_d( mask_l, is_less_than_beta,
+                                                       zero );
+                if( !__lasx_xbz_v( is_less_than_beta_l ) )
+                {
+                    p2_org_l = __lasx_xvilvl_b( zero, p2_org );
+
+                    LASX_LPF_P1_OR_Q1( p0_org_l, q0_org_l, p1_org_l, p2_org_l,
+                                       negate_tc_l, tc_l, src2 );
+                }
+
+                is_less_than_beta_h = __lasx_xvshuf_d( mask_h, is_less_than_beta,
+                                                       zero );
+                if( !__lasx_xbz_v( is_less_than_beta_h ) )
+                {
+                    p2_org_h = __lasx_xvilvh_b( zero, p2_org );
+
+                    LASX_LPF_P1_OR_Q1( p0_org_h, q0_org_h, p1_org_h, p2_org_h,
+                                       i16_negatetc_h, tc_h, src6 );
+                }
+            }
+
+            if( !__lasx_xbz_v( is_less_than_beta ) )
+            {
+                src6 = __lasx_xvpickev_b( src6, src2 );
+                p1_org = __lasx_xvbitsel_v( p1_org, src6, is_less_than_beta );
+
+                is_less_than_beta = __lasx_xvandi_b( is_less_than_beta, 1 );
+                tc = __lasx_xvadd_b( tc, is_less_than_beta );
+            }
+
+            {
+                __m256i u8_q2asub_q0;
+                __m256i is_less_than_beta_h, is_less_than_beta_l;
+
+                u8_q2asub_q0 = __lasx_xvabsd_bu( q2_org, q0_org );
+                is_less_than_beta = __lasx_xvslt_bu( u8_q2asub_q0, beta );
+                is_less_than_beta = __lasx_xvand_v( is_less_than_beta,
+                                                    is_less_than );
+
+                q1_org_l = __lasx_xvilvl_b( zero, q1_org );
+
+                is_less_than_beta_l = __lasx_xvshuf_d( mask_l, is_less_than_beta,
+                                                       zero );
+                if( !__lasx_xbz_v( is_less_than_beta_l ) )
+                {
+                    q2_org_l = __lasx_xvilvl_b( zero, q2_org );
+                    LASX_LPF_P1_OR_Q1( p0_org_l, q0_org_l, q1_org_l, q2_org_l,
+                                       negate_tc_l, tc_l, src3 );
+                }
+
+                q1_org_h = __lasx_xvilvh_b( zero, q1_org );
+
+                is_less_than_beta_h = __lasx_xvshuf_d( mask_h, is_less_than_beta,
+                                                       zero );
+                if( !__lasx_xbz_v( is_less_than_beta_h ) )
+                {
+                    q2_org_h = __lasx_xvilvh_b( zero, q2_org );
+                    LASX_LPF_P1_OR_Q1( p0_org_h, q0_org_h, q1_org_h, q2_org_h,
+                                       i16_negatetc_h, tc_h, src7 );
+                }
+            }
+
+            if( !__lasx_xbz_v( is_less_than_beta ) )
+            {
+                src7 = __lasx_xvpickev_b( src7, src3 );
+                q1_org = __lasx_xvbitsel_v( q1_org, src7, is_less_than_beta );
+
+                is_less_than_beta = __lasx_xvandi_b( is_less_than_beta, 1 );
+                tc = __lasx_xvadd_b( tc, is_less_than_beta );
+            }
+
+            {
+                __m256i threshold_l, negate_thresh_l;
+                __m256i threshold_h, negate_thresh_h;
+                __m256i negate_thresh, sign_negate_thresh;
+
+                negate_thresh = __lasx_xvsub_b( zero, tc );
+                sign_negate_thresh = __lasx_xvslti_b( negate_thresh, 0 );
+
+                DUP2_ARG2( __lasx_xvilvl_b, zero, tc, sign_negate_thresh, negate_thresh,
+                           threshold_l, negate_thresh_l );
+
+                LASX_LPF_P0Q0( q0_org_l, p0_org_l, p1_org_l, q1_org_l,
+                               negate_thresh_l, threshold_l, src0, src1 );
+
+                threshold_h = __lasx_xvilvh_b( zero, tc );
+                negate_thresh_h = __lasx_xvilvh_b( sign_negate_thresh,
+                                                   negate_thresh );
+
+                LASX_LPF_P0Q0( q0_org_h, p0_org_h, p1_org_h, q1_org_h,
+                               negate_thresh_h, threshold_h, src4, src5 );
+            }
+
+            src4 = __lasx_xvpickev_b( src4, src0 );
+            src5 = __lasx_xvpickev_b( src5, src1 );
+
+            p0_org = __lasx_xvbitsel_v( p0_org, src4, is_less_than );
+            q0_org = __lasx_xvbitsel_v( q0_org, src5, is_less_than );
+        }
+        {
+            p_src = p_pix - 3;
+
+            src0 = __lasx_xvilvl_b( p1_org, p2_org );
+            src2 = __lasx_xvilvh_b( p1_org, p2_org );
+            src1 = __lasx_xvilvl_b( q0_org, p0_org );
+            src3 = __lasx_xvilvh_b( q0_org, p0_org );
+            src4 = __lasx_xvilvl_b( q2_org, q1_org );
+            src5 = __lasx_xvilvh_b( q2_org, q1_org );
+
+            src6 = __lasx_xvilvl_h( src1, src0 );
+            src7 = __lasx_xvilvh_h( src1, src0 );
+            src0 = __lasx_xvilvl_h( src3, src2 );
+            src1 = __lasx_xvilvh_h( src3, src2 );
+
+            __lasx_xvstelm_w( src6, p_src, 0, 0 );
+            __lasx_xvstelm_h( src4, p_src, 4, 0 );
+            p_src += i_stride;
+            __lasx_xvstelm_w( src6, p_src, 0, 1 );
+            __lasx_xvstelm_h( src4, p_src, 4, 1 );
+
+            p_src += i_stride;
+            __lasx_xvstelm_w( src6, p_src, 0, 2 );
+            __lasx_xvstelm_h( src4, p_src, 4, 2 );
+            p_src += i_stride;
+            __lasx_xvstelm_w( src6, p_src, 0, 3 );
+            __lasx_xvstelm_h( src4, p_src, 4, 3 );
+
+            p_src += i_stride;
+            __lasx_xvstelm_w( src7, p_src, 0, 0 );
+            __lasx_xvstelm_h( src4, p_src, 4, 4 );
+            p_src += i_stride;
+            __lasx_xvstelm_w( src7, p_src, 0, 1 );
+            __lasx_xvstelm_h( src4, p_src, 4, 5 );
+
+            p_src += i_stride;
+            __lasx_xvstelm_w( src7, p_src, 0, 2 );
+            __lasx_xvstelm_h( src4, p_src, 4, 6 );
+            p_src += i_stride;
+            __lasx_xvstelm_w( src7, p_src, 0, 3 );
+            __lasx_xvstelm_h( src4, p_src, 4, 7 );
+
+            p_src += i_stride;
+            __lasx_xvstelm_w( src0, p_src, 0, 0 );
+            __lasx_xvstelm_h( src5, p_src, 4, 0 );
+            p_src += i_stride;
+            __lasx_xvstelm_w( src0, p_src, 0, 1 );
+            __lasx_xvstelm_h( src5, p_src, 4, 1 );
+
+            p_src += i_stride;
+            __lasx_xvstelm_w( src0, p_src, 0, 2 );
+            __lasx_xvstelm_h( src5, p_src, 4, 2 );
+            p_src += i_stride;
+            __lasx_xvstelm_w( src0, p_src, 0, 3 );
+            __lasx_xvstelm_h( src5, p_src, 4, 3 );
+
+            p_src += i_stride;
+            __lasx_xvstelm_w( src1, p_src, 0, 0 );
+            __lasx_xvstelm_h( src5, p_src, 4, 4 );
+            p_src += i_stride;
+            __lasx_xvstelm_w( src1, p_src, 0, 1 );
+            __lasx_xvstelm_h( src5, p_src, 4, 5 );
+
+            p_src += i_stride;
+            __lasx_xvstelm_w( src1, p_src, 0, 2 );
+            __lasx_xvstelm_h( src5, p_src, 4, 6 );
+            p_src += i_stride;
+            __lasx_xvstelm_w( src1, p_src, 0, 3 );
+            __lasx_xvstelm_h( src5, p_src, 4, 7 );
+        }
+    }
+}
+
+void x264_deblock_v_luma_lasx( uint8_t *p_pix, intptr_t i_stride,
+                               int32_t i_alpha, int32_t i_beta, int8_t *p_tc0 )
+{
+    __m256i bs, tc, beta;
+    __m256i zero = __lasx_xvldi( 0 );
+    intptr_t i_stride_2x = ( i_stride << 1 );
+    intptr_t i_stride_3x = i_stride_2x + i_stride;
+
+    tc = __lasx_xvld( p_tc0, 0 );
+    tc = __lasx_xvilvl_b( tc, tc );
+    tc = __lasx_xvilvl_h( tc, tc );
+
+    beta = __lasx_xvsle_w( zero, tc );
+    bs = __lasx_xvandi_b( beta, 0x01 );
+
+    if( !__lasx_xbz_v( bs ) )
+    {
+        __m256i p2_asub_p0, u8_q2asub_q0;
+        __m256i alpha, is_less_than, is_less_than_beta;
+        __m256i src0, src1, src2, src3, src6, src4, src5, src7;
+        __m256i p2_org, p1_org, p0_org, q0_org, q1_org, q2_org;
+        __m256i p2_org_l, p1_org_l, p0_org_l, q0_org_l, q1_org_l, q2_org_l;
+        __m256i p2_org_h, p1_org_h, p0_org_h, q0_org_h, q1_org_h, q2_org_h;
+        __m256i mask_l = { 0, 2, 0, 2 };
+        __m256i mask_h = { 3, 0, 3, 0 };
+
+        alpha = __lasx_xvreplgr2vr_b( i_alpha );
+        beta = __lasx_xvreplgr2vr_b( i_beta );
+
+        p2_org = __lasx_xvldx( p_pix , -i_stride_3x );
+        p_pix -= i_stride_2x;
+        DUP4_ARG2(__lasx_xvldx, p_pix, 0, p_pix, i_stride, p_pix, i_stride_2x, p_pix,
+                  i_stride_3x, p1_org, p0_org, q0_org, q1_org );
+        p_pix += i_stride_2x;
+        {
+            src5 = __lasx_xvslt_bu( zero, bs );
+            src0 = __lasx_xvabsd_bu( p0_org, q0_org );
+            src1 = __lasx_xvabsd_bu( p1_org, p0_org );
+            src2 = __lasx_xvabsd_bu( q1_org, q0_org );
+
+            src4 = __lasx_xvslt_bu( src0, alpha );
+            is_less_than_beta = __lasx_xvslt_bu( src1, beta );
+            is_less_than = __lasx_xvand_v( is_less_than_beta,
+                                           src4 );
+            is_less_than_beta = __lasx_xvslt_bu( src2, beta );
+            is_less_than = __lasx_xvand_v( is_less_than_beta,
+                                           is_less_than );
+            is_less_than = __lasx_xvand_v( is_less_than, src5 );
+        }
+
+        if( !__lasx_xbz_v( is_less_than ) )
+        {
+            __m256i sign_negate_tc, negate_tc;
+            __m256i negate_tc_l, i16_negatetc_h, tc_h, tc_l;
+
+            q2_org = __lasx_xvldx( p_pix, i_stride_2x );
+            negate_tc = __lasx_xvsub_b( zero, tc );
+            sign_negate_tc = __lasx_xvslti_b( negate_tc, 0 );
+
+            negate_tc_l = __lasx_xvilvl_b( sign_negate_tc, negate_tc );
+            i16_negatetc_h = __lasx_xvilvh_b( sign_negate_tc, negate_tc );
+
+            tc_l = __lasx_xvilvl_b( zero, tc );
+            tc_h = __lasx_xvilvh_b( zero, tc );
+            p1_org_l = __lasx_xvilvl_b( zero, p1_org );
+            p1_org_h = __lasx_xvilvh_b( zero, p1_org );
+            p0_org_l = __lasx_xvilvl_b( zero, p0_org );
+            p0_org_h = __lasx_xvilvh_b( zero, p0_org );
+            q0_org_l = __lasx_xvilvl_b( zero, q0_org );
+            q0_org_h = __lasx_xvilvh_b( zero, q0_org );
+
+            p2_asub_p0 = __lasx_xvabsd_bu( p2_org, p0_org );
+            is_less_than_beta = __lasx_xvslt_bu( p2_asub_p0, beta );
+            is_less_than_beta = __lasx_xvand_v( is_less_than_beta,
+                                                is_less_than );
+            {
+                __m256i is_less_than_beta_l, is_less_than_beta_h;
+
+                is_less_than_beta_l = __lasx_xvshuf_d( mask_l, is_less_than_beta,
+                                                       zero );
+                if( !__lasx_xbz_v( is_less_than_beta_l ) )
+                {
+                    p2_org_l = __lasx_xvilvl_b( zero, p2_org );
+
+                    LASX_LPF_P1_OR_Q1( p0_org_l, q0_org_l, p1_org_l, p2_org_l,
+                                       negate_tc_l, tc_l, src2 );
+                }
+
+                is_less_than_beta_h = __lasx_xvshuf_d( mask_h, is_less_than_beta,
+                                                       zero );
+                if( !__lasx_xbz_v( is_less_than_beta_h ) )
+                {
+                    p2_org_h = __lasx_xvilvh_b( zero, p2_org );
+
+                    LASX_LPF_P1_OR_Q1( p0_org_h, q0_org_h, p1_org_h, p2_org_h,
+                                       i16_negatetc_h, tc_h, src6 );
+                }
+            }
+            if( !__lasx_xbz_v( is_less_than_beta ) )
+            {
+                src6 = __lasx_xvpickev_b( src6, src2 );
+                p1_org = __lasx_xvbitsel_v( p1_org, src6, is_less_than_beta );
+                __lasx_xvstelm_d( p1_org, p_pix - i_stride_2x, 0, 0 );
+                __lasx_xvstelm_d( p1_org, p_pix - i_stride_2x, 8, 1 );
+
+                is_less_than_beta = __lasx_xvandi_b( is_less_than_beta, 1 );
+                tc = __lasx_xvadd_b( tc, is_less_than_beta );
+            }
+
+            u8_q2asub_q0 = __lasx_xvabsd_bu( q2_org, q0_org );
+            is_less_than_beta = __lasx_xvslt_bu( u8_q2asub_q0, beta );
+            is_less_than_beta = __lasx_xvand_v( is_less_than_beta,
+                                                is_less_than );
+
+            {
+                __m256i is_less_than_beta_l, is_less_than_beta_h;
+                is_less_than_beta_l = __lasx_xvshuf_d( mask_l, is_less_than_beta,
+                                                       zero );
+
+                q1_org_l = __lasx_xvilvl_b( zero, q1_org );
+                if( !__lasx_xbz_v( is_less_than_beta_l ) )
+                {
+                    q2_org_l = __lasx_xvilvl_b( zero, q2_org );
+
+                    LASX_LPF_P1_OR_Q1( p0_org_l, q0_org_l, q1_org_l, q2_org_l,
+                                       negate_tc_l, tc_l, src3 );
+                }
+                is_less_than_beta_h = __lasx_xvshuf_d( mask_h, is_less_than_beta,
+                                                       zero );
+
+                q1_org_h = __lasx_xvilvh_b( zero, q1_org );
+                if( !__lasx_xbz_v( is_less_than_beta_h ) )
+                {
+                    q2_org_h = __lasx_xvilvh_b( zero, q2_org );
+
+                    LASX_LPF_P1_OR_Q1( p0_org_h, q0_org_h, q1_org_h, q2_org_h,
+                                       i16_negatetc_h, tc_h, src7 );
+                }
+            }
+            if( !__lasx_xbz_v( is_less_than_beta ) )
+            {
+                src7 = __lasx_xvpickev_b( src7, src3 );
+                q1_org = __lasx_xvbitsel_v( q1_org, src7, is_less_than_beta );
+                __lasx_xvstelm_d( q1_org, p_pix + i_stride, 0, 0 );
+                __lasx_xvstelm_d( q1_org, p_pix + i_stride, 8, 1 );
+
+                is_less_than_beta = __lasx_xvandi_b( is_less_than_beta, 1 );
+                tc = __lasx_xvadd_b( tc, is_less_than_beta );
+            }
+            {
+                __m256i negate_thresh, sign_negate_thresh;
+                __m256i threshold_l, threshold_h;
+                __m256i negate_thresh_h, negate_thresh_l;
+
+                negate_thresh = __lasx_xvsub_b( zero, tc );
+                sign_negate_thresh = __lasx_xvslti_b( negate_thresh, 0 );
+
+                DUP2_ARG2( __lasx_xvilvl_b, zero, tc, sign_negate_thresh, negate_thresh,
+                           threshold_l, negate_thresh_l );
+                LASX_LPF_P0Q0( q0_org_l, p0_org_l, p1_org_l, q1_org_l,
+                               negate_thresh_l, threshold_l, src0, src1 );
+
+                threshold_h = __lasx_xvilvh_b( zero, tc );
+                negate_thresh_h = __lasx_xvilvh_b( sign_negate_thresh,
+                                                   negate_thresh );
+                LASX_LPF_P0Q0( q0_org_h, p0_org_h, p1_org_h, q1_org_h,
+                               negate_thresh_h, threshold_h, src4, src5 );
+            }
+
+            src4 = __lasx_xvpickev_b( src4, src0 );
+            src5 = __lasx_xvpickev_b( src5, src1 );
+
+            p0_org = __lasx_xvbitsel_v( p0_org, src4, is_less_than );
+            q0_org = __lasx_xvbitsel_v( q0_org, src5, is_less_than );
+
+            __lasx_xvstelm_d( p0_org, p_pix - i_stride, 0, 0 );
+            __lasx_xvstelm_d( p0_org, p_pix - i_stride, 8, 1 );
+            __lasx_xvstelm_d( q0_org, p_pix, 0, 0 );
+            __lasx_xvstelm_d( q0_org, p_pix, 8, 1 );
+        }
+    }
+}
+
+static void avc_deblock_strength_lasx( uint8_t *nnz,
+                                       int8_t pi_lef[2][X264_SCAN8_LUMA_SIZE],
+                                       int16_t pi_mv[2][X264_SCAN8_LUMA_SIZE][2],
+                                       uint8_t pu_bs[2][8][4],
+                                       int32_t i_mvy_himit )
+{
+    __m256i nnz0, nnz1, nnz2, nnz3, nnz4;
+    __m256i nnz_mask, ref_mask, mask, one, two, dst = { 0 };
+    __m256i ref0, ref1, ref2, ref3, ref4;
+    __m256i temp_vec0, temp_vec1, temp_vec2;
+    __m256i mv0, mv1, mv2, mv3, mv4, mv5, mv6, mv7, mv8, mv9, mv_a, mv_b;
+    __m256i four, mvy_himit_vec, sub0, sub1;
+    int8_t* p_lef = pi_lef[0];
+    int16_t* p_mv = pi_mv[0][0];
+
+    DUP2_ARG2(__lasx_xvld, nnz, 4, nnz, 20, nnz0, nnz2 );
+    nnz4 = __lasx_xvld( nnz, 36 );
+
+    DUP2_ARG2(__lasx_xvld, p_lef, 4, p_lef, 20, ref0, ref2 );
+    ref4 = __lasx_xvld( p_lef, 36 );
+
+    DUP4_ARG2(__lasx_xvld, p_mv, 16, p_mv, 48, p_mv, 80, p_mv, 112, mv0, mv1, mv2, mv3 );
+    mv4 = __lasx_xvld( p_mv, 144 );
+
+    mvy_himit_vec = __lasx_xvreplgr2vr_h( i_mvy_himit );
+    four = __lasx_xvreplgr2vr_h( 4 );
+    mask = __lasx_xvldi( 0 );
+    one = __lasx_xvldi( 1 );
+    two = __lasx_xvldi( 2 );
+
+    mv5 = __lasx_xvpickod_h( mv0, mv0 );
+    mv6 = __lasx_xvpickod_h( mv1, mv1 );
+    mv_a = __lasx_xvpickev_h( mv0, mv0 );
+    mv_b = __lasx_xvpickev_h( mv1, mv1 );
+    nnz1 = __lasx_xvrepl128vei_w( nnz0, 2 );
+    ref1 = __lasx_xvrepl128vei_w( ref0, 2 );
+    nnz_mask = __lasx_xvor_v( nnz0, nnz1 );
+    nnz_mask = __lasx_xvseq_b( mask, nnz_mask );
+    two = __lasx_xvbitsel_v( two, mask, nnz_mask );
+
+    ref_mask = __lasx_xvseq_b( ref0, ref1 );
+    ref_mask = __lasx_xvxori_b( ref_mask, 255 );
+
+    sub0 = __lasx_xvabsd_h( mv_b, mv_a );
+    sub1 = __lasx_xvabsd_h( mv6, mv5 );
+
+    sub0 = __lasx_xvsle_hu( four, sub0 );
+    sub1 = __lasx_xvsle_hu( mvy_himit_vec, sub1 );
+
+    sub0 = __lasx_xvpickev_b( sub0, sub0 );
+    sub1 = __lasx_xvpickev_b( sub1, sub1 );
+    ref_mask = __lasx_xvor_v( ref_mask, sub0 );
+    ref_mask = __lasx_xvor_v( ref_mask, sub1 );
+
+    dst = __lasx_xvbitsel_v( dst, one, ref_mask );
+    dst = __lasx_xvbitsel_v( two, dst, nnz_mask );
+
+    __lasx_xvstelm_w( dst, pu_bs[1][0], 0, 0 );
+
+    dst = __lasx_xvldi( 0 );
+    two = __lasx_xvldi( 2 );
+
+    mv5 = __lasx_xvpickod_h( mv1, mv1 );
+    mv6 = __lasx_xvpickod_h( mv2, mv2 );
+    mv_a = __lasx_xvpickev_h( mv1, mv1 );
+    mv_b = __lasx_xvpickev_h( mv2, mv2 );
+
+    nnz_mask = __lasx_xvor_v( nnz2, nnz1 );
+    nnz_mask = __lasx_xvseq_b( mask, nnz_mask );
+    two = __lasx_xvbitsel_v( two, mask, nnz_mask );
+
+    ref_mask = __lasx_xvseq_b( ref1, ref2 );
+    ref_mask = __lasx_xvxori_b( ref_mask, 255 );
+
+    sub0 = __lasx_xvabsd_h( mv_b, mv_a );
+    sub1 = __lasx_xvabsd_h( mv6, mv5 );
+    sub0 = __lasx_xvsle_hu( four, sub0 );
+    sub1 = __lasx_xvsle_hu( mvy_himit_vec, sub1 );
+
+    sub0 = __lasx_xvpickev_b( sub0, sub0 );
+    sub1 = __lasx_xvpickev_b( sub1, sub1 );
+    ref_mask = __lasx_xvor_v( ref_mask, sub0 );
+    ref_mask = __lasx_xvor_v( ref_mask, sub1 );
+
+    dst = __lasx_xvbitsel_v( dst, one, ref_mask );
+    dst = __lasx_xvbitsel_v( two, dst, nnz_mask );
+
+    __lasx_xvstelm_w( dst, pu_bs[1][1], 0, 0 );
+
+    dst = __lasx_xvldi( 0 );
+    two = __lasx_xvldi( 2 );
+
+    mv5 = __lasx_xvpickod_h( mv2, mv2 );
+    mv6 = __lasx_xvpickod_h( mv3, mv3 );
+    mv_a = __lasx_xvpickev_h( mv2, mv2 );
+    mv_b = __lasx_xvpickev_h( mv3, mv3 );
+
+    nnz3 = __lasx_xvrepl128vei_w( nnz2, 2 );
+    ref3 = __lasx_xvrepl128vei_w( ref2, 2 );
+
+    nnz_mask = __lasx_xvor_v( nnz3, nnz2 );
+    nnz_mask = __lasx_xvseq_b( mask, nnz_mask );
+    two = __lasx_xvbitsel_v( two, mask, nnz_mask );
+
+    ref_mask = __lasx_xvseq_b( ref2, ref3 );
+    ref_mask = __lasx_xvxori_b( ref_mask, 255 );
+
+    sub0 = __lasx_xvabsd_h( mv_b, mv_a );
+    sub1 = __lasx_xvabsd_h( mv6, mv5 );
+
+    sub0 = __lasx_xvsle_hu( four, sub0 );
+    sub1 = __lasx_xvsle_hu( mvy_himit_vec, sub1 );
+
+    sub0 = __lasx_xvpickev_b( sub0, sub0 );
+    sub1 = __lasx_xvpickev_b( sub1, sub1 );
+    ref_mask = __lasx_xvor_v( ref_mask, sub0 );
+    ref_mask = __lasx_xvor_v( ref_mask, sub1 );
+
+    dst = __lasx_xvbitsel_v( dst, one, ref_mask );
+    dst = __lasx_xvbitsel_v( two, dst, nnz_mask );
+
+    __lasx_xvstelm_w( dst, pu_bs[1][2], 0, 0 );
+
+    dst = __lasx_xvldi( 0 );
+    two = __lasx_xvldi( 2 );
+
+    mv5 = __lasx_xvpickod_h( mv3, mv3 );
+    mv6 = __lasx_xvpickod_h( mv4, mv4 );
+    mv_a = __lasx_xvpickev_h( mv3, mv3 );
+    mv_b = __lasx_xvpickev_h( mv4, mv4 );
+
+    nnz_mask = __lasx_xvor_v( nnz4, nnz3 );
+    nnz_mask = __lasx_xvseq_b( mask, nnz_mask );
+    two = __lasx_xvbitsel_v( two, mask, nnz_mask );
+
+    ref_mask = __lasx_xvseq_b( ref3, ref4 );
+    ref_mask = __lasx_xvxori_b( ref_mask, 255 );
+
+    sub0 = __lasx_xvabsd_h( mv_b, mv_a );
+    sub1 = __lasx_xvabsd_h( mv6, mv5 );
+
+    sub0 = __lasx_xvsle_hu( four, sub0 );
+    sub1 = __lasx_xvsle_hu( mvy_himit_vec, sub1 );
+
+    sub0 = __lasx_xvpickev_b( sub0, sub0 );
+    sub1 = __lasx_xvpickev_b( sub1, sub1 );
+    ref_mask = __lasx_xvor_v( ref_mask, sub0 );
+    ref_mask = __lasx_xvor_v( ref_mask, sub1 );
+
+    dst = __lasx_xvbitsel_v( dst, one, ref_mask );
+    dst = __lasx_xvbitsel_v( two, dst, nnz_mask );
+
+    __lasx_xvstelm_w( dst, pu_bs[1][3], 0, 0 );
+
+    DUP2_ARG2( __lasx_xvld, nnz, 8, nnz, 24, nnz0, nnz2 );
+    DUP2_ARG2( __lasx_xvld, p_lef, 8, p_lef, 24, ref0, ref2);
+
+    DUP4_ARG2(__lasx_xvld, p_mv, 32, p_mv, 48, p_mv, 64, p_mv, 80, mv0, mv1, mv2, mv3 );
+    DUP4_ARG2(__lasx_xvld, p_mv, 96, p_mv, 112, p_mv, 128, p_mv, 144, mv4, mv7, mv8, mv9 );
+
+    nnz1 = __lasx_xvrepl128vei_d( nnz0, 1 );
+    nnz3 = __lasx_xvrepl128vei_d( nnz2, 1 );
+
+    DUP2_ARG2( __lasx_xvilvl_b, nnz2, nnz0, nnz3, nnz1, temp_vec0, temp_vec1 );
+
+    temp_vec2 = __lasx_xvilvl_b( temp_vec1, temp_vec0 );
+    nnz1 = __lasx_xvilvh_b( temp_vec1, temp_vec0 );
+
+    nnz0 = __lasx_xvrepl128vei_w( temp_vec2, 3 );
+    nnz2 = __lasx_xvrepl128vei_w( nnz1, 1 );
+    nnz3 = __lasx_xvrepl128vei_w( nnz1, 2 );
+    nnz4 = __lasx_xvrepl128vei_w( nnz1, 3 );
+
+    ref1 = __lasx_xvrepl128vei_d( ref0, 1 );
+    ref3 = __lasx_xvrepl128vei_d( ref2, 1 );
+
+    DUP2_ARG2( __lasx_xvilvl_b, ref2, ref0, ref3, ref1, temp_vec0, temp_vec1 );
+
+    temp_vec2 = __lasx_xvilvl_b( temp_vec1, temp_vec0 );
+    ref1 = __lasx_xvilvh_b( temp_vec1, temp_vec0 );
+
+    ref0 = __lasx_xvrepl128vei_w( temp_vec2, 3 );
+
+    ref2 = __lasx_xvrepl128vei_w( ref1, 1 );
+    ref3 = __lasx_xvrepl128vei_w( ref1, 2 );
+    ref4 = __lasx_xvrepl128vei_w( ref1, 3 );
+
+    LASX_TRANSPOSE8X4_H( mv0, mv2, mv4, mv8, mv5, mv5, mv5, mv0 );
+    LASX_TRANSPOSE8X4_H( mv1, mv3, mv7, mv9, mv1, mv2, mv3, mv4 );
+
+    mvy_himit_vec = __lasx_xvreplgr2vr_h( i_mvy_himit );
+    four = __lasx_xvreplgr2vr_h( 4 );
+    mask = __lasx_xvldi( 0 );
+    one = __lasx_xvldi( 1 );
+    two = __lasx_xvldi( 2 );
+    dst = __lasx_xvldi( 0 );
+
+    mv5 = __lasx_xvrepl128vei_d( mv0, 1 );
+    mv6 = __lasx_xvrepl128vei_d( mv1, 1 );
+
+    nnz_mask = __lasx_xvor_v( nnz0, nnz1 );
+    nnz_mask = __lasx_xvseq_b( mask, nnz_mask );
+    two = __lasx_xvbitsel_v( two, mask, nnz_mask );
+
+    ref_mask = __lasx_xvseq_b( ref0, ref1 );
+    ref_mask = __lasx_xvxori_b( ref_mask, 255 );
+
+    sub0 = __lasx_xvabsd_h( mv1, mv0 );
+    sub1 = __lasx_xvabsd_h( mv6, mv5 );
+
+    sub0 = __lasx_xvsle_hu( four, sub0 );
+    sub1 = __lasx_xvsle_hu( mvy_himit_vec, sub1 );
+
+    sub0 = __lasx_xvpickev_b( sub0, sub0 );
+    sub1 = __lasx_xvpickev_b( sub1, sub1 );
+    ref_mask = __lasx_xvor_v( ref_mask, sub0 );
+    ref_mask = __lasx_xvor_v( ref_mask, sub1 );
+
+    dst = __lasx_xvbitsel_v( dst, one, ref_mask );
+    dst = __lasx_xvbitsel_v( two, dst, nnz_mask );
+
+    __lasx_xvstelm_w( dst, pu_bs[0][0], 0, 0 );
+
+    two = __lasx_xvldi( 2 );
+    dst = __lasx_xvldi( 0 );
+
+    mv5 = __lasx_xvrepl128vei_d( mv1, 1 );
+    mv6 = __lasx_xvrepl128vei_d( mv2, 1 );
+
+    nnz_mask = __lasx_xvor_v( nnz1, nnz2 );
+    nnz_mask = __lasx_xvseq_b( mask, nnz_mask );
+    two = __lasx_xvbitsel_v( two, mask, nnz_mask );
+
+    ref_mask = __lasx_xvseq_b( ref1, ref2 );
+    ref_mask = __lasx_xvxori_b( ref_mask, 255 );
+
+    sub0 = __lasx_xvabsd_h( mv2, mv1 );
+    sub1 = __lasx_xvabsd_h( mv6, mv5 );
+    sub0 = __lasx_xvsle_hu( four, sub0 );
+    sub1 = __lasx_xvsle_hu( mvy_himit_vec, sub1 );
+
+    sub0 = __lasx_xvpickev_b( sub0, sub0 );
+    sub1 = __lasx_xvpickev_b( sub1, sub1 );
+    ref_mask = __lasx_xvor_v( ref_mask, sub0 );
+    ref_mask = __lasx_xvor_v( ref_mask, sub1 );
+
+    dst = __lasx_xvbitsel_v( dst, one, ref_mask );
+    dst = __lasx_xvbitsel_v( two, dst, nnz_mask );
+
+    __lasx_xvstelm_w( dst, pu_bs[0][1], 0, 0 );
+
+    two = __lasx_xvldi( 2 );
+    dst = __lasx_xvldi( 0 );
+
+    mv5 = __lasx_xvrepl128vei_d( mv2, 1 );
+    mv6 = __lasx_xvrepl128vei_d( mv3, 1 );
+
+    nnz_mask = __lasx_xvor_v( nnz2, nnz3 );
+    nnz_mask = __lasx_xvseq_b( mask, nnz_mask );
+    two = __lasx_xvbitsel_v( two, mask, nnz_mask );
+
+    ref_mask = __lasx_xvseq_b( ref2, ref3 );
+    ref_mask = __lasx_xvxori_b( ref_mask, 255 );
+
+    sub0 = __lasx_xvabsd_h( mv3, mv2 );
+    sub1 = __lasx_xvabsd_h( mv6, mv5 );
+    sub0 = __lasx_xvsle_hu( four, sub0 );
+    sub1 = __lasx_xvsle_hu( mvy_himit_vec, sub1 );
+
+    sub0 = __lasx_xvpickev_b( sub0, sub0 );
+    sub1 = __lasx_xvpickev_b( sub1, sub1 );
+    ref_mask = __lasx_xvor_v( ref_mask, sub0 );
+    ref_mask = __lasx_xvor_v( ref_mask, sub1 );
+
+    dst = __lasx_xvbitsel_v( dst, one, ref_mask );
+    dst = __lasx_xvbitsel_v( two, dst, nnz_mask );
+
+    __lasx_xvstelm_w( dst, pu_bs[0][2], 0, 0 );
+
+    two = __lasx_xvldi( 2 );
+    dst = __lasx_xvldi( 0 );
+
+    mv5 = __lasx_xvrepl128vei_d( mv3, 1 );
+    mv6 = __lasx_xvrepl128vei_d( mv4, 1 );
+
+    nnz_mask = __lasx_xvor_v( nnz3, nnz4 );
+    nnz_mask = __lasx_xvseq_b( mask, nnz_mask );
+    two = __lasx_xvbitsel_v( two, mask, nnz_mask );
+
+    ref_mask = __lasx_xvseq_b( ref3, ref4 );
+    ref_mask = __lasx_xvxori_b( ref_mask, 255 );
+
+    sub0 = __lasx_xvabsd_h( mv4, mv3 );
+    sub1 = __lasx_xvabsd_h( mv6, mv5 );
+    sub0 = __lasx_xvsle_hu( four, sub0 );
+    sub1 = __lasx_xvsle_hu( mvy_himit_vec, sub1 );
+
+    sub0 = __lasx_xvpickev_b( sub0, sub0 );
+    sub1 = __lasx_xvpickev_b( sub1, sub1 );
+    ref_mask = __lasx_xvor_v( ref_mask, sub0 );
+    ref_mask = __lasx_xvor_v( ref_mask, sub1 );
+
+    dst = __lasx_xvbitsel_v( dst, one, ref_mask );
+    dst = __lasx_xvbitsel_v( two, dst, nnz_mask );
+
+    __lasx_xvstelm_w( dst, pu_bs[0][3], 0, 0 );
+}
+
+void x264_deblock_strength_lasx( uint8_t u_nnz[X264_SCAN8_SIZE],
+                                 int8_t pi_lef[2][X264_SCAN8_LUMA_SIZE],
+                                 int16_t pi_mv[2][X264_SCAN8_LUMA_SIZE][2],
+                                 uint8_t pu_bs[2][8][4], int32_t i_mvy_himit,
+                                 int32_t i_bframe )
+{
+    int32_t i_edge, i, loc, locn;
+    int8_t* p_lef0 = pi_lef[0];
+    int8_t* p_lef1 = pi_lef[1];
+    uint8_t (*p_bs0)[4] = pu_bs[0];
+    uint8_t (*p_bs1)[4] = pu_bs[1];
+    int16_t (*p_mv0)[2] = pi_mv[0];
+    int16_t (*p_mv1)[2] = pi_mv[1];
+
+    if( i_bframe )
+    {
+        for( i_edge = 0; i_edge < 4; i_edge++ )
+        {
+            loc = X264_SCAN8_0 + i_edge;
+            for( i = 0; i < 4; i++, loc += 8 )
+            {
+                locn = loc - 1;
+                if( u_nnz[loc] || u_nnz[locn] )
+                {
+                    p_bs0[i_edge][i] = 2;
+                }
+                else if( p_lef0[loc] != p_lef0[locn] ||
+                         abs( p_mv0[loc][0] - p_mv0[locn][0] ) >= 4 ||
+                         abs( p_mv0[loc][1] - p_mv0[locn][1] ) >= i_mvy_himit ||
+                         ( p_lef1[loc] != p_lef1[locn] ||
+                           abs( p_mv1[loc][0] - p_mv1[locn][0] ) >= 4 ||
+                           abs( p_mv1[loc][1] - p_mv1[locn][1] ) >= i_mvy_himit )
+                       )
+                {
+                    p_bs0[i_edge][i] = 1;
+                }
+                else
+                {
+                    p_bs0[i_edge][i] = 0;
+                }
+            }
+        }
+
+        for( i_edge = 0; i_edge < 4; i_edge++ )
+        {
+            loc = X264_SCAN8_0 + ( i_edge << 3 );
+            for( i = 0; i < 4; i++, loc++ )
+            {
+                locn = loc - 8;
+                if( u_nnz[loc] || u_nnz[locn] )
+                {
+                    p_bs1[i_edge][i] = 2;
+                }
+                else if( p_lef0[loc] != p_lef0[locn] ||
+                         abs( p_mv0[loc][0] - p_mv0[locn][0] ) >= 4 ||
+                         abs( p_mv0[loc][1] - p_mv0[locn][1] ) >= i_mvy_himit ||
+                         ( p_lef1[loc] != p_lef1[locn] ||
+                           abs( p_mv1[loc][0] - p_mv1[locn][0] ) >= 4 ||
+                           abs( p_mv1[loc][1] - p_mv1[locn][1] ) >= i_mvy_himit )
+                       )
+                {
+                    p_bs1[i_edge][i] = 1;
+                }
+                else
+                {
+                    p_bs1[i_edge][i] = 0;
+                }
+            }
+        }
+    }
+    else
+    {
+        avc_deblock_strength_lasx( u_nnz, pi_lef, pi_mv, pu_bs, i_mvy_himit );
+    }
+}
+
+#endif
diff --git a/common/loongarch/deblock.h b/common/loongarch/deblock.h
new file mode 100644
index 0000000..4af3a4e
--- /dev/null
+++ b/common/loongarch/deblock.h
@@ -0,0 +1,41 @@
+/*****************************************************************************
+ * deblock.h: loongarch deblocking
+ *****************************************************************************
+ * Copyright (C) 2017-2018 x264 project
+ * Copyright (C) 2020 Loongson Technology Corporation Limited
+ *
+ * Authors: zhou peng <zhoupeng@loongson.cn>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02111, USA.
+ *
+ * This program is also available under a commercial proprietary license.
+ * For more information, contact us at licensing@x264.com.
+ *****************************************************************************/
+
+#ifndef X264_LOONGARCH_DEBLOCK_H
+#define X264_LOONGARCH_DEBLOCK_H
+
+#if !HIGH_BIT_DEPTH
+#define x264_deblock_v_luma_lasx x264_template(deblock_v_luma_lasx)
+void x264_deblock_v_luma_lasx( uint8_t *pix, intptr_t stride, int alpha, int beta, int8_t *tc0 );
+#define x264_deblock_h_luma_lasx x264_template(deblock_h_luma_lasx)
+void x264_deblock_h_luma_lasx( uint8_t *pix, intptr_t stride, int alpha, int beta, int8_t *tc0 );
+#define x264_deblock_strength_lasx x264_template(deblock_strength_lasx)
+void x264_deblock_strength_lasx( uint8_t nnz[X264_SCAN8_SIZE], int8_t ref[2][X264_SCAN8_LUMA_SIZE],
+                                 int16_t mv[2][X264_SCAN8_LUMA_SIZE][2], uint8_t bs[2][8][4], int mvy_limit,
+                                 int bframe );
+#endif
+
+#endif
diff --git a/common/loongarch/loongson_intrinsics.h b/common/loongarch/loongson_intrinsics.h
new file mode 100644
index 0000000..a21a68a
--- /dev/null
+++ b/common/loongarch/loongson_intrinsics.h
@@ -0,0 +1,1965 @@
+/*****************************************************************************
+ * loongson_intrinsics.h: loongarch macros
+ *****************************************************************************
+ * Copyright (C) 2020 x264 project
+ * Copyright (C) 2020 Loongson Technology Corporation Limited
+ *
+ * Authors: Peng Zhou    <zhoupeng@loongson.cn>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02111, USA.
+ *
+ * This program is also available under a commercial proprietary license.
+ * For more information, contact us at licensing@x264.com.
+ *****************************************************************************/
+
+/*
+ * Copyright (c) 2021 Loongson Technology Corporation Limited
+ * All rights reserved.
+ * Contributed by Shiyou Yin <yinshiyou-hf@loongson.cn>
+ *                Xiwei Gu   <guxiwei-hf@loongson.cn>
+ *                Lu Wang    <wanglu@loongson.cn>
+ *
+ * This file is maintained in LSOM project, don't change it directly.
+ * You can get the latest version of this header from: ***
+ *
+ */
+
+#ifndef LOONGSON_INTRINSICS_H
+#define LOONGSON_INTRINSICS_H
+
+/**
+ * MAJOR version: Macro usage changes.
+ * MINOR version: Add new functions, or bug fix.
+ * MICRO version: Comment changes or implementation changes.
+ */
+#define LSOM_VERSION_MAJOR 1
+#define LSOM_VERSION_MINOR 0
+#define LSOM_VERSION_MICRO 1
+
+#define DUP2_ARG1(_INS, _IN0, _IN1, _OUT0, _OUT1) \
+{ \
+    _OUT0 = _INS(_IN0); \
+    _OUT1 = _INS(_IN1); \
+}
+
+#define DUP2_ARG2(_INS, _IN0, _IN1, _IN2, _IN3, _OUT0, _OUT1) \
+{ \
+    _OUT0 = _INS(_IN0, _IN1); \
+    _OUT1 = _INS(_IN2, _IN3); \
+}
+
+#define DUP2_ARG3(_INS, _IN0, _IN1, _IN2, _IN3, _IN4, _IN5, _OUT0, _OUT1) \
+{ \
+    _OUT0 = _INS(_IN0, _IN1, _IN2); \
+    _OUT1 = _INS(_IN3, _IN4, _IN5); \
+}
+
+#define DUP4_ARG1(_INS, _IN0, _IN1, _IN2, _IN3, _OUT0, _OUT1, _OUT2, _OUT3) \
+{ \
+    DUP2_ARG1(_INS, _IN0, _IN1, _OUT0, _OUT1); \
+    DUP2_ARG1(_INS, _IN2, _IN3, _OUT2, _OUT3); \
+}
+
+#define DUP4_ARG2(_INS, _IN0, _IN1, _IN2, _IN3, _IN4, _IN5, _IN6, _IN7, \
+                  _OUT0, _OUT1, _OUT2, _OUT3) \
+{ \
+    DUP2_ARG2(_INS, _IN0, _IN1, _IN2, _IN3, _OUT0, _OUT1); \
+    DUP2_ARG2(_INS, _IN4, _IN5, _IN6, _IN7, _OUT2, _OUT3); \
+}
+
+#define DUP4_ARG3(_INS, _IN0, _IN1, _IN2, _IN3, _IN4, _IN5, _IN6, _IN7, \
+                  _IN8, _IN9, _IN10, _IN11, _OUT0, _OUT1, _OUT2, _OUT3) \
+{ \
+    DUP2_ARG3(_INS, _IN0, _IN1, _IN2, _IN3, _IN4,  _IN5,  _OUT0, _OUT1); \
+    DUP2_ARG3(_INS, _IN6, _IN7, _IN8, _IN9, _IN10, _IN11, _OUT2, _OUT3); \
+}
+
+/*
+ * =============================================================================
+ * Description : Print out elements in vector.
+ * Arguments   : Inputs  - RTYPE, _element_num, _in0, _enter
+ *               Outputs -
+ * Details     : Print out '_element_num' elements in 'RTYPE' vector '_in0', if
+ *               '_enter' is TRUE, prefix "\nVP:" will be added first.
+ * Example     : VECT_PRINT(v4i32,4,in0,1); // in0: 1,2,3,4
+ *               VP:1,2,3,4,
+ * =============================================================================
+ */
+#define VECT_PRINT(RTYPE, element_num, in0, enter)    \
+{                                                     \
+    RTYPE _tmp0 = (RTYPE)in0;                         \
+    int _i = 0;                                       \
+    if (enter)                                        \
+        printf("\nVP:");                              \
+    for(_i = 0; _i < element_num; _i++)               \
+        printf("%d,",_tmp0[_i]);                      \
+}
+
+#ifdef __loongarch_sx
+#include <lsxintrin.h>
+/*
+ * =============================================================================
+ * Description : Dot product & addition of byte vector elements
+ * Arguments   : Inputs  - in_c, in_h, in_l
+ *               Outputs - out
+ *               Retrun Type - halfword
+ * Details     : Signed byte elements from in_h are multiplied by
+ *               signed byte elements from in_l, and then added adjacent to
+ *               each other to get results with the twice size of input.
+ *               Then the results plus to signed half word elements from in_c.
+ * Example     : out = __lsx_vdp2add_h_b(in_c, in_h, in_l)
+ *        in_c : 1,2,3,4, 1,2,3,4
+ *        in_h : 1,2,3,4, 5,6,7,8, 1,2,3,4, 5,6,7,8
+ *        in_l : 8,7,6,5, 4,3,2,1, 8,7,6,5, 4,3,2,1
+ *         out : 23,40,41,26, 23,40,41,26
+ * =============================================================================
+ */
+static inline __m128i __lsx_vdp2add_h_b(__m128i in_c, __m128i in_h, __m128i in_l)
+{
+    __m128i out;
+
+    out = __lsx_vmaddwev_h_b(in_c, in_h, in_l);
+    out = __lsx_vmaddwod_h_b(out, in_h, in_l);
+    return out;
+}
+
+/*
+ * =============================================================================
+ * Description : Dot product & addition of byte vector elements
+ * Arguments   : Inputs  - in_c, in_h, in_l
+ *               Outputs - out
+ *               Retrun Type - halfword
+ * Details     : Unsigned byte elements from in_h are multiplied by
+ *               unsigned byte elements from in_l, and then added adjacent to
+ *               each other to get results with the twice size of input.
+ *               The results plus to signed half word elements from in_c.
+ * Example     : out = __lsx_vdp2add_h_b(in_c, in_h, in_l)
+ *        in_c : 1,2,3,4, 1,2,3,4
+ *        in_h : 1,2,3,4, 5,6,7,8, 1,2,3,4, 5,6,7,8
+ *        in_l : 8,7,6,5, 4,3,2,1, 8,7,6,5, 4,3,2,1
+ *         out : 23,40,41,26, 23,40,41,26
+ * =============================================================================
+ */
+static inline __m128i __lsx_vdp2add_h_bu(__m128i in_c, __m128i in_h, __m128i in_l)
+{
+    __m128i out;
+
+    out = __lsx_vmaddwev_h_bu(in_c, in_h, in_l);
+    out = __lsx_vmaddwod_h_bu(out, in_h, in_l);
+    return out;
+}
+
+/*
+ * =============================================================================
+ * Description : Dot product & addition of half word vector elements
+ * Arguments   : Inputs  - in_c, in_h, in_l
+ *               Outputs - out
+ *               Retrun Type - __m128i
+ * Details     : Signed half word elements from in_h are multiplied by
+ *               signed half word elements from in_l, and then added adjacent to
+ *               each other to get results with the twice size of input.
+ *               Then the results plus to signed word elements from in_c.
+ * Example     : out = __lsx_vdp2add_h_b(in_c, in_h, in_l)
+ *        in_c : 1,2,3,4
+ *        in_h : 1,2,3,4, 5,6,7,8
+ *        in_l : 8,7,6,5, 4,3,2,1
+ *         out : 23,40,41,26
+ * =============================================================================
+ */
+static inline __m128i __lsx_vdp2add_w_h(__m128i in_c, __m128i in_h, __m128i in_l)
+{
+    __m128i out;
+
+    out = __lsx_vmaddwev_w_h(in_c, in_h, in_l);
+    out = __lsx_vmaddwod_w_h(out, in_h, in_l);
+    return out;
+}
+
+/*
+ * =============================================================================
+ * Description : Dot product of byte vector elements
+ * Arguments   : Inputs  - in_h, in_l
+ *               Outputs - out
+ *               Retrun Type - halfword
+ * Details     : Signed byte elements from in_h are multiplied by
+ *               signed byte elements from in_l, and then added adjacent to
+ *               each other to get results with the twice size of input.
+ * Example     : out = __lsx_vdp2_h_b(in_h, in_l)
+ *        in_h : 1,2,3,4, 5,6,7,8, 1,2,3,4, 5,6,7,8
+ *        in_l : 8,7,6,5, 4,3,2,1, 8,7,6,5, 4,3,2,1
+ *         out : 22,38,38,22, 22,38,38,22
+ * =============================================================================
+ */
+static inline __m128i __lsx_vdp2_h_b(__m128i in_h, __m128i in_l)
+{
+    __m128i out;
+
+    out = __lsx_vmulwev_h_b(in_h, in_l);
+    out = __lsx_vmaddwod_h_b(out, in_h, in_l);
+    return out;
+}
+
+/*
+ * =============================================================================
+ * Description : Dot product of byte vector elements
+ * Arguments   : Inputs  - in_h, in_l
+ *               Outputs - out
+ *               Retrun Type - halfword
+ * Details     : Unsigned byte elements from in_h are multiplied by
+ *               unsigned byte elements from in_l, and then added adjacent to
+ *               each other to get results with the twice size of input.
+ * Example     : out = __lsx_vdp2_h_bu(in_h, in_l)
+ *        in_h : 1,2,3,4, 5,6,7,8, 1,2,3,4, 5,6,7,8
+ *        in_l : 8,7,6,5, 4,3,2,1, 8,7,6,5, 4,3,2,1
+ *         out : 22,38,38,22, 22,38,38,22
+ * =============================================================================
+ */
+static inline __m128i __lsx_vdp2_h_bu(__m128i in_h, __m128i in_l)
+{
+    __m128i out;
+
+    out = __lsx_vmulwev_h_bu(in_h, in_l);
+    out = __lsx_vmaddwod_h_bu(out, in_h, in_l);
+    return out;
+}
+
+/*
+ * =============================================================================
+ * Description : Dot product of byte vector elements
+ * Arguments   : Inputs  - in_h, in_l
+ *               Outputs - out
+ *               Retrun Type - halfword
+ * Details     : Unsigned byte elements from in_h are multiplied by
+ *               signed byte elements from in_l, and then added adjacent to
+ *               each other to get results with the twice size of input.
+ * Example     : out = __lsx_vdp2_h_bu_b(in_h, in_l)
+ *        in_h : 1,2,3,4, 5,6,7,8, 1,2,3,4, 5,6,7,8
+ *        in_l : 8,7,6,5, 4,3,2,1, 8,7,6,5, 4,3,2,-1
+ *         out : 22,38,38,22, 22,38,38,6
+ * =============================================================================
+ */
+static inline __m128i __lsx_vdp2_h_bu_b(__m128i in_h, __m128i in_l)
+{
+    __m128i out;
+
+    out = __lsx_vmulwev_h_bu_b(in_h, in_l);
+    out = __lsx_vmaddwod_h_bu_b(out, in_h, in_l);
+    return out;
+}
+
+/*
+ * =============================================================================
+ * Description : Dot product of byte vector elements
+ * Arguments   : Inputs  - in_h, in_l
+ *               Outputs - out
+ *               Retrun Type - halfword
+ * Details     : Signed byte elements from in_h are multiplied by
+ *               signed byte elements from in_l, and then added adjacent to
+ *               each other to get results with the twice size of input.
+ * Example     : out = __lsx_vdp2_w_h(in_h, in_l)
+ *        in_h : 1,2,3,4, 5,6,7,8
+ *        in_l : 8,7,6,5, 4,3,2,1
+ *         out : 22,38,38,22
+ * =============================================================================
+ */
+static inline __m128i __lsx_vdp2_w_h(__m128i in_h, __m128i in_l)
+{
+    __m128i out;
+
+    out = __lsx_vmulwev_w_h(in_h, in_l);
+    out = __lsx_vmaddwod_w_h(out, in_h, in_l);
+    return out;
+}
+
+/*
+ * =============================================================================
+ * Description : Clip all halfword elements of input vector between min & max
+ *               out = ((_in) < (min)) ? (min) : (((_in) > (max)) ? (max) : (_in))
+ * Arguments   : Inputs  - _in  (input vector)
+ *                       - min  (min threshold)
+ *                       - max  (max threshold)
+ *               Outputs - out  (output vector with clipped elements)
+ *               Return Type - signed halfword
+ * Example     : out = __lsx_vclip_h(_in)
+ *         _in : -8,2,280,249, -8,255,280,249
+ *         min : 1,1,1,1, 1,1,1,1
+ *         max : 9,9,9,9, 9,9,9,9
+ *         out : 1,2,9,9, 1,9,9,9
+ * =============================================================================
+ */
+static inline __m128i __lsx_vclip_h(__m128i _in, __m128i min, __m128i max)
+{
+    __m128i out;
+
+    out = __lsx_vmax_h(min, _in);
+    out = __lsx_vmin_h(max, out);
+    return out;
+}
+
+/*
+ * =============================================================================
+ * Description : Set each element of vector between 0 and 255
+ * Arguments   : Inputs  - _in
+ *               Outputs - out
+ *               Retrun Type - halfword
+ * Details     : Signed byte elements from _in are clamped between 0 and 255.
+ * Example     : out = __lsx_vclip255_h(_in)
+ *         _in : -8,255,280,249, -8,255,280,249
+ *         out : 0,255,255,249, 0,255,255,249
+ * =============================================================================
+ */
+static inline __m128i __lsx_vclip255_h(__m128i _in)
+{
+    __m128i out;
+
+    out = __lsx_vmaxi_h(_in, 0);
+    out = __lsx_vsat_hu(out, 7);
+    return out;
+}
+
+/*
+ * =============================================================================
+ * Description : Set each element of vector between 0 and 255
+ * Arguments   : Inputs  - _in
+ *               Outputs - out
+ *               Retrun Type - word
+ * Details     : Signed byte elements from _in are clamped between 0 and 255.
+ * Example     : out = __lsx_vclip255_w(_in)
+ *         _in : -8,255,280,249
+ *         out : 0,255,255,249
+ * =============================================================================
+ */
+static inline __m128i __lsx_vclip255_w(__m128i _in)
+{
+    __m128i out;
+
+    out = __lsx_vmaxi_w(_in, 0);
+    out = __lsx_vsat_wu(out, 7);
+    return out;
+}
+
+/*
+ * =============================================================================
+ * Description : Swap two variables
+ * Arguments   : Inputs  - _in0, _in1
+ *               Outputs - _in0, _in1 (in-place)
+ * Details     : Swapping of two input variables using xor
+ * Example     : LSX_SWAP(_in0, _in1)
+ *        _in0 : 1,2,3,4
+ *        _in1 : 5,6,7,8
+ *   _in0(out) : 5,6,7,8
+ *   _in1(out) : 1,2,3,4
+ * =============================================================================
+ */
+#define LSX_SWAP(_in0, _in1)                                            \
+{                                                                       \
+    _in0 = __lsx_vxor_v(_in0, _in1);                                    \
+    _in1 = __lsx_vxor_v(_in0, _in1);                                    \
+    _in0 = __lsx_vxor_v(_in0, _in1);                                    \
+}                                                                       \
+
+/*
+ * =============================================================================
+ * Description : Transpose 4x4 block with word elements in vectors
+ * Arguments   : Inputs  - in0, in1, in2, in3
+ *               Outputs - out0, out1, out2, out3
+ * Details     :
+ * Example     :
+ *               1, 2, 3, 4            1, 5, 9,13
+ *               5, 6, 7, 8    to      2, 6,10,14
+ *               9,10,11,12  =====>    3, 7,11,15
+ *              13,14,15,16            4, 8,12,16
+ * =============================================================================
+ */
+#define LSX_TRANSPOSE4x4_W(_in0, _in1, _in2, _in3, _out0, _out1, _out2, _out3) \
+{                                                                              \
+    __m128i _t0, _t1, _t2, _t3;                                                \
+                                                                               \
+    _t0   = __lsx_vilvl_w(_in1, _in0);                                         \
+    _t1   = __lsx_vilvh_w(_in1, _in0);                                         \
+    _t2   = __lsx_vilvl_w(_in3, _in2);                                         \
+    _t3   = __lsx_vilvh_w(_in3, _in2);                                         \
+    _out0 = __lsx_vilvl_d(_t2, _t0);                                           \
+    _out1 = __lsx_vilvh_d(_t2, _t0);                                           \
+    _out2 = __lsx_vilvl_d(_t3, _t1);                                           \
+    _out3 = __lsx_vilvh_d(_t3, _t1);                                           \
+}
+
+/*
+ * =============================================================================
+ * Description : Transpose 8x8 block with byte elements in vectors
+ * Arguments   : Inputs  - _in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7
+ *               Outputs - _out0, _out1, _out2, _out3, _out4, _out5, _out6, _out7
+ * Details     : The rows of the matrix become columns, and the columns become rows.
+ * Example     : LSX_TRANSPOSE8x8_B
+ *        _in0 : 00,01,02,03,04,05,06,07, 00,00,00,00,00,00,00,00
+ *        _in1 : 10,11,12,13,14,15,16,17, 00,00,00,00,00,00,00,00
+ *        _in2 : 20,21,22,23,24,25,26,27, 00,00,00,00,00,00,00,00
+ *        _in3 : 30,31,32,33,34,35,36,37, 00,00,00,00,00,00,00,00
+ *        _in4 : 40,41,42,43,44,45,46,47, 00,00,00,00,00,00,00,00
+ *        _in5 : 50,51,52,53,54,55,56,57, 00,00,00,00,00,00,00,00
+ *        _in6 : 60,61,62,63,64,65,66,67, 00,00,00,00,00,00,00,00
+ *        _in7 : 70,71,72,73,74,75,76,77, 00,00,00,00,00,00,00,00
+ *
+ *      _ out0 : 00,10,20,30,40,50,60,70, 00,00,00,00,00,00,00,00
+ *      _ out1 : 01,11,21,31,41,51,61,71, 00,00,00,00,00,00,00,00
+ *      _ out2 : 02,12,22,32,42,52,62,72, 00,00,00,00,00,00,00,00
+ *      _ out3 : 03,13,23,33,43,53,63,73, 00,00,00,00,00,00,00,00
+ *      _ out4 : 04,14,24,34,44,54,64,74, 00,00,00,00,00,00,00,00
+ *      _ out5 : 05,15,25,35,45,55,65,75, 00,00,00,00,00,00,00,00
+ *      _ out6 : 06,16,26,36,46,56,66,76, 00,00,00,00,00,00,00,00
+ *      _ out7 : 07,17,27,37,47,57,67,77, 00,00,00,00,00,00,00,00
+ * =============================================================================
+ */
+#define LSX_TRANSPOSE8x8_B(_in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7,        \
+                           _out0, _out1, _out2, _out3, _out4, _out5, _out6, _out7)\
+{                                                                                 \
+   __m128i zero = {0};                                                            \
+   __m128i shuf8 = {0x0F0E0D0C0B0A0908, 0x1716151413121110};                      \
+   __m128i _t0, _t1, _t2, _t3, _t4, _t5, _t6, _t7;                                \
+                                                                                  \
+   _t0 = __lsx_vilvl_b(_in2, _in0);                                               \
+   _t1 = __lsx_vilvl_b(_in3, _in1);                                               \
+   _t2 = __lsx_vilvl_b(_in6, _in4);                                               \
+   _t3 = __lsx_vilvl_b(_in7, _in5);                                               \
+   _t4 = __lsx_vilvl_b(_t1, _t0);                                                 \
+   _t5 = __lsx_vilvh_b(_t1, _t0);                                                 \
+   _t6 = __lsx_vilvl_b(_t3, _t2);                                                 \
+   _t7 = __lsx_vilvh_b(_t3, _t2);                                                 \
+   _out0 = __lsx_vilvl_w(_t6, _t4);                                               \
+   _out2 = __lsx_vilvh_w(_t6, _t4);                                               \
+   _out4 = __lsx_vilvl_w(_t7, _t5);                                               \
+   _out6 = __lsx_vilvh_w(_t7, _t5);                                               \
+   _out1 = __lsx_vshuf_b(zero, _out0, shuf8);                                     \
+   _out3 = __lsx_vshuf_b(zero, _out2, shuf8);                                     \
+   _out5 = __lsx_vshuf_b(zero, _out4, shuf8);                                     \
+   _out7 = __lsx_vshuf_b(zero, _out6, shuf8);                                     \
+}
+
+/*
+ * =============================================================================
+ * Description : Transpose 8x8 block with half word elements in vectors
+ * Arguments   : Inputs  - in0, in1, in2, in3, in4, in5, in6, in7
+ *               Outputs - out0, out1, out2, out3, out4, out5, out6, out7
+ * Details     :
+ * Example     :
+ *              00,01,02,03,04,05,06,07           00,10,20,30,40,50,60,70
+ *              10,11,12,13,14,15,16,17           01,11,21,31,41,51,61,71
+ *              20,21,22,23,24,25,26,27           02,12,22,32,42,52,62,72
+ *              30,31,32,33,34,35,36,37    to     03,13,23,33,43,53,63,73
+ *              40,41,42,43,44,45,46,47  ======>  04,14,24,34,44,54,64,74
+ *              50,51,52,53,54,55,56,57           05,15,25,35,45,55,65,75
+ *              60,61,62,63,64,65,66,67           06,16,26,36,46,56,66,76
+ *              70,71,72,73,74,75,76,77           07,17,27,37,47,57,67,77
+ * =============================================================================
+ */
+#define LSX_TRANSPOSE8x8_H(_in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7,        \
+                           _out0, _out1, _out2, _out3, _out4, _out5, _out6, _out7)\
+{                                                                                 \
+    __m128i _s0, _s1, _t0, _t1, _t2, _t3, _t4, _t5, _t6, _t7;                     \
+                                                                                  \
+    _s0 = __lsx_vilvl_h(_in6, _in4);                                              \
+    _s1 = __lsx_vilvl_h(_in7, _in5);                                              \
+    _t0 = __lsx_vilvl_h(_s1, _s0);                                                \
+    _t1 = __lsx_vilvh_h(_s1, _s0);                                                \
+    _s0 = __lsx_vilvh_h(_in6, _in4);                                              \
+    _s1 = __lsx_vilvh_h(_in7, _in5);                                              \
+    _t2 = __lsx_vilvl_h(_s1, _s0);                                                \
+    _t3 = __lsx_vilvh_h(_s1, _s0);                                                \
+    _s0 = __lsx_vilvl_h(_in2, _in0);                                              \
+    _s1 = __lsx_vilvl_h(_in3, _in1);                                              \
+    _t4 = __lsx_vilvl_h(_s1, _s0);                                                \
+    _t5 = __lsx_vilvh_h(_s1, _s0);                                                \
+    _s0 = __lsx_vilvh_h(_in2, _in0);                                              \
+    _s1 = __lsx_vilvh_h(_in3, _in1);                                              \
+    _t6 = __lsx_vilvl_h(_s1, _s0);                                                \
+    _t7 = __lsx_vilvh_h(_s1, _s0);                                                \
+                                                                                  \
+    _out0 = __lsx_vpickev_d(_t0, _t4);                                            \
+    _out2 = __lsx_vpickev_d(_t1, _t5);                                            \
+    _out4 = __lsx_vpickev_d(_t2, _t6);                                            \
+    _out6 = __lsx_vpickev_d(_t3, _t7);                                            \
+    _out1 = __lsx_vpickod_d(_t0, _t4);                                            \
+    _out3 = __lsx_vpickod_d(_t1, _t5);                                            \
+    _out5 = __lsx_vpickod_d(_t2, _t6);                                            \
+    _out7 = __lsx_vpickod_d(_t3, _t7);                                            \
+}
+
+/*
+ * =============================================================================
+ * Description : Transpose input 8x4 byte block into 4x8
+ * Arguments   : Inputs  - _in0, _in1, _in2, _in3      (input 8x4 byte block)
+ *               Outputs - _out0, _out1, _out2, _out3  (output 4x8 byte block)
+ *               Return Type - as per RTYPE
+ * Details     : The rows of the matrix become columns, and the columns become rows.
+ * Example     : LSX_TRANSPOSE8x4_B
+ *        _in0 : 00,01,02,03,00,00,00,00, 00,00,00,00,00,00,00,00
+ *        _in1 : 10,11,12,13,00,00,00,00, 00,00,00,00,00,00,00,00
+ *        _in2 : 20,21,22,23,00,00,00,00, 00,00,00,00,00,00,00,00
+ *        _in3 : 30,31,32,33,00,00,00,00, 00,00,00,00,00,00,00,00
+ *        _in4 : 40,41,42,43,00,00,00,00, 00,00,00,00,00,00,00,00
+ *        _in5 : 50,51,52,53,00,00,00,00, 00,00,00,00,00,00,00,00
+ *        _in6 : 60,61,62,63,00,00,00,00, 00,00,00,00,00,00,00,00
+ *        _in7 : 70,71,72,73,00,00,00,00, 00,00,00,00,00,00,00,00
+ *
+ *       _out0 : 00,10,20,30,40,50,60,70, 00,00,00,00,00,00,00,00
+ *       _out1 : 01,11,21,31,41,51,61,71, 00,00,00,00,00,00,00,00
+ *       _out2 : 02,12,22,32,42,52,62,72, 00,00,00,00,00,00,00,00
+ *       _out3 : 03,13,23,33,43,53,63,73, 00,00,00,00,00,00,00,00
+ * =============================================================================
+ */
+#define LSX_TRANSPOSE8x4_B(_in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7,       \
+                           _out0, _out1, _out2, _out3)                           \
+{                                                                                \
+    __m128i _tmp0_m, _tmp1_m, _tmp2_m, _tmp3_m;                                  \
+                                                                                 \
+    _tmp0_m = __lsx_vpackev_w(_in4, _in0);                                       \
+    _tmp1_m = __lsx_vpackev_w(_in5, _in1);                                       \
+    _tmp2_m = __lsx_vilvl_b(_tmp1_m, _tmp0_m);                                   \
+    _tmp0_m = __lsx_vpackev_w(_in6, _in2);                                       \
+    _tmp1_m = __lsx_vpackev_w(_in7, _in3);                                       \
+                                                                                 \
+    _tmp3_m = __lsx_vilvl_b(_tmp1_m, _tmp0_m);                                   \
+    _tmp0_m = __lsx_vilvl_h(_tmp3_m, _tmp2_m);                                   \
+    _tmp1_m = __lsx_vilvh_h(_tmp3_m, _tmp2_m);                                   \
+                                                                                 \
+    _out0 = __lsx_vilvl_w(_tmp1_m, _tmp0_m);                                     \
+    _out2 = __lsx_vilvh_w(_tmp1_m, _tmp0_m);                                     \
+    _out1 = __lsx_vilvh_d(_out2, _out0);                                         \
+    _out3 = __lsx_vilvh_d(_out0, _out2);                                         \
+}
+
+/*
+ * =============================================================================
+ * Description : Transpose 16x8 block with byte elements in vectors
+ * Arguments   : Inputs  - in0, in1, in2, in3, in4, in5, in6, in7, in8
+ *                         in9, in10, in11, in12, in13, in14, in15
+ *               Outputs - out0, out1, out2, out3, out4, out5, out6, out7
+ * Details     :
+ * Example     :
+ *              000,001,002,003,004,005,006,007
+ *              008,009,010,011,012,013,014,015
+ *              016,017,018,019,020,021,022,023
+ *              024,025,026,027,028,029,030,031
+ *              032,033,034,035,036,037,038,039
+ *              040,041,042,043,044,045,046,047        000,008,...,112,120
+ *              048,049,050,051,052,053,054,055        001,009,...,113,121
+ *              056,057,058,059,060,061,062,063   to   002,010,...,114,122
+ *              064,068,066,067,068,069,070,071 =====> 003,011,...,115,123
+ *              072,073,074,075,076,077,078,079        004,012,...,116,124
+ *              080,081,082,083,084,085,086,087        005,013,...,117,125
+ *              088,089,090,091,092,093,094,095        006,014,...,118,126
+ *              096,097,098,099,100,101,102,103        007,015,...,119,127
+ *              104,105,106,107,108,109,110,111
+ *              112,113,114,115,116,117,118,119
+ *              120,121,122,123,124,125,126,127
+ * =============================================================================
+ */
+#define LSX_TRANSPOSE16x8_B(_in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7, _in8,  \
+                            _in9, _in10, _in11, _in12, _in13, _in14, _in15, _out0, \
+                            _out1, _out2, _out3, _out4, _out5, _out6, _out7)       \
+{                                                                                  \
+    __m128i _tmp0, _tmp1, _tmp2, _tmp3, _tmp4, _tmp5, _tmp6, _tmp7;                \
+    __m128i _t0, _t1, _t2, _t3, _t4, _t5, _t6, _t7;                                \
+    DUP4_ARG2(__lsx_vilvl_b, _in2, _in0, _in3, _in1, _in6, _in4, _in7, _in5,       \
+              _tmp0, _tmp1, _tmp2, _tmp3);                                         \
+    DUP4_ARG2(__lsx_vilvl_b, _in10, _in8, _in11, _in9, _in14, _in12, _in15,        \
+              _in13, _tmp4, _tmp5, _tmp6, _tmp7);                                  \
+    DUP2_ARG2(__lsx_vilvl_b, _tmp1, _tmp0, _tmp3, _tmp2, _t0, _t2);                \
+    DUP2_ARG2(__lsx_vilvh_b, _tmp1, _tmp0, _tmp3, _tmp2, _t1, _t3);                \
+    DUP2_ARG2(__lsx_vilvl_b, _tmp5, _tmp4, _tmp7, _tmp6, _t4, _t6);                \
+    DUP2_ARG2(__lsx_vilvh_b, _tmp5, _tmp4, _tmp7, _tmp6, _t5, _t7);                \
+    DUP2_ARG2(__lsx_vilvl_w, _t2, _t0, _t3, _t1, _tmp0, _tmp4);                    \
+    DUP2_ARG2(__lsx_vilvh_w, _t2, _t0, _t3, _t1, _tmp2, _tmp6);                    \
+    DUP2_ARG2(__lsx_vilvl_w, _t6, _t4, _t7, _t5, _tmp1, _tmp5);                    \
+    DUP2_ARG2(__lsx_vilvh_w, _t6, _t4, _t7, _t5, _tmp3, _tmp7);                    \
+    DUP2_ARG2(__lsx_vilvl_d, _tmp1, _tmp0, _tmp3, _tmp2, _out0, _out2);            \
+    DUP2_ARG2(__lsx_vilvh_d, _tmp1, _tmp0, _tmp3, _tmp2, _out1, _out3);            \
+    DUP2_ARG2(__lsx_vilvl_d, _tmp5, _tmp4, _tmp7, _tmp6, _out4, _out6);            \
+    DUP2_ARG2(__lsx_vilvh_d, _tmp5, _tmp4, _tmp7, _tmp6, _out5, _out7);            \
+}
+
+/*
+ * =============================================================================
+ * Description : Butterfly of 4 input vectors
+ * Arguments   : Inputs  - in0, in1, in2, in3
+ *               Outputs - out0, out1, out2, out3
+ * Details     : Butterfly operation
+ * Example     :
+ *               out0 = in0 + in3;
+ *               out1 = in1 + in2;
+ *               out2 = in1 - in2;
+ *               out3 = in0 - in3;
+ * =============================================================================
+ */
+#define LSX_BUTTERFLY_4_B(_in0, _in1, _in2, _in3, _out0, _out1, _out2, _out3) \
+{                                                                             \
+    _out0 = __lsx_vadd_b(_in0, _in3);                                         \
+    _out1 = __lsx_vadd_b(_in1, _in2);                                         \
+    _out2 = __lsx_vsub_b(_in1, _in2);                                         \
+    _out3 = __lsx_vsub_b(_in0, _in3);                                         \
+}
+#define LSX_BUTTERFLY_4_H(_in0, _in1, _in2, _in3, _out0, _out1, _out2, _out3) \
+{                                                                             \
+    _out0 = __lsx_vadd_h(_in0, _in3);                                         \
+    _out1 = __lsx_vadd_h(_in1, _in2);                                         \
+    _out2 = __lsx_vsub_h(_in1, _in2);                                         \
+    _out3 = __lsx_vsub_h(_in0, _in3);                                         \
+}
+#define LSX_BUTTERFLY_4_W(_in0, _in1, _in2, _in3, _out0, _out1, _out2, _out3) \
+{                                                                             \
+    _out0 = __lsx_vadd_w(_in0, _in3);                                         \
+    _out1 = __lsx_vadd_w(_in1, _in2);                                         \
+    _out2 = __lsx_vsub_w(_in1, _in2);                                         \
+    _out3 = __lsx_vsub_w(_in0, _in3);                                         \
+}
+#define LSX_BUTTERFLY_4_D(_in0, _in1, _in2, _in3, _out0, _out1, _out2, _out3) \
+{                                                                             \
+    _out0 = __lsx_vadd_d(_in0, _in3);                                         \
+    _out1 = __lsx_vadd_d(_in1, _in2);                                         \
+    _out2 = __lsx_vsub_d(_in1, _in2);                                         \
+    _out3 = __lsx_vsub_d(_in0, _in3);                                         \
+}
+
+/*
+ * =============================================================================
+ * Description : Butterfly of 8 input vectors
+ * Arguments   : Inputs  - _in0, _in1, _in2, _in3, ~
+ *               Outputs - _out0, _out1, _out2, _out3, ~
+ * Details     : Butterfly operation
+ * Example     :
+ *              _out0 = _in0 + _in7;
+ *              _out1 = _in1 + _in6;
+ *              _out2 = _in2 + _in5;
+ *              _out3 = _in3 + _in4;
+ *              _out4 = _in3 - _in4;
+ *              _out5 = _in2 - _in5;
+ *              _out6 = _in1 - _in6;
+ *              _out7 = _in0 - _in7;
+ * =============================================================================
+ */
+#define LSX_BUTTERFLY_8_B(_in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7,        \
+                          _out0, _out1, _out2, _out3, _out4, _out5, _out6, _out7)\
+{                                                                                \
+    _out0 = __lsx_vadd_b(_in0, _in7);                                            \
+    _out1 = __lsx_vadd_b(_in1, _in6);                                            \
+    _out2 = __lsx_vadd_b(_in2, _in5);                                            \
+    _out3 = __lsx_vadd_b(_in3, _in4);                                            \
+    _out4 = __lsx_vsub_b(_in3, _in4);                                            \
+    _out5 = __lsx_vsub_b(_in2, _in5);                                            \
+    _out6 = __lsx_vsub_b(_in1, _in6);                                            \
+    _out7 = __lsx_vsub_b(_in0, _in7);                                            \
+}
+
+#define LSX_BUTTERFLY_8_H(_in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7,        \
+                          _out0, _out1, _out2, _out3, _out4, _out5, _out6, _out7)\
+{                                                                                \
+    _out0 = __lsx_vadd_h(_in0, _in7);                                            \
+    _out1 = __lsx_vadd_h(_in1, _in6);                                            \
+    _out2 = __lsx_vadd_h(_in2, _in5);                                            \
+    _out3 = __lsx_vadd_h(_in3, _in4);                                            \
+    _out4 = __lsx_vsub_h(_in3, _in4);                                            \
+    _out5 = __lsx_vsub_h(_in2, _in5);                                            \
+    _out6 = __lsx_vsub_h(_in1, _in6);                                            \
+    _out7 = __lsx_vsub_h(_in0, _in7);                                            \
+}
+
+#define LSX_BUTTERFLY_8_W(_in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7,        \
+                          _out0, _out1, _out2, _out3, _out4, _out5, _out6, _out7)\
+{                                                                                \
+    _out0 = __lsx_vadd_w(_in0, _in7);                                            \
+    _out1 = __lsx_vadd_w(_in1, _in6);                                            \
+    _out2 = __lsx_vadd_w(_in2, _in5);                                            \
+    _out3 = __lsx_vadd_w(_in3, _in4);                                            \
+    _out4 = __lsx_vsub_w(_in3, _in4);                                            \
+    _out5 = __lsx_vsub_w(_in2, _in5);                                            \
+    _out6 = __lsx_vsub_w(_in1, _in6);                                            \
+    _out7 = __lsx_vsub_w(_in0, _in7);                                            \
+}
+
+#define LSX_BUTTERFLY_8_D(_in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7,        \
+                          _out0, _out1, _out2, _out3, _out4, _out5, _out6, _out7)\
+{                                                                                \
+    _out0 = __lsx_vadd_d(_in0, _in7);                                            \
+    _out1 = __lsx_vadd_d(_in1, _in6);                                            \
+    _out2 = __lsx_vadd_d(_in2, _in5);                                            \
+    _out3 = __lsx_vadd_d(_in3, _in4);                                            \
+    _out4 = __lsx_vsub_d(_in3, _in4);                                            \
+    _out5 = __lsx_vsub_d(_in2, _in5);                                            \
+    _out6 = __lsx_vsub_d(_in1, _in6);                                            \
+    _out7 = __lsx_vsub_d(_in0, _in7);                                            \
+}
+
+#endif //LSX
+
+#ifdef __loongarch_asx
+#include <lasxintrin.h>
+/*
+ * =============================================================================
+ * Description : Dot product of byte vector elements
+ * Arguments   : Inputs - in_h, in_l
+ *               Output - out
+ *               Return Type - signed halfword
+ * Details     : Unsigned byte elements from in_h are multiplied with
+ *               unsigned byte elements from in_l producing a result
+ *               twice the size of input i.e. signed halfword.
+ *               Then this multiplied results of adjacent odd-even elements
+ *               are added to the out vector
+ * Example     : See out = __lasx_xvdp2_w_h(in_h, in_l)
+ * =============================================================================
+ */
+static inline __m256i __lasx_xvdp2_h_bu(__m256i in_h, __m256i in_l)
+{
+    __m256i out;
+
+    out = __lasx_xvmulwev_h_bu(in_h, in_l);
+    out = __lasx_xvmaddwod_h_bu(out, in_h, in_l);
+    return out;
+}
+
+/*
+ * =============================================================================
+ * Description : Dot product of byte vector elements
+ * Arguments   : Inputs - in_h, in_l
+ *               Output - out
+ *               Return Type - signed halfword
+ * Details     : Signed byte elements from in_h are multiplied with
+ *               signed byte elements from in_l producing a result
+ *               twice the size of input i.e. signed halfword.
+ *               Then this iniplication results of adjacent odd-even elements
+ *               are added to the out vector
+ * Example     : See out = __lasx_xvdp2_w_h(in_h, in_l)
+ * =============================================================================
+ */
+static inline __m256i __lasx_xvdp2_h_b(__m256i in_h, __m256i in_l)
+{
+    __m256i out;
+
+    out = __lasx_xvmulwev_h_b(in_h, in_l);
+    out = __lasx_xvmaddwod_h_b(out, in_h, in_l);
+    return out;
+}
+
+/*
+ * =============================================================================
+ * Description : Dot product of halfword vector elements
+ * Arguments   : Inputs - in_h, in_l
+ *               Output - out
+ *               Return Type - signed word
+ * Details     : Signed halfword elements from in_h are multiplied with
+ *               signed halfword elements from in_l producing a result
+ *               twice the size of input i.e. signed word.
+ *               Then this multiplied results of adjacent odd-even elements
+ *               are added to the out vector.
+ * Example     : out = __lasx_xvdp2_w_h(in_h, in_l)
+ *        in_h : 1,2,3,4, 5,6,7,8, 1,2,3,4, 5,6,7,8
+ *        in_l : 8,7,6,5, 4,3,2,1, 8,7,6,5, 4,3,2,1
+ *         out : 22,38,38,22, 22,38,38,22
+ * =============================================================================
+ */
+static inline __m256i __lasx_xvdp2_w_h(__m256i in_h, __m256i in_l)
+{
+    __m256i out;
+
+    out = __lasx_xvmulwev_w_h(in_h, in_l);
+    out = __lasx_xvmaddwod_w_h(out, in_h, in_l);
+    return out;
+}
+
+/*
+ * =============================================================================
+ * Description : Dot product of word vector elements
+ * Arguments   : Inputs - in_h, in_l
+ *               Output - out
+ *               Retrun Type - signed double
+ * Details     : Signed word elements from in_h are multiplied with
+ *               signed word elements from in_l producing a result
+ *               twice the size of input i.e. signed double word.
+ *               Then this multiplied results of adjacent odd-even elements
+ *               are added to the out vector.
+ * Example     : See out = __lasx_xvdp2_w_h(in_h, in_l)
+ * =============================================================================
+ */
+static inline __m256i __lasx_xvdp2_d_w(__m256i in_h, __m256i in_l)
+{
+    __m256i out;
+
+    out = __lasx_xvmulwev_d_w(in_h, in_l);
+    out = __lasx_xvmaddwod_d_w(out, in_h, in_l);
+    return out;
+}
+
+/*
+ * =============================================================================
+ * Description : Dot product of halfword vector elements
+ * Arguments   : Inputs - in_h, in_l
+ *               Output - out
+ *               Return Type - signed word
+ * Details     : Unsigned halfword elements from in_h are multiplied with
+ *               signed halfword elements from in_l producing a result
+ *               twice the size of input i.e. unsigned word.
+ *               Multiplication result of adjacent odd-even elements
+ *               are added to the out vector
+ * Example     : See out = __lasx_xvdp2_w_h(in_h, in_l)
+ * =============================================================================
+ */
+static inline __m256i __lasx_xvdp2_w_hu_h(__m256i in_h, __m256i in_l)
+{
+    __m256i out;
+
+    out = __lasx_xvmulwev_w_hu_h(in_h, in_l);
+    out = __lasx_xvmaddwod_w_hu_h(out, in_h, in_l);
+    return out;
+}
+
+/*
+ * =============================================================================
+ * Description : Dot product & addition of byte vector elements
+ * Arguments   : Inputs - in_h, in_l
+ *               Output - out
+ *               Retrun Type - halfword
+ * Details     : Signed byte elements from in_h are multiplied with
+ *               signed byte elements from in_l producing a result
+ *               twice the size of input i.e. signed halfword.
+ *               Then this multiplied results of adjacent odd-even elements
+ *               are added to the in_c vector.
+ * Example     : See out = __lasx_xvdp2add_w_h(in_c, in_h, in_l)
+ * =============================================================================
+ */
+static inline __m256i __lasx_xvdp2add_h_b(__m256i in_c,__m256i in_h, __m256i in_l)
+{
+    __m256i out;
+
+    out = __lasx_xvmaddwev_h_b(in_c, in_h, in_l);
+    out = __lasx_xvmaddwod_h_b(out, in_h, in_l);
+    return out;
+}
+
+/*
+ * =============================================================================
+ * Description : Dot product of halfword vector elements
+ * Arguments   : Inputs - in_c, in_h, in_l
+ *               Output - out
+ *               Return Type - per RTYPE
+ * Details     : Signed halfword elements from in_h are multiplied with
+ *               signed halfword elements from in_l producing a result
+ *               twice the size of input i.e. signed word.
+ *               Multiplication result of adjacent odd-even elements
+ *               are added to the in_c vector.
+ * Example     : out = __lasx_xvdp2add_w_h(in_c, in_h, in_l)
+ *        in_c : 1,2,3,4, 1,2,3,4
+ *        in_h : 1,2,3,4, 5,6,7,8, 1,2,3,4, 5,6,7,8,
+ *        in_l : 8,7,6,5, 4,3,2,1, 8,7,6,5, 4,3,2,1,
+ *         out : 23,40,41,26, 23,40,41,26
+ * =============================================================================
+ */
+static inline __m256i __lasx_xvdp2add_w_h(__m256i in_c, __m256i in_h, __m256i in_l)
+{
+    __m256i out;
+
+    out = __lasx_xvmaddwev_w_h(in_c, in_h, in_l);
+    out = __lasx_xvmaddwod_w_h(out, in_h, in_l);
+    return out;
+}
+
+/*
+ * =============================================================================
+ * Description : Dot product of halfword vector elements
+ * Arguments   : Inputs - in_c, in_h, in_l
+ *               Output - out
+ *               Return Type - signed word
+ * Details     : Unsigned halfword elements from in_h are multiplied with
+ *               unsigned halfword elements from in_l producing a result
+ *               twice the size of input i.e. signed word.
+ *               Multiplication result of adjacent odd-even elements
+ *               are added to the in_c vector.
+ * Example     : See out = __lasx_xvdp2add_w_h(in_c, in_h, in_l)
+ * =============================================================================
+ */
+static inline __m256i __lasx_xvdp2add_w_hu(__m256i in_c, __m256i in_h, __m256i in_l)
+{
+    __m256i out;
+
+    out = __lasx_xvmaddwev_w_hu(in_c, in_h, in_l);
+    out = __lasx_xvmaddwod_w_hu(out, in_h, in_l);
+    return out;
+}
+
+/*
+ * =============================================================================
+ * Description : Dot product of halfword vector elements
+ * Arguments   : Inputs - in_c, in_h, in_l
+ *               Output - out
+ *               Return Type - signed word
+ * Details     : Unsigned halfword elements from in_h are multiplied with
+ *               signed halfword elements from in_l producing a result
+ *               twice the size of input i.e. signed word.
+ *               Multiplication result of adjacent odd-even elements
+ *               are added to the in_c vector
+ * Example     : See out = __lasx_xvdp2add_w_h(in_c, in_h, in_l)
+ * =============================================================================
+ */
+static inline __m256i __lasx_xvdp2add_w_hu_h(__m256i in_c, __m256i in_h, __m256i in_l)
+{
+    __m256i out;
+
+    out = __lasx_xvmaddwev_w_hu_h(in_c, in_h, in_l);
+    out = __lasx_xvmaddwod_w_hu_h(out, in_h, in_l);
+    return out;
+}
+
+/*
+ * =============================================================================
+ * Description : Vector Unsigned Dot Product and Subtract
+ * Arguments   : Inputs - in_c, in_h, in_l
+ *               Output - out
+ *               Return Type - signed halfword
+ * Details     : Unsigned byte elements from in_h are multiplied with
+ *               unsigned byte elements from in_l producing a result
+ *               twice the size of input i.e. signed halfword.
+ *               Multiplication result of adjacent odd-even elements
+ *               are added together and subtracted from double width elements
+ *               in_c vector.
+ * Example     : See out = __lasx_xvdp2sub_w_h(in_c, in_h, in_l)
+ * =============================================================================
+ */
+static inline __m256i __lasx_xvdp2sub_h_bu(__m256i in_c, __m256i in_h, __m256i in_l)
+{
+    __m256i out;
+
+    out = __lasx_xvmulwev_h_bu(in_h, in_l);
+    out = __lasx_xvmaddwod_h_bu(out, in_h, in_l);
+    out = __lasx_xvsub_h(in_c, out);
+    return out;
+}
+
+/*
+ * =============================================================================
+ * Description : Vector Signed Dot Product and Subtract
+ * Arguments   : Inputs - in_c, in_h, in_l
+ *               Output - out
+ *               Return Type - signed word
+ * Details     : Signed halfword elements from in_h are multiplied with
+ *               Signed halfword elements from in_l producing a result
+ *               twice the size of input i.e. signed word.
+ *               Multiplication result of adjacent odd-even elements
+ *               are added together and subtracted from double width elements
+ *               in_c vector.
+ * Example     : out = __lasx_xvdp2sub_w_h(in_c, in_h, in_l)
+ *        in_c : 0,0,0,0, 0,0,0,0
+ *        in_h : 3,1,3,0, 0,0,0,1, 0,0,1,1, 0,0,0,1
+ *        in_l : 2,1,1,0, 1,0,0,0, 0,0,1,0, 1,0,0,1
+ *         out : -7,-3,0,0, 0,-1,0,-1
+ * =============================================================================
+ */
+static inline __m256i __lasx_xvdp2sub_w_h(__m256i in_c, __m256i in_h, __m256i in_l)
+{
+    __m256i out;
+
+    out = __lasx_xvmulwev_w_h(in_h, in_l);
+    out = __lasx_xvmaddwod_w_h(out, in_h, in_l);
+    out = __lasx_xvsub_w(in_c, out);
+    return out;
+}
+
+/*
+ * =============================================================================
+ * Description : Dot product of halfword vector elements
+ * Arguments   : Inputs - in_h, in_l
+ *               Output - out
+ *               Return Type - signed word
+ * Details     : Signed halfword elements from in_h are iniplied with
+ *               signed halfword elements from in_l producing a result
+ *               four times the size of input i.e. signed doubleword.
+ *               Then this iniplication results of four adjacent elements
+ *               are added together and stored to the out vector.
+ * Example     : out = __lasx_xvdp4_d_h(in_h, in_l)
+ *        in_h :  3,1,3,0, 0,0,0,1, 0,0,1,-1, 0,0,0,1
+ *        in_l : -2,1,1,0, 1,0,0,0, 0,0,1, 0, 1,0,0,1
+ *         out : -2,0,1,1
+ * =============================================================================
+ */
+static inline __m256i __lasx_xvdp4_d_h(__m256i in_h, __m256i in_l)
+{
+    __m256i out;
+
+    out = __lasx_xvmulwev_w_h(in_h, in_l);
+    out = __lasx_xvmaddwod_w_h(out, in_h, in_l);
+    out = __lasx_xvhaddw_d_w(out, out);
+    return out;
+}
+
+/*
+ * =============================================================================
+ * Description : The high half of the vector elements are expanded and
+ *               added after being doubled.
+ * Arguments   : Inputs - in_h, in_l
+ *               Output - out
+ * Details     : The in_h vector and the in_l vector are added after the
+ *               higher half of the two-fold sign extension (signed byte
+ *               to signed halfword) and stored to the out vector.
+ * Example     : See out = __lasx_xvaddwh_w_h(in_h, in_l)
+ * =============================================================================
+ */
+static inline __m256i __lasx_xvaddwh_h_b(__m256i in_h, __m256i in_l)
+{
+    __m256i out;
+
+    out = __lasx_xvilvh_b(in_h, in_l);
+    out = __lasx_xvhaddw_h_b(out, out);
+    return out;
+}
+
+/*
+ * =============================================================================
+ * Description : The high half of the vector elements are expanded and
+ *               added after being doubled.
+ * Arguments   : Inputs - in_h, in_l
+ *               Output - out
+ * Details     : The in_h vector and the in_l vector are added after the
+ *               higher half of the two-fold sign extension (signed halfword
+ *               to signed word) and stored to the out vector.
+ * Example     : out = __lasx_xvaddwh_w_h(in_h, in_l)
+ *        in_h : 3, 0,3,0, 0,0,0,-1, 0,0,1,-1, 0,0,0,1
+ *        in_l : 2,-1,1,2, 1,0,0, 0, 1,0,1, 0, 1,0,0,1
+ *         out : 1,0,0,-1, 1,0,0, 2
+ * =============================================================================
+ */
+ static inline __m256i __lasx_xvaddwh_w_h(__m256i in_h, __m256i in_l)
+{
+    __m256i out;
+
+    out = __lasx_xvilvh_h(in_h, in_l);
+    out = __lasx_xvhaddw_w_h(out, out);
+    return out;
+}
+
+/*
+ * =============================================================================
+ * Description : The low half of the vector elements are expanded and
+ *               added after being doubled.
+ * Arguments   : Inputs - in_h, in_l
+ *               Output - out
+ * Details     : The in_h vector and the in_l vector are added after the
+ *               lower half of the two-fold sign extension (signed byte
+ *               to signed halfword) and stored to the out vector.
+ * Example     : See out = __lasx_xvaddwl_w_h(in_h, in_l)
+ * =============================================================================
+ */
+static inline __m256i __lasx_xvaddwl_h_b(__m256i in_h, __m256i in_l)
+{
+    __m256i out;
+
+    out = __lasx_xvilvl_b(in_h, in_l);
+    out = __lasx_xvhaddw_h_b(out, out);
+    return out;
+}
+
+/*
+ * =============================================================================
+ * Description : The low half of the vector elements are expanded and
+ *               added after being doubled.
+ * Arguments   : Inputs - in_h, in_l
+ *               Output - out
+ * Details     : The in_h vector and the in_l vector are added after the
+ *               lower half of the two-fold sign extension (signed halfword
+ *               to signed word) and stored to the out vector.
+ * Example     : out = __lasx_xvaddwl_w_h(in_h, in_l)
+ *        in_h : 3, 0,3,0, 0,0,0,-1, 0,0,1,-1, 0,0,0,1
+ *        in_l : 2,-1,1,2, 1,0,0, 0, 1,0,1, 0, 1,0,0,1
+ *         out : 5,-1,4,2, 1,0,2,-1
+ * =============================================================================
+ */
+static inline __m256i __lasx_xvaddwl_w_h(__m256i in_h, __m256i in_l)
+{
+    __m256i out;
+
+    out = __lasx_xvilvl_h(in_h, in_l);
+    out = __lasx_xvhaddw_w_h(out, out);
+    return out;
+}
+
+/*
+ * =============================================================================
+ * Description : The low half of the vector elements are expanded and
+ *               added after being doubled.
+ * Arguments   : Inputs - in_h, in_l
+ *               Output - out
+ * Details     : The out vector and the out vector are added after the
+ *               lower half of the two-fold zero extension (unsigned byte
+ *               to unsigned halfword) and stored to the out vector.
+ * Example     : See out = __lasx_xvaddwl_w_h(in_h, in_l)
+ * =============================================================================
+ */
+static inline __m256i __lasx_xvaddwl_h_bu(__m256i in_h, __m256i in_l)
+{
+    __m256i out;
+
+    out = __lasx_xvilvl_b(in_h, in_l);
+    out = __lasx_xvhaddw_hu_bu(out, out);
+    return out;
+}
+
+/*
+ * =============================================================================
+ * Description : The low half of the vector elements are expanded and
+ *               added after being doubled.
+ * Arguments   : Inputs - in_h, in_l
+ *               Output - out
+ * Details     : The in_l vector after double zero extension (unsigned byte to
+ *               signed halfword)added to the in_h vector.
+ * Example     : See out = __lasx_xvaddw_w_w_h(in_h, in_l)
+ * =============================================================================
+ */
+static inline __m256i __lasx_xvaddw_h_h_bu(__m256i in_h, __m256i in_l)
+{
+    __m256i out;
+
+    out = __lasx_xvsllwil_hu_bu(in_l, 0);
+    out = __lasx_xvadd_h(in_h, out);
+    return out;
+}
+
+/*
+ * =============================================================================
+ * Description : The low half of the vector elements are expanded and
+ *               added after being doubled.
+ * Arguments   : Inputs - in_h, in_l
+ *               Output - out
+ * Details     : The in_l vector after double sign extension (signed halfword to
+ *               signed word), added to the in_h vector.
+ * Example     : out = __lasx_xvaddw_w_w_h(in_h, in_l)
+ *        in_h : 0, 1,0,0, -1,0,0,1,
+ *        in_l : 2,-1,1,2,  1,0,0,0, 0,0,1,0, 1,0,0,1,
+ *         out : 2, 0,1,2, -1,0,1,1,
+ * =============================================================================
+ */
+static inline __m256i __lasx_xvaddw_w_w_h(__m256i in_h, __m256i in_l)
+{
+    __m256i out;
+
+    out = __lasx_xvsllwil_w_h(in_l, 0);
+    out = __lasx_xvadd_w(in_h, out);
+    return out;
+}
+
+/*
+ * =============================================================================
+ * Description : Multiplication and addition calculation after expansion
+ *               of the lower half of the vector.
+ * Arguments   : Inputs - in_c, in_h, in_l
+ *               Output - out
+ * Details     : The in_h vector and the in_l vector are multiplied after
+ *               the lower half of the two-fold sign extension (signed halfword
+ *               to signed word), and the result is added to the vector in_c,
+ *               then stored to the out vector.
+ * Example     : out = __lasx_xvmaddwl_w_h(in_c, in_h, in_l)
+ *        in_c : 1,2,3,4, 5,6,7,8
+ *        in_h : 1,2,3,4, 1,2,3,4, 5,6,7,8, 5,6,7,8
+ *        in_l : 200, 300, 400, 500,  2000, 3000, 4000, 5000,
+ *              -200,-300,-400,-500, -2000,-3000,-4000,-5000
+ *         out : 201, 602,1203,2004, -995, -1794,-2793,-3992
+ * =============================================================================
+ */
+static inline __m256i __lasx_xvmaddwl_w_h(__m256i in_c, __m256i in_h, __m256i in_l)
+{
+    __m256i tmp0, tmp1, out;
+
+    tmp0 = __lasx_xvsllwil_w_h(in_h, 0);
+    tmp1 = __lasx_xvsllwil_w_h(in_l, 0);
+    tmp0 = __lasx_xvmul_w(tmp0, tmp1);
+    out  = __lasx_xvadd_w(tmp0, in_c);
+    return out;
+}
+
+/*
+ * =============================================================================
+ * Description : Multiplication and addition calculation after expansion
+ *               of the higher half of the vector.
+ * Arguments   : Inputs - in_c, in_h, in_l
+ *               Output - out
+ * Details     : The in_h vector and the in_l vector are multiplied after
+ *               the higher half of the two-fold sign extension (signed
+ *               halfword to signed word), and the result is added to
+ *               the vector in_c, then stored to the out vector.
+ * Example     : See out = __lasx_xvmaddwl_w_h(in_c, in_h, in_l)
+ * =============================================================================
+ */
+static inline __m256i __lasx_xvmaddwh_w_h(__m256i in_c, __m256i in_h, __m256i in_l)
+{
+    __m256i tmp0, tmp1, out;
+
+    tmp0 = __lasx_xvilvh_h(in_h, in_h);
+    tmp1 = __lasx_xvilvh_h(in_l, in_l);
+    tmp0 = __lasx_xvmulwev_w_h(tmp0, tmp1);
+    out  = __lasx_xvadd_w(tmp0, in_c);
+    return out;
+}
+
+/*
+ * =============================================================================
+ * Description : Multiplication calculation after expansion of the lower
+ *               half of the vector.
+ * Arguments   : Inputs - in_h, in_l
+ *               Output - out
+ * Details     : The in_h vector and the in_l vector are multiplied after
+ *               the lower half of the two-fold sign extension (signed
+ *               halfword to signed word), then stored to the out vector.
+ * Example     : out = __lasx_xvmulwl_w_h(in_h, in_l)
+ *        in_h : 3,-1,3,0, 0,0,0,-1, 0,0,1,-1, 0,0,0,1
+ *        in_l : 2,-1,1,2, 1,0,0, 0, 0,0,1, 0, 1,0,0,1
+ *         out : 6,1,3,0, 0,0,1,0
+ * =============================================================================
+ */
+static inline __m256i __lasx_xvmulwl_w_h(__m256i in_h, __m256i in_l)
+{
+    __m256i tmp0, tmp1, out;
+
+    tmp0 = __lasx_xvsllwil_w_h(in_h, 0);
+    tmp1 = __lasx_xvsllwil_w_h(in_l, 0);
+    out  = __lasx_xvmul_w(tmp0, tmp1);
+    return out;
+}
+
+/*
+ * =============================================================================
+ * Description : Multiplication calculation after expansion of the lower
+ *               half of the vector.
+ * Arguments   : Inputs - in_h, in_l
+ *               Output - out
+ * Details     : The in_h vector and the in_l vector are multiplied after
+ *               the lower half of the two-fold sign extension (signed
+ *               halfword to signed word), then stored to the out vector.
+ * Example     : out = __lasx_xvmulwh_w_h(in_h, in_l)
+ *        in_h : 3,-1,3,0, 0,0,0,-1, 0,0,1,-1, 0,0,0,1
+ *        in_l : 2,-1,1,2, 1,0,0, 0, 0,0,1, 0, 1,0,0,1
+ *         out : 0,0,0,0, 0,0,0,1
+ * =============================================================================
+ */
+static inline __m256i __lasx_xvmulwh_w_h(__m256i in_h, __m256i in_l)
+{
+    __m256i tmp0, tmp1, out;
+
+    tmp0 = __lasx_xvilvh_h(in_h, in_h);
+    tmp1 = __lasx_xvilvh_h(in_l, in_l);
+    out  = __lasx_xvmulwev_w_h(tmp0, tmp1);
+    return out;
+}
+
+/*
+ * =============================================================================
+ * Description : The low half of the vector elements are expanded and
+ *               added saturately after being doubled.
+ * Arguments   : Inputs - in_h, in_l
+ *               Output - out
+ * Details     : The in_h vector adds the in_l vector saturately after the lower
+ *               half of the two-fold zero extension (unsigned byte to unsigned
+ *               halfword) and the results are stored to the out vector.
+ * Example     : out = __lasx_xvsaddw_hu_hu_bu(in_h, in_l)
+ *        in_h : 2,65532,1,2, 1,0,0,0, 0,0,1,0, 1,0,0,1
+ *        in_l : 3,6,3,0, 0,0,0,1, 0,0,1,1, 0,0,0,1, 3,18,3,0, 0,0,0,1, 0,0,1,1, 0,0,0,1
+ *         out : 5,65535,4,2, 1,0,0,1, 3,18,4,0, 1,0,0,2,
+ * =============================================================================
+ */
+static inline __m256i __lasx_xvsaddw_hu_hu_bu(__m256i in_h, __m256i in_l)
+{
+    __m256i tmp1, out;
+    __m256i zero = {0};
+
+    tmp1 = __lasx_xvilvl_b(zero, in_l);
+    out  = __lasx_xvsadd_hu(in_h, tmp1);
+    return out;
+}
+
+/*
+ * =============================================================================
+ * Description : Clip all halfword elements of input vector between min & max
+ *               out = ((in) < (min)) ? (min) : (((in) > (max)) ? (max) : (in))
+ * Arguments   : Inputs  - in    (input vector)
+ *                       - min   (min threshold)
+ *                       - max   (max threshold)
+ *               Outputs - in    (output vector with clipped elements)
+ *               Return Type - signed halfword
+ * Example     : out = __lasx_xvclip_h(in, min, max)
+ *          in : -8,2,280,249, -8,255,280,249, 4,4,4,4, 5,5,5,5
+ *         min : 1,1,1,1, 1,1,1,1, 1,1,1,1, 1,1,1,1
+ *         max : 9,9,9,9, 9,9,9,9, 9,9,9,9, 9,9,9,9
+ *         out : 1,2,9,9, 1,9,9,9, 4,4,4,4, 5,5,5,5
+ * =============================================================================
+ */
+static inline __m256i __lasx_xvclip_h(__m256i in, __m256i min, __m256i max)
+{
+    __m256i out;
+
+    out = __lasx_xvmax_h(min, in);
+    out = __lasx_xvmin_h(max, out);
+    return out;
+}
+
+/*
+ * =============================================================================
+ * Description : Clip all signed halfword elements of input vector
+ *               between 0 & 255
+ * Arguments   : Inputs  - in   (input vector)
+ *               Outputs - out  (output vector with clipped elements)
+ *               Return Type - signed halfword
+ * Example     : See out = __lasx_xvclamp255_w(in)
+ * =============================================================================
+ */
+static inline __m256i __lasx_xvclip255_h(__m256i in)
+{
+    __m256i out;
+
+    out = __lasx_xvmaxi_h(in, 0);
+    out = __lasx_xvsat_hu(out, 7);
+    return out;
+}
+
+/*
+ * =============================================================================
+ * Description : Clip all signed word elements of input vector
+ *               between 0 & 255
+ * Arguments   : Inputs - in   (input vector)
+ *               Output - out  (output vector with clipped elements)
+ *               Return Type - signed word
+ * Example     : out = __lasx_xvclamp255_w(in)
+ *          in : -8,255,280,249, -8,255,280,249
+ *         out :  0,255,255,249,  0,255,255,249
+ * =============================================================================
+ */
+static inline __m256i __lasx_xvclip255_w(__m256i in)
+{
+    __m256i out;
+
+    out = __lasx_xvmaxi_w(in, 0);
+    out = __lasx_xvsat_wu(out, 7);
+    return out;
+}
+
+/*
+ * =============================================================================
+ * Description : Indexed halfword element values are replicated to all
+ *               elements in output vector. If 'indx < 8' use xvsplati_l_*,
+ *               if 'indx >= 8' use xvsplati_h_*.
+ * Arguments   : Inputs - in, idx
+ *               Output - out
+ * Details     : Idx element value from in vector is replicated to all
+ *               elements in out vector.
+ *               Valid index range for halfword operation is 0-7
+ * Example     : out = __lasx_xvsplati_l_h(in, idx)
+ *          in : 20,10,11,12, 13,14,15,16, 0,0,2,0, 0,0,0,0
+ *         idx : 0x02
+ *         out : 11,11,11,11, 11,11,11,11, 11,11,11,11, 11,11,11,11
+ * =============================================================================
+ */
+static inline __m256i __lasx_xvsplati_l_h(__m256i in, int idx)
+{
+    __m256i out;
+
+    out = __lasx_xvpermi_q(in, in, 0x02);
+    out = __lasx_xvreplve_h(out, idx);
+    return out;
+}
+
+/*
+ * =============================================================================
+ * Description : Indexed halfword element values are replicated to all
+ *               elements in output vector. If 'indx < 8' use xvsplati_l_*,
+ *               if 'indx >= 8' use xvsplati_h_*.
+ * Arguments   : Inputs - in, idx
+ *               Output - out
+ * Details     : Idx element value from in vector is replicated to all
+ *               elements in out vector.
+ *               Valid index range for halfword operation is 0-7
+ * Example     : out = __lasx_xvsplati_h_h(in, idx)
+ *          in : 20,10,11,12, 13,14,15,16, 0,2,0,0, 0,0,0,0
+ *         idx : 0x09
+ *         out : 2,2,2,2, 2,2,2,2, 2,2,2,2, 2,2,2,2
+ * =============================================================================
+ */
+static inline __m256i __lasx_xvsplati_h_h(__m256i in, int idx)
+{
+    __m256i out;
+
+    out = __lasx_xvpermi_q(in, in, 0x13);
+    out = __lasx_xvreplve_h(out, idx);
+    return out;
+}
+
+/*
+ * =============================================================================
+ * Description : Transpose 4x4 block with double word elements in vectors
+ * Arguments   : Inputs  - _in0, _in1, _in2, _in3
+ *               Outputs - _out0, _out1, _out2, _out3
+ * Example     : LASX_TRANSPOSE4x4_D
+ *         _in0 : 1,2,3,4
+ *         _in1 : 1,2,3,4
+ *         _in2 : 1,2,3,4
+ *         _in3 : 1,2,3,4
+ *
+ *        _out0 : 1,1,1,1
+ *        _out1 : 2,2,2,2
+ *        _out2 : 3,3,3,3
+ *        _out3 : 4,4,4,4
+ * =============================================================================
+ */
+#define LASX_TRANSPOSE4x4_D(_in0, _in1, _in2, _in3, _out0, _out1, _out2, _out3) \
+{                                                                               \
+    __m256i _tmp0, _tmp1, _tmp2, _tmp3;                                         \
+    _tmp0 = __lasx_xvilvl_d(_in1, _in0);                                        \
+    _tmp1 = __lasx_xvilvh_d(_in1, _in0);                                        \
+    _tmp2 = __lasx_xvilvl_d(_in3, _in2);                                        \
+    _tmp3 = __lasx_xvilvh_d(_in3, _in2);                                        \
+    _out0 = __lasx_xvpermi_q(_tmp2, _tmp0, 0x20);                               \
+    _out2 = __lasx_xvpermi_q(_tmp2, _tmp0, 0x31);                               \
+    _out1 = __lasx_xvpermi_q(_tmp3, _tmp1, 0x20);                               \
+    _out3 = __lasx_xvpermi_q(_tmp3, _tmp1, 0x31);                               \
+}
+
+/*
+ * =============================================================================
+ * Description : Transpose 8x8 block with word elements in vectors
+ * Arguments   : Inputs  - _in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7
+ *               Outputs - _out0, _out1, _out2, _out3, _out4, _out5, _out6, _out7
+ * Example     : LASX_TRANSPOSE8x8_W
+ *         _in0 : 1,2,3,4,5,6,7,8
+ *         _in1 : 2,2,3,4,5,6,7,8
+ *         _in2 : 3,2,3,4,5,6,7,8
+ *         _in3 : 4,2,3,4,5,6,7,8
+ *         _in4 : 5,2,3,4,5,6,7,8
+ *         _in5 : 6,2,3,4,5,6,7,8
+ *         _in6 : 7,2,3,4,5,6,7,8
+ *         _in7 : 8,2,3,4,5,6,7,8
+ *
+ *        _out0 : 1,2,3,4,5,6,7,8
+ *        _out1 : 2,2,2,2,2,2,2,2
+ *        _out2 : 3,3,3,3,3,3,3,3
+ *        _out3 : 4,4,4,4,4,4,4,4
+ *        _out4 : 5,5,5,5,5,5,5,5
+ *        _out5 : 6,6,6,6,6,6,6,6
+ *        _out6 : 7,7,7,7,7,7,7,7
+ *        _out7 : 8,8,8,8,8,8,8,8
+ * =============================================================================
+ */
+#define LASX_TRANSPOSE8x8_W(_in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7,         \
+                            _out0, _out1, _out2, _out3, _out4, _out5, _out6, _out7) \
+{                                                                                   \
+    __m256i _s0_m, _s1_m;                                                           \
+    __m256i _tmp0_m, _tmp1_m, _tmp2_m, _tmp3_m;                                     \
+    __m256i _tmp4_m, _tmp5_m, _tmp6_m, _tmp7_m;                                     \
+                                                                                    \
+    _s0_m   = __lasx_xvilvl_w(_in2, _in0);                                          \
+    _s1_m   = __lasx_xvilvl_w(_in3, _in1);                                          \
+    _tmp0_m = __lasx_xvilvl_w(_s1_m, _s0_m);                                        \
+    _tmp1_m = __lasx_xvilvh_w(_s1_m, _s0_m);                                        \
+    _s0_m   = __lasx_xvilvh_w(_in2, _in0);                                          \
+    _s1_m   = __lasx_xvilvh_w(_in3, _in1);                                          \
+    _tmp2_m = __lasx_xvilvl_w(_s1_m, _s0_m);                                        \
+    _tmp3_m = __lasx_xvilvh_w(_s1_m, _s0_m);                                        \
+    _s0_m   = __lasx_xvilvl_w(_in6, _in4);                                          \
+    _s1_m   = __lasx_xvilvl_w(_in7, _in5);                                          \
+    _tmp4_m = __lasx_xvilvl_w(_s1_m, _s0_m);                                        \
+    _tmp5_m = __lasx_xvilvh_w(_s1_m, _s0_m);                                        \
+    _s0_m   = __lasx_xvilvh_w(_in6, _in4);                                          \
+    _s1_m   = __lasx_xvilvh_w(_in7, _in5);                                          \
+    _tmp6_m = __lasx_xvilvl_w(_s1_m, _s0_m);                                        \
+    _tmp7_m = __lasx_xvilvh_w(_s1_m, _s0_m);                                        \
+    _out0 = __lasx_xvpermi_q(_tmp4_m, _tmp0_m, 0x20);                               \
+    _out1 = __lasx_xvpermi_q(_tmp5_m, _tmp1_m, 0x20);                               \
+    _out2 = __lasx_xvpermi_q(_tmp6_m, _tmp2_m, 0x20);                               \
+    _out3 = __lasx_xvpermi_q(_tmp7_m, _tmp3_m, 0x20);                               \
+    _out4 = __lasx_xvpermi_q(_tmp4_m, _tmp0_m, 0x31);                               \
+    _out5 = __lasx_xvpermi_q(_tmp5_m, _tmp1_m, 0x31);                               \
+    _out6 = __lasx_xvpermi_q(_tmp6_m, _tmp2_m, 0x31);                               \
+    _out7 = __lasx_xvpermi_q(_tmp7_m, _tmp3_m, 0x31);                               \
+}
+
+/*
+ * =============================================================================
+ * Description : Transpose input 16x8 byte block
+ * Arguments   : Inputs  - _in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7,
+ *                         _in8, _in9, _in10, _in11, _in12, _in13, _in14, _in15
+ *                         (input 16x8 byte block)
+ *               Outputs - _out0, _out1, _out2, _out3, _out4, _out5, _out6, _out7
+ *                         (output 8x16 byte block)
+ * Details     : The rows of the matrix become columns, and the columns become rows.
+ * Example     : See LASX_TRANSPOSE16x8_H
+ * =============================================================================
+ */
+#define LASX_TRANSPOSE16x8_B(_in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7,         \
+                             _in8, _in9, _in10, _in11, _in12, _in13, _in14, _in15,   \
+                             _out0, _out1, _out2, _out3, _out4, _out5, _out6, _out7) \
+{                                                                                    \
+    __m256i _tmp0_m, _tmp1_m, _tmp2_m, _tmp3_m;                                      \
+    __m256i _tmp4_m, _tmp5_m, _tmp6_m, _tmp7_m;                                      \
+                                                                                     \
+    _tmp0_m = __lasx_xvilvl_b(_in2, _in0);                                           \
+    _tmp1_m = __lasx_xvilvl_b(_in3, _in1);                                           \
+    _tmp2_m = __lasx_xvilvl_b(_in6, _in4);                                           \
+    _tmp3_m = __lasx_xvilvl_b(_in7, _in5);                                           \
+    _tmp4_m = __lasx_xvilvl_b(_in10, _in8);                                          \
+    _tmp5_m = __lasx_xvilvl_b(_in11, _in9);                                          \
+    _tmp6_m = __lasx_xvilvl_b(_in14, _in12);                                         \
+    _tmp7_m = __lasx_xvilvl_b(_in15, _in13);                                         \
+    _out0 = __lasx_xvilvl_b(_tmp1_m, _tmp0_m);                                       \
+    _out1 = __lasx_xvilvh_b(_tmp1_m, _tmp0_m);                                       \
+    _out2 = __lasx_xvilvl_b(_tmp3_m, _tmp2_m);                                       \
+    _out3 = __lasx_xvilvh_b(_tmp3_m, _tmp2_m);                                       \
+    _out4 = __lasx_xvilvl_b(_tmp5_m, _tmp4_m);                                       \
+    _out5 = __lasx_xvilvh_b(_tmp5_m, _tmp4_m);                                       \
+    _out6 = __lasx_xvilvl_b(_tmp7_m, _tmp6_m);                                       \
+    _out7 = __lasx_xvilvh_b(_tmp7_m, _tmp6_m);                                       \
+    _tmp0_m = __lasx_xvilvl_w(_out2, _out0);                                         \
+    _tmp2_m = __lasx_xvilvh_w(_out2, _out0);                                         \
+    _tmp4_m = __lasx_xvilvl_w(_out3, _out1);                                         \
+    _tmp6_m = __lasx_xvilvh_w(_out3, _out1);                                         \
+    _tmp1_m = __lasx_xvilvl_w(_out6, _out4);                                         \
+    _tmp3_m = __lasx_xvilvh_w(_out6, _out4);                                         \
+    _tmp5_m = __lasx_xvilvl_w(_out7, _out5);                                         \
+    _tmp7_m = __lasx_xvilvh_w(_out7, _out5);                                         \
+    _out0 = __lasx_xvilvl_d(_tmp1_m, _tmp0_m);                                       \
+    _out1 = __lasx_xvilvh_d(_tmp1_m, _tmp0_m);                                       \
+    _out2 = __lasx_xvilvl_d(_tmp3_m, _tmp2_m);                                       \
+    _out3 = __lasx_xvilvh_d(_tmp3_m, _tmp2_m);                                       \
+    _out4 = __lasx_xvilvl_d(_tmp5_m, _tmp4_m);                                       \
+    _out5 = __lasx_xvilvh_d(_tmp5_m, _tmp4_m);                                       \
+    _out6 = __lasx_xvilvl_d(_tmp7_m, _tmp6_m);                                       \
+    _out7 = __lasx_xvilvh_d(_tmp7_m, _tmp6_m);                                       \
+}
+
+/*
+ * =============================================================================
+ * Description : Transpose input 16x8 byte block
+ * Arguments   : Inputs  - _in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7,
+ *                         _in8, _in9, _in10, _in11, _in12, _in13, _in14, _in15
+ *                         (input 16x8 byte block)
+ *               Outputs - _out0, _out1, _out2, _out3, _out4, _out5, _out6, _out7
+ *                         (output 8x16 byte block)
+ * Details     : The rows of the matrix become columns, and the columns become rows.
+ * Example     : LASX_TRANSPOSE16x8_H
+ *        _in0 : 1,2,3,4,5,6,7,8,0,0,0,0,0,0,0,0
+ *        _in1 : 2,2,3,4,5,6,7,8,0,0,0,0,0,0,0,0
+ *        _in2 : 3,2,3,4,5,6,7,8,0,0,0,0,0,0,0,0
+ *        _in3 : 4,2,3,4,5,6,7,8,0,0,0,0,0,0,0,0
+ *        _in4 : 5,2,3,4,5,6,7,8,0,0,0,0,0,0,0,0
+ *        _in5 : 6,2,3,4,5,6,7,8,0,0,0,0,0,0,0,0
+ *        _in6 : 7,2,3,4,5,6,7,8,0,0,0,0,0,0,0,0
+ *        _in7 : 8,2,3,4,5,6,7,8,0,0,0,0,0,0,0,0
+ *        _in8 : 9,2,3,4,5,6,7,8,0,0,0,0,0,0,0,0
+ *        _in9 : 1,2,3,4,5,6,7,8,0,0,0,0,0,0,0,0
+ *       _in10 : 0,2,3,4,5,6,7,8,0,0,0,0,0,0,0,0
+ *       _in11 : 2,2,3,4,5,6,7,8,0,0,0,0,0,0,0,0
+ *       _in12 : 3,2,3,4,5,6,7,8,0,0,0,0,0,0,0,0
+ *       _in13 : 7,2,3,4,5,6,7,8,0,0,0,0,0,0,0,0
+ *       _in14 : 5,2,3,4,5,6,7,8,0,0,0,0,0,0,0,0
+ *       _in15 : 6,2,3,4,5,6,7,8,0,0,0,0,0,0,0,0
+ *
+ *       _out0 : 1,2,3,4,5,6,7,8,9,1,0,2,3,7,5,6
+ *       _out1 : 2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2
+ *       _out2 : 3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3
+ *       _out3 : 4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4
+ *       _out4 : 5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5
+ *       _out5 : 6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6
+ *       _out6 : 7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7
+ *       _out7 : 8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8
+ * =============================================================================
+ */
+#define LASX_TRANSPOSE16x8_H(_in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7,         \
+                             _in8, _in9, _in10, _in11, _in12, _in13, _in14, _in15,   \
+                             _out0, _out1, _out2, _out3, _out4, _out5, _out6, _out7) \
+   {                                                                                 \
+    __m256i _tmp0_m, _tmp1_m, _tmp2_m, _tmp3_m;                                      \
+    __m256i _tmp4_m, _tmp5_m, _tmp6_m, _tmp7_m;                                      \
+    __m256i _t0, _t1, _t2, _t3, _t4, _t5, _t6, _t7;                                  \
+                                                                                     \
+    _tmp0_m = __lasx_xvilvl_h(_in2, _in0);                                           \
+    _tmp1_m = __lasx_xvilvl_h(_in3, _in1);                                           \
+    _tmp2_m = __lasx_xvilvl_h(_in6, _in4);                                           \
+    _tmp3_m = __lasx_xvilvl_h(_in7, _in5);                                           \
+    _tmp4_m = __lasx_xvilvl_h(_in10, _in8);                                          \
+    _tmp5_m = __lasx_xvilvl_h(_in11, _in9);                                          \
+    _tmp6_m = __lasx_xvilvl_h(_in14, _in12);                                         \
+    _tmp7_m = __lasx_xvilvl_h(_in15, _in13);                                         \
+    _t0 = __lasx_xvilvl_h(_tmp1_m, _tmp0_m);                                         \
+    _t1 = __lasx_xvilvh_h(_tmp1_m, _tmp0_m);                                         \
+    _t2 = __lasx_xvilvl_h(_tmp3_m, _tmp2_m);                                         \
+    _t3 = __lasx_xvilvh_h(_tmp3_m, _tmp2_m);                                         \
+    _t4 = __lasx_xvilvl_h(_tmp5_m, _tmp4_m);                                         \
+    _t5 = __lasx_xvilvh_h(_tmp5_m, _tmp4_m);                                         \
+    _t6 = __lasx_xvilvl_h(_tmp7_m, _tmp6_m);                                         \
+    _t7 = __lasx_xvilvh_h(_tmp7_m, _tmp6_m);                                         \
+    _tmp0_m = __lasx_xvilvl_d(_t2, _t0);                                             \
+    _tmp2_m = __lasx_xvilvh_d(_t2, _t0);                                             \
+    _tmp4_m = __lasx_xvilvl_d(_t3, _t1);                                             \
+    _tmp6_m = __lasx_xvilvh_d(_t3, _t1);                                             \
+    _tmp1_m = __lasx_xvilvl_d(_t6, _t4);                                             \
+    _tmp3_m = __lasx_xvilvh_d(_t6, _t4);                                             \
+    _tmp5_m = __lasx_xvilvl_d(_t7, _t5);                                             \
+    _tmp7_m = __lasx_xvilvh_d(_t7, _t5);                                             \
+    _out0 = __lasx_xvpermi_q(_tmp1_m, _tmp0_m, 0x20);                                \
+    _out1 = __lasx_xvpermi_q(_tmp3_m, _tmp2_m, 0x20);                                \
+    _out2 = __lasx_xvpermi_q(_tmp5_m, _tmp4_m, 0x20);                                \
+    _out3 = __lasx_xvpermi_q(_tmp7_m, _tmp6_m, 0x20);                                \
+                                                                                     \
+    _tmp0_m = __lasx_xvilvh_h(_in2, _in0);                                           \
+    _tmp1_m = __lasx_xvilvh_h(_in3, _in1);                                           \
+    _tmp2_m = __lasx_xvilvh_h(_in6, _in4);                                           \
+    _tmp3_m = __lasx_xvilvh_h(_in7, _in5);                                           \
+    _tmp4_m = __lasx_xvilvh_h(_in10, _in8);                                          \
+    _tmp5_m = __lasx_xvilvh_h(_in11, _in9);                                          \
+    _tmp6_m = __lasx_xvilvh_h(_in14, _in12);                                         \
+    _tmp7_m = __lasx_xvilvh_h(_in15, _in13);                                         \
+    _t0 = __lasx_xvilvl_h(_tmp1_m, _tmp0_m);                                         \
+    _t1 = __lasx_xvilvh_h(_tmp1_m, _tmp0_m);                                         \
+    _t2 = __lasx_xvilvl_h(_tmp3_m, _tmp2_m);                                         \
+    _t3 = __lasx_xvilvh_h(_tmp3_m, _tmp2_m);                                         \
+    _t4 = __lasx_xvilvl_h(_tmp5_m, _tmp4_m);                                         \
+    _t5 = __lasx_xvilvh_h(_tmp5_m, _tmp4_m);                                         \
+    _t6 = __lasx_xvilvl_h(_tmp7_m, _tmp6_m);                                         \
+    _t7 = __lasx_xvilvh_h(_tmp7_m, _tmp6_m);                                         \
+    _tmp0_m = __lasx_xvilvl_d(_t2, _t0);                                             \
+    _tmp2_m = __lasx_xvilvh_d(_t2, _t0);                                             \
+    _tmp4_m = __lasx_xvilvl_d(_t3, _t1);                                             \
+    _tmp6_m = __lasx_xvilvh_d(_t3, _t1);                                             \
+    _tmp1_m = __lasx_xvilvl_d(_t6, _t4);                                             \
+    _tmp3_m = __lasx_xvilvh_d(_t6, _t4);                                             \
+    _tmp5_m = __lasx_xvilvl_d(_t7, _t5);                                             \
+    _tmp7_m = __lasx_xvilvh_d(_t7, _t5);                                             \
+    _out4 = __lasx_xvpermi_q(_tmp1_m, _tmp0_m, 0x20);                                \
+    _out5 = __lasx_xvpermi_q(_tmp3_m, _tmp2_m, 0x20);                                \
+    _out6 = __lasx_xvpermi_q(_tmp5_m, _tmp4_m, 0x20);                                \
+    _out7 = __lasx_xvpermi_q(_tmp7_m, _tmp6_m, 0x20);                                \
+}
+
+/*
+ * =============================================================================
+ * Description : Transpose 4x4 block with halfword elements in vectors
+ * Arguments   : Inputs  - _in0, _in1, _in2, _in3
+ *               Outputs - _out0, _out1, _out2, _out3
+ *               Return Type - signed halfword
+ * Details     : The rows of the matrix become columns, and the columns become rows.
+ * Example     : See LASX_TRANSPOSE8x8_H
+ * =============================================================================
+ */
+#define LASX_TRANSPOSE4x4_H(_in0, _in1, _in2, _in3, _out0, _out1, _out2, _out3)     \
+{                                                                                   \
+    __m256i _s0_m, _s1_m;                                                           \
+                                                                                    \
+    _s0_m = __lasx_xvilvl_h(_in1, _in0);                                            \
+    _s1_m = __lasx_xvilvl_h(_in3, _in2);                                            \
+    _out0 = __lasx_xvilvl_w(_s1_m, _s0_m);                                          \
+    _out2 = __lasx_xvilvh_w(_s1_m, _s0_m);                                          \
+    _out1 = __lasx_xvilvh_d(_out0, _out0);                                          \
+    _out3 = __lasx_xvilvh_d(_out2, _out2);                                          \
+}
+
+/*
+ * =============================================================================
+ * Description : Transpose input 8x8 byte block
+ * Arguments   : Inputs  - _in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7
+ *                         (input 8x8 byte block)
+ *               Outputs - _out0, _out1, _out2, _out3, _out4, _out5, _out6, _out7
+ *                         (output 8x8 byte block)
+ * Example     : See LASX_TRANSPOSE8x8_H
+ * =============================================================================
+ */
+#define LASX_TRANSPOSE8x8_B(_in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7, _out0,  \
+                            _out1, _out2, _out3, _out4, _out5, _out6, _out7)        \
+{                                                                                   \
+    __m256i _tmp0_m, _tmp1_m, _tmp2_m, _tmp3_m;                                     \
+    __m256i _tmp4_m, _tmp5_m, _tmp6_m, _tmp7_m;                                     \
+    _tmp0_m = __lasx_xvilvl_b(_in2, _in0);                                          \
+    _tmp1_m = __lasx_xvilvl_b(_in3, _in1);                                          \
+    _tmp2_m = __lasx_xvilvl_b(_in6, _in4);                                          \
+    _tmp3_m = __lasx_xvilvl_b(_in7, _in5);                                          \
+    _tmp4_m = __lasx_xvilvl_b(_tmp1_m, _tmp0_m);                                    \
+    _tmp5_m = __lasx_xvilvh_b(_tmp1_m, _tmp0_m);                                    \
+    _tmp6_m = __lasx_xvilvl_b(_tmp3_m, _tmp2_m);                                    \
+    _tmp7_m = __lasx_xvilvh_b(_tmp3_m, _tmp2_m);                                    \
+    _out0 = __lasx_xvilvl_w(_tmp6_m, _tmp4_m);                                      \
+    _out2 = __lasx_xvilvh_w(_tmp6_m, _tmp4_m);                                      \
+    _out4 = __lasx_xvilvl_w(_tmp7_m, _tmp5_m);                                      \
+    _out6 = __lasx_xvilvh_w(_tmp7_m, _tmp5_m);                                      \
+    _out1 = __lasx_xvbsrl_v(_out0, 8);                                              \
+    _out3 = __lasx_xvbsrl_v(_out2, 8);                                              \
+    _out5 = __lasx_xvbsrl_v(_out4, 8);                                              \
+    _out7 = __lasx_xvbsrl_v(_out6, 8);                                              \
+}
+
+/*
+ * =============================================================================
+ * Description : Transpose 8x8 block with halfword elements in vectors.
+ * Arguments   : Inputs  - _in0, _in1, ~
+ *               Outputs - _out0, _out1, ~
+ * Details     : The rows of the matrix become columns, and the columns become rows.
+ * Example     : LASX_TRANSPOSE8x8_H
+ *        _in0 : 1,2,3,4, 5,6,7,8, 1,2,3,4, 5,6,7,8
+ *        _in1 : 8,2,3,4, 5,6,7,8, 8,2,3,4, 5,6,7,8
+ *        _in2 : 8,2,3,4, 5,6,7,8, 8,2,3,4, 5,6,7,8
+ *        _in3 : 1,2,3,4, 5,6,7,8, 1,2,3,4, 5,6,7,8
+ *        _in4 : 9,2,3,4, 5,6,7,8, 9,2,3,4, 5,6,7,8
+ *        _in5 : 1,2,3,4, 5,6,7,8, 1,2,3,4, 5,6,7,8
+ *        _in6 : 1,2,3,4, 5,6,7,8, 1,2,3,4, 5,6,7,8
+ *        _in7 : 9,2,3,4, 5,6,7,8, 9,2,3,4, 5,6,7,8
+ *
+ *       _out0 : 1,8,8,1, 9,1,1,9, 1,8,8,1, 9,1,1,9
+ *       _out1 : 2,2,2,2, 2,2,2,2, 2,2,2,2, 2,2,2,2
+ *       _out2 : 3,3,3,3, 3,3,3,3, 3,3,3,3, 3,3,3,3
+ *       _out3 : 4,4,4,4, 4,4,4,4, 4,4,4,4, 4,4,4,4
+ *       _out4 : 5,5,5,5, 5,5,5,5, 5,5,5,5, 5,5,5,5
+ *       _out5 : 6,6,6,6, 6,6,6,6, 6,6,6,6, 6,6,6,6
+ *       _out6 : 7,7,7,7, 7,7,7,7, 7,7,7,7, 7,7,7,7
+ *       _out7 : 8,8,8,8, 8,8,8,8, 8,8,8,8, 8,8,8,8
+ * =============================================================================
+ */
+#define LASX_TRANSPOSE8x8_H(_in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7, _out0,  \
+                            _out1, _out2, _out3, _out4, _out5, _out6, _out7)        \
+{                                                                                   \
+    __m256i _s0_m, _s1_m;                                                           \
+    __m256i _tmp0_m, _tmp1_m, _tmp2_m, _tmp3_m;                                     \
+    __m256i _tmp4_m, _tmp5_m, _tmp6_m, _tmp7_m;                                     \
+                                                                                    \
+    _s0_m   = __lasx_xvilvl_h(_in6, _in4);                                          \
+    _s1_m   = __lasx_xvilvl_h(_in7, _in5);                                          \
+    _tmp0_m = __lasx_xvilvl_h(_s1_m, _s0_m);                                        \
+    _tmp1_m = __lasx_xvilvh_h(_s1_m, _s0_m);                                        \
+    _s0_m   = __lasx_xvilvh_h(_in6, _in4);                                          \
+    _s1_m   = __lasx_xvilvh_h(_in7, _in5);                                          \
+    _tmp2_m = __lasx_xvilvl_h(_s1_m, _s0_m);                                        \
+    _tmp3_m = __lasx_xvilvh_h(_s1_m, _s0_m);                                        \
+                                                                                    \
+    _s0_m   = __lasx_xvilvl_h(_in2, _in0);                                          \
+    _s1_m   = __lasx_xvilvl_h(_in3, _in1);                                          \
+    _tmp4_m = __lasx_xvilvl_h(_s1_m, _s0_m);                                        \
+    _tmp5_m = __lasx_xvilvh_h(_s1_m, _s0_m);                                        \
+    _s0_m   = __lasx_xvilvh_h(_in2, _in0);                                          \
+    _s1_m   = __lasx_xvilvh_h(_in3, _in1);                                          \
+    _tmp6_m = __lasx_xvilvl_h(_s1_m, _s0_m);                                        \
+    _tmp7_m = __lasx_xvilvh_h(_s1_m, _s0_m);                                        \
+                                                                                    \
+    _out0 = __lasx_xvpickev_d(_tmp0_m, _tmp4_m);                                    \
+    _out2 = __lasx_xvpickev_d(_tmp1_m, _tmp5_m);                                    \
+    _out4 = __lasx_xvpickev_d(_tmp2_m, _tmp6_m);                                    \
+    _out6 = __lasx_xvpickev_d(_tmp3_m, _tmp7_m);                                    \
+    _out1 = __lasx_xvpickod_d(_tmp0_m, _tmp4_m);                                    \
+    _out3 = __lasx_xvpickod_d(_tmp1_m, _tmp5_m);                                    \
+    _out5 = __lasx_xvpickod_d(_tmp2_m, _tmp6_m);                                    \
+    _out7 = __lasx_xvpickod_d(_tmp3_m, _tmp7_m);                                    \
+}
+
+/*
+ * =============================================================================
+ * Description : Butterfly of 4 input vectors
+ * Arguments   : Inputs  - _in0, _in1, _in2, _in3
+ *               Outputs - _out0, _out1, _out2, _out3
+ * Details     : Butterfly operation
+ * Example     : LASX_BUTTERFLY_4
+ *               _out0 = _in0 + _in3;
+ *               _out1 = _in1 + _in2;
+ *               _out2 = _in1 - _in2;
+ *               _out3 = _in0 - _in3;
+ * =============================================================================
+ */
+#define LASX_BUTTERFLY_4_B(_in0, _in1, _in2, _in3, _out0, _out1, _out2, _out3)  \
+{                                                                               \
+    _out0 = __lasx_xvadd_b(_in0, _in3);                                         \
+    _out1 = __lasx_xvadd_b(_in1, _in2);                                         \
+    _out2 = __lasx_xvsub_b(_in1, _in2);                                         \
+    _out3 = __lasx_xvsub_b(_in0, _in3);                                         \
+}
+#define LASX_BUTTERFLY_4_H(_in0, _in1, _in2, _in3, _out0, _out1, _out2, _out3)  \
+{                                                                               \
+    _out0 = __lasx_xvadd_h(_in0, _in3);                                         \
+    _out1 = __lasx_xvadd_h(_in1, _in2);                                         \
+    _out2 = __lasx_xvsub_h(_in1, _in2);                                         \
+    _out3 = __lasx_xvsub_h(_in0, _in3);                                         \
+}
+#define LASX_BUTTERFLY_4_W(_in0, _in1, _in2, _in3, _out0, _out1, _out2, _out3)  \
+{                                                                               \
+    _out0 = __lasx_xvadd_w(_in0, _in3);                                         \
+    _out1 = __lasx_xvadd_w(_in1, _in2);                                         \
+    _out2 = __lasx_xvsub_w(_in1, _in2);                                         \
+    _out3 = __lasx_xvsub_w(_in0, _in3);                                         \
+}
+#define LASX_BUTTERFLY_4_D(_in0, _in1, _in2, _in3, _out0, _out1, _out2, _out3)  \
+{                                                                               \
+    _out0 = __lasx_xvadd_d(_in0, _in3);                                         \
+    _out1 = __lasx_xvadd_d(_in1, _in2);                                         \
+    _out2 = __lasx_xvsub_d(_in1, _in2);                                         \
+    _out3 = __lasx_xvsub_d(_in0, _in3);                                         \
+}
+
+/*
+ * =============================================================================
+ * Description : Butterfly of 8 input vectors
+ * Arguments   : Inputs  - _in0, _in1, _in2, _in3, ~
+ *               Outputs - _out0, _out1, _out2, _out3, ~
+ * Details     : Butterfly operation
+ * Example     : LASX_BUTTERFLY_8
+ *               _out0 = _in0 + _in7;
+ *               _out1 = _in1 + _in6;
+ *               _out2 = _in2 + _in5;
+ *               _out3 = _in3 + _in4;
+ *               _out4 = _in3 - _in4;
+ *               _out5 = _in2 - _in5;
+ *               _out6 = _in1 - _in6;
+ *               _out7 = _in0 - _in7;
+ * =============================================================================
+ */
+#define LASX_BUTTERFLY_8_B(_in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7,        \
+                           _out0, _out1, _out2, _out3, _out4, _out5, _out6, _out7)\
+{                                                                                 \
+    _out0 = __lasx_xvadd_b(_in0, _in7);                                           \
+    _out1 = __lasx_xvadd_b(_in1, _in6);                                           \
+    _out2 = __lasx_xvadd_b(_in2, _in5);                                           \
+    _out3 = __lasx_xvadd_b(_in3, _in4);                                           \
+    _out4 = __lasx_xvsub_b(_in3, _in4);                                           \
+    _out5 = __lasx_xvsub_b(_in2, _in5);                                           \
+    _out6 = __lasx_xvsub_b(_in1, _in6);                                           \
+    _out7 = __lasx_xvsub_b(_in0, _in7);                                           \
+}
+
+#define LASX_BUTTERFLY_8_H(_in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7,        \
+                           _out0, _out1, _out2, _out3, _out4, _out5, _out6, _out7)\
+{                                                                                 \
+    _out0 = __lasx_xvadd_h(_in0, _in7);                                           \
+    _out1 = __lasx_xvadd_h(_in1, _in6);                                           \
+    _out2 = __lasx_xvadd_h(_in2, _in5);                                           \
+    _out3 = __lasx_xvadd_h(_in3, _in4);                                           \
+    _out4 = __lasx_xvsub_h(_in3, _in4);                                           \
+    _out5 = __lasx_xvsub_h(_in2, _in5);                                           \
+    _out6 = __lasx_xvsub_h(_in1, _in6);                                           \
+    _out7 = __lasx_xvsub_h(_in0, _in7);                                           \
+}
+
+#define LASX_BUTTERFLY_8_W(_in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7,        \
+                           _out0, _out1, _out2, _out3, _out4, _out5, _out6, _out7)\
+{                                                                                 \
+    _out0 = __lasx_xvadd_w(_in0, _in7);                                           \
+    _out1 = __lasx_xvadd_w(_in1, _in6);                                           \
+    _out2 = __lasx_xvadd_w(_in2, _in5);                                           \
+    _out3 = __lasx_xvadd_w(_in3, _in4);                                           \
+    _out4 = __lasx_xvsub_w(_in3, _in4);                                           \
+    _out5 = __lasx_xvsub_w(_in2, _in5);                                           \
+    _out6 = __lasx_xvsub_w(_in1, _in6);                                           \
+    _out7 = __lasx_xvsub_w(_in0, _in7);                                           \
+}
+
+#define LASX_BUTTERFLY_8_D(_in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7,        \
+                           _out0, _out1, _out2, _out3, _out4, _out5, _out6, _out7)\
+{                                                                                 \
+    _out0 = __lasx_xvadd_d(_in0, _in7);                                           \
+    _out1 = __lasx_xvadd_d(_in1, _in6);                                           \
+    _out2 = __lasx_xvadd_d(_in2, _in5);                                           \
+    _out3 = __lasx_xvadd_d(_in3, _in4);                                           \
+    _out4 = __lasx_xvsub_d(_in3, _in4);                                           \
+    _out5 = __lasx_xvsub_d(_in2, _in5);                                           \
+    _out6 = __lasx_xvsub_d(_in1, _in6);                                           \
+    _out7 = __lasx_xvsub_d(_in0, _in7);                                           \
+}
+
+/*
+ ****************************************************************************
+ ***************  Non-generic macro definition ******************************
+ ****************************************************************************
+ */
+
+/* Description : Horizontal addition of 8 signed word elements of input vector
+ * Arguments   : Input  - in       (signed word vector)
+ *               Output - sum_m    (s32 sum)
+ * Details     : 8 signed word elements of 'in' vector are added together and
+ *               the resulting integer sum is returned
+ */
+#define LASX_HADD_SW_S32( in )                               \
+( {                                                          \
+    int32_t s_sum_m;                                         \
+    v4i64  out;                                              \
+                                                             \
+    out = __lasx_xvhaddw_d_w( in, in );                      \
+    s_sum_m = out[0] + out[1] + out[2] + out[3];             \
+    s_sum_m;                                                 \
+} )
+
+/* Description : Horizontal addition of 16 half word elements of input vector
+ * Arguments   : Input  - in       (half word vector)
+ *               Output - sum_m    (i32 sum)
+ * Details     : 16 half word elements of 'in' vector are added together and
+ *               the resulting integer sum is returned
+ */
+#define LASX_HADD_UH_U32( in )                               \
+( {                                                          \
+    uint32_t u_sum_m;                                        \
+    v4u64  out;                                              \
+    __m256i res_m;                                           \
+                                                             \
+    res_m = __lasx_xvhaddw_wu_hu( in, in );                  \
+    out = ( v4u64 )__lasx_xvhaddw_du_wu( res_m, res_m );     \
+    u_sum_m = out[0] + out[1] + out[2] + out[3];             \
+    u_sum_m;                                                 \
+} )
+
+/* Description : Sign extend halfword elements from input vector and return
+ *               the result in pair of vectors
+ * Arguments   : Input  - in            (halfword vector)
+ *               Outputs - out0, out1   (sign extended word vectors)
+ *               Return Type - signed word
+ * Details     : Sign bit of halfword elements from input vector 'in' is
+ *               extracted and interleaved right with same vector 'in0' to
+ *               generate 4 signed word elements in 'out0'
+ *               Then interleaved left with same vector 'in0' to
+ *               generate 4 signed word elements in 'out1'
+ */
+#define LASX_UNPCK_SH( in, out0, out1 )         \
+{                                               \
+    __m256i tmp_m;                              \
+                                                \
+    tmp_m = __lasx_xvslti_h( in, 0 );           \
+    out0 = __lasx_xvilvl_h( tmp_m, in );        \
+    out1 = __lasx_xvilvh_h( tmp_m, in );        \
+}
+
+/*
+ * Description : Transpose 8x4 block with half word elements in vectors
+ * Arguments   : Inputs  - _in0, _in1, _in2, _in3
+ *               Outputs - _out0, _out1, _out2, _out3
+ * Example     : LASX_TRANSPOSE8x4_H
+ *        _in0 : 1,2,3,4, 5,6,7,8, 2,2,2,2, 2,2,2,2
+ *        _in1 : 8,2,3,4, 5,6,7,8, 3,3,3,3, 3,3,3,3
+ *        _in2 : 8,2,3,4, 5,6,7,8, 4,4,4,4, 4,4,4,4
+ *        _in3 : 1,2,3,4, 5,6,7,8, 0,0,0,0, 0,0,0,0
+ *
+ *       _out0 : 1,8,8,1, 2,2,2,2, 2,3,4,0, 2,3,4,0
+ *       _out1 : 3,3,3,3, 4,4,4,4, 2,3,4,0, 2,3,4,0
+ *       _out2 : 5,5,5,5, 6,6,6,6, 2,3,4,0, 2,3,4,0
+ *       _out3 : 7,7,7,7, 8,8,8,8, 2,3,4,0, 2,3,4,0
+ */
+#define LASX_TRANSPOSE8X4_H(_in0, _in1, _in2, _in3, _out0, _out1, _out2, _out3) \
+{                                                                               \
+    __m256i tmp0_m, tmp1_m, tmp2_m, tmp3_m;                                     \
+                                                                                \
+    tmp0_m = __lasx_xvilvl_h(_in1, _in0);                                       \
+    tmp1_m = __lasx_xvilvl_h(_in3, _in2);                                       \
+    tmp2_m = __lasx_xvilvh_h(_in1, _in0);                                       \
+    tmp3_m = __lasx_xvilvh_h(_in3, _in2);                                       \
+    _out0 = __lasx_xvilvl_w(tmp1_m, tmp0_m);                                    \
+    _out2 = __lasx_xvilvl_w(tmp3_m, tmp2_m);                                    \
+    _out1 = __lasx_xvilvh_w(tmp1_m, tmp0_m);                                    \
+    _out3 = __lasx_xvilvh_w(tmp3_m, tmp2_m);                                    \
+}
+
+#endif //LASX
+
+#endif /* LOONGSON_INTRINSICS_H */
+
diff --git a/common/loongarch/mc-a.S b/common/loongarch/mc-a.S
new file mode 100644
index 0000000..135dfb5
--- /dev/null
+++ b/common/loongarch/mc-a.S
@@ -0,0 +1,136 @@
+/*****************************************************************************
+ * mc-a.S: LoongArch motion compensation
+ *****************************************************************************
+ * Copyright (C) 2015-2018 x264 project
+ * Copyright (C) 2022 Loongson Technology Corporation Limited
+ *
+ * Authors: gxw <guxiwei-hf@loongson.cn>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02111, USA.
+ *
+ * This program is also available under a commercial proprietary license.
+ * For more information, contact us at licensing@x264.com.
+ *****************************************************************************/
+
+#include "asm.S"
+
+const ch_shuf
+.byte 0, 2, 2, 4, 4, 6, 6, 8, 1, 3, 3, 5, 5, 7, 7, 9
+.byte 0, 2, 2, 4, 4, 6, 6, 8, 1, 3, 3, 5, 5, 7, 7, 9
+endconst
+
+#if !HIGH_BIT_DEPTH
+
+.macro  MC_CHROMA_START
+    srai.d  t0,  a5,  3
+    srai.d  t1,  a6,  3
+    slli.d  t0,  t0,  1
+    mul.d   t1,  t1,  a4
+    add.d   t1,  t1,  t0
+    add.d   a3,  a3,  t1 /* src += (m_vy >> 3) * i_src_stride + (m_vx >> 3) * 2 */
+.endm
+
+/* void mc_chroma( uint8_t *p_dst_u, uint8_t *p_dst_v,
+ *                 intptr_t i_dst_stride,
+ *                 uint8_t *p_src, intptr_t i_src_stride,
+ *                 int32_t m_vx, int32_t m_vy,
+ *                 int32_t i_width, int32_t i_height )
+ */
+function mc_chroma_lasx
+    MC_CHROMA_START
+    andi    a5,    a5,    0x07    /* m_vx & 0x07 */
+    andi    a6,    a6,    0x07    /* m_vy & 0x07 */
+    move    t0,    a5
+    slli.d  t0,    t0,    8
+    sub.d   t0,    t0,    a5
+    li.d    a5,    8
+    addi.d  t0,    t0,    8
+    sub.d   a5,    a5,    a6
+    mul.d   a6,    a6,    t0      /* (x * 255 + 8) * y */
+    mul.d   a5,    a5,    t0      /* (x * 255 + 8) * (8 - y) */
+    xvreplgr2vr.h  xr6,   a6      /* cD cC ... cD cC */
+    xvreplgr2vr.h  xr7,   a5      /* cB cA ... cB cA */
+    la.local t0,   ch_shuf
+    xvld    xr5,   t0,    0
+    addi.d  t0,    a7,    -4
+    ldptr.w a7,    sp,    0       /* a7 = i_height */
+    slli.d  t1,    a4,    1
+    blt     zero,  t0,    .L_WIDTH8
+.L_LOOP4:
+    vld       vr0,    a3,   0
+    vldx      vr1,    a3,   a4
+    vldx      vr2,    a3,   t1
+    xvpermi.q xr0,   xr1,  0x02
+    xvpermi.q xr1,   xr2,  0x02
+    xvshuf.b  xr0,   xr0,  xr0,   xr5
+    xvshuf.b  xr1,   xr1,  xr1,   xr5
+    xvdp2.h.bu xr2,  xr0,  xr7
+    xvdp2.h.bu xr3,  xr1,  xr6
+    xvadd.h   xr0,   xr2,  xr3
+    xvssrlrni.bu.h   xr0,  xr0,   6
+    xvstelm.w xr0,   a0,   0,     0
+    xvstelm.w xr0,   a1,   0,     1
+    add.d     a0,    a0,   a2
+    add.d     a1,    a1,   a2
+    xvstelm.w xr0,   a0,   0,     4
+    xvstelm.w xr0,   a1,   0,     5
+    add.d     a0,    a0,   a2
+    add.d     a1,    a1,   a2
+    add.d     a3,    a3,   t1
+    addi.d    a7,    a7,   -2
+    blt       zero,  a7,   .L_LOOP4
+    b         .ENDFUNC
+.L_WIDTH8:
+    xvld      xr0,   a3,    0
+    xvpermi.d xr0,   xr0,   0x94
+    xvshuf.b  xr0,   xr0,   xr0,   xr5
+.L_LOOP8:
+    xvldx     xr3,   a3,    a4
+    xvpermi.d xr3,   xr3,   0x94
+    xvshuf.b  xr3,   xr3,   xr3,   xr5
+    xvdp2.h.bu xr1,  xr0,   xr7
+    xvdp2.h.bu xr2,  xr3,   xr6
+    xvdp2.h.bu xr8,  xr3,   xr7
+
+    xvldx     xr0,   a3,    t1
+    xvpermi.d xr0,   xr0,   0x94
+    xvshuf.b  xr0,   xr0,   xr0,   xr5
+    xvdp2.h.bu xr4,  xr0,   xr6
+    xvadd.h   xr1,   xr1,   xr2
+    xvadd.h   xr3,   xr8,   xr4
+
+    xvssrlrni.bu.h   xr3,   xr1,    6
+
+    xvpermi.q   xr4,   xr3,    0x01
+    xvpackev.w  xr8,   xr4,    xr3
+    xvpackod.w  xr9,   xr4,    xr3
+    vstelm.d    vr8,   a0,     0,    0
+    vstelm.d    vr9,   a1,     0,    0
+    add.d       a0,    a0,     a2
+    add.d       a1,    a1,     a2
+    vstelm.d    vr8,   a0,     0,    1
+    vstelm.d    vr9,   a1,     0,    1
+
+    addi.d      a7,    a7,     -2
+    add.d       a0,    a0,     a2
+    add.d       a1,    a1,     a2
+    add.d       a3,    a3,     t1
+    blt         zero,  a7,     .L_LOOP8
+.ENDFUNC:
+    endfunc
+
+.end    mc-a.S
+
+#endif /* !HIGH_BIT_DEPTH */
diff --git a/common/loongarch/mc-c.c b/common/loongarch/mc-c.c
new file mode 100644
index 0000000..bcaae71
--- /dev/null
+++ b/common/loongarch/mc-c.c
@@ -0,0 +1,3935 @@
+/*****************************************************************************
+ * mc-c.c: loongarch motion compensation
+ *****************************************************************************
+ * Copyright (C) 2015-2018 x264 project
+ * Copyright (C) 2020 Loongson Technology Corporation Limited
+ *
+ * Authors: zhou peng <zhoupeng@loongson.cn>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02111, USA.
+ *
+ * This program is also available under a commercial proprietary license.
+ * For more information, contact us at licensing@x264.com.
+ *****************************************************************************/
+
+#include "common/common.h"
+#include "loongson_intrinsics.h"
+#include "mc.h"
+
+#if !HIGH_BIT_DEPTH
+
+static const uint8_t pu_luma_mask_arr[16 * 6] =
+{
+    0, 5, 1, 6, 2, 7, 3, 8, 4, 9, 5, 10, 6, 11, 7, 12,
+    0, 5, 1, 6, 2, 7, 3, 8, 4, 9, 5, 10, 6, 11, 7, 12,
+    1, 4, 2, 5, 3, 6, 4, 7, 5, 8, 6, 9, 7, 10, 8, 11,
+    1, 4, 2, 5, 3, 6, 4, 7, 5, 8, 6, 9, 7, 10, 8, 11,
+    2, 3, 3, 4, 4, 5, 5, 6, 6, 7, 7, 8, 8, 9, 9, 10,
+    2, 3, 3, 4, 4, 5, 5, 6, 6, 7, 7, 8, 8, 9, 9, 10
+};
+
+static const uint8_t pu_core_mask_arr[16 * 2] =
+{
+    1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,
+    1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16
+};
+
+static void mc_weight_w20_lasx( uint8_t *dst, intptr_t dst_stride, uint8_t *src, intptr_t src_stride,
+                                const x264_weight_t *weight, int height )
+{
+    int i_denom = weight->i_denom, i_scale = weight->i_scale, i_offset = weight->i_offset;
+    int zero = 0, i_4 = 4, src_stride2, src_stride3, src_stride4;
+
+    i_offset <<= i_denom;
+
+
+    __asm__ volatile(
+    "slli.d           %[src_stride2],   %[src_stride],        1                      \n\t"
+    "add.d            %[src_stride3],   %[src_stride2],       %[src_stride]          \n\t"
+    "slli.d           %[src_stride4],   %[src_stride2],       1                      \n\t"
+    "xvreplgr2vr.h    $xr2,             %[i_denom]                                   \n\t"
+    "xvreplgr2vr.b    $xr0,             %[i_scale]                                   \n\t"
+    "xvreplgr2vr.h    $xr1,             %[i_offset]                                  \n\t"
+    "1:                                                                              \n\t"
+    "addi.d           %[height],        %[height],            -4                     \n\t"
+    "xvld             $xr3,             %[src],               0                      \n\t"
+    "xvldx            $xr4,             %[src],               %[src_stride]          \n\t"
+    "xvldx            $xr5,             %[src],               %[src_stride2]         \n\t"
+    "xvldx            $xr6,             %[src],               %[src_stride3]         \n\t"
+    "xvmulwev.h.bu.b  $xr7,             $xr3,                 $xr0                   \n\t"
+    "xvmulwev.h.bu.b  $xr8,             $xr4,                 $xr0                   \n\t"
+    "xvmulwev.h.bu.b  $xr9,             $xr5,                 $xr0                   \n\t"
+    "xvmulwev.h.bu.b  $xr10,            $xr6,                 $xr0                   \n\t"
+    "xvmulwod.h.bu.b  $xr3,             $xr3,                 $xr0                   \n\t"
+    "xvmulwod.h.bu.b  $xr4,             $xr4,                 $xr0                   \n\t"
+    "xvmulwod.h.bu.b  $xr5,             $xr5,                 $xr0                   \n\t"
+    "xvmulwod.h.bu.b  $xr6,             $xr6,                 $xr0                   \n\t"
+    "xvsadd.h         $xr7,             $xr7,                 $xr1                   \n\t"
+    "xvsadd.h         $xr8,             $xr8,                 $xr1                   \n\t"
+    "xvsadd.h         $xr9,             $xr9,                 $xr1                   \n\t"
+    "xvsadd.h         $xr10,            $xr10,                $xr1                   \n\t"
+    "xvsadd.h         $xr3,             $xr3,                 $xr1                   \n\t"
+    "xvsadd.h         $xr4,             $xr4,                 $xr1                   \n\t"
+    "xvsadd.h         $xr5,             $xr5,                 $xr1                   \n\t"
+    "xvsadd.h         $xr6,             $xr6,                 $xr1                   \n\t"
+    "xvmaxi.h         $xr7,             $xr7,                 0                      \n\t"
+    "xvmaxi.h         $xr8,             $xr8,                 0                      \n\t"
+    "xvmaxi.h         $xr9,             $xr9,                 0                      \n\t"
+    "xvmaxi.h         $xr10,            $xr10,                0                      \n\t"
+    "xvmaxi.h         $xr3,             $xr3,                 0                      \n\t"
+    "xvmaxi.h         $xr4,             $xr4,                 0                      \n\t"
+    "xvmaxi.h         $xr5,             $xr5,                 0                      \n\t"
+    "xvmaxi.h         $xr6,             $xr6,                 0                      \n\t"
+    "xvssrlrn.bu.h    $xr7,             $xr7,                 $xr2                   \n\t"
+    "xvssrlrn.bu.h    $xr8,             $xr8,                 $xr2                   \n\t"
+    "xvssrlrn.bu.h    $xr9,             $xr9,                 $xr2                   \n\t"
+    "xvssrlrn.bu.h    $xr10,            $xr10,                $xr2                   \n\t"
+    "xvssrlrn.bu.h    $xr3,             $xr3,                 $xr2                   \n\t"
+    "xvssrlrn.bu.h    $xr4,             $xr4,                 $xr2                   \n\t"
+    "xvssrlrn.bu.h    $xr5,             $xr5,                 $xr2                   \n\t"
+    "xvssrlrn.bu.h    $xr6,             $xr6,                 $xr2                   \n\t"
+    "xvilvl.b         $xr3,             $xr3,                 $xr7                   \n\t"
+    "xvilvl.b         $xr4,             $xr4,                 $xr8                   \n\t"
+    "xvilvl.b         $xr5,             $xr5,                 $xr9                   \n\t"
+    "xvilvl.b         $xr6,             $xr6,                 $xr10                  \n\t"
+    "vst              $vr3,             %[dst],               0                      \n\t"
+    "xvstelm.w        $xr3,             %[dst],               16,          4         \n\t"
+    "add.d            %[dst],           %[dst],               %[dst_stride]          \n\t"
+    "vst              $vr4,             %[dst],               0                      \n\t"
+    "xvstelm.w        $xr4,             %[dst],               16,          4         \n\t"
+    "add.d            %[dst],           %[dst],               %[dst_stride]          \n\t"
+    "vst              $vr5,             %[dst],               0                      \n\t"
+    "xvstelm.w        $xr5,             %[dst],               16,          4         \n\t"
+    "add.d            %[dst],           %[dst],               %[dst_stride]          \n\t"
+    "vst              $vr6,             %[dst],               0                      \n\t"
+    "xvstelm.w        $xr6,             %[dst],               16,          4         \n\t"
+    "add.d            %[src],           %[src],               %[src_stride4]         \n\t"
+    "add.d            %[dst],           %[dst],               %[dst_stride]          \n\t"
+    "bge              %[height],        %[i_4],               1b                     \n\t"
+    "beqz             %[height],        3f                                           \n\t"
+    "2:                                                                              \n\t"
+    "addi.d           %[height],        %[height],            -2                     \n\t"
+    "xvld             $xr3,             %[src],               0                      \n\t"
+    "xvldx            $xr4,             %[src],               %[src_stride]          \n\t"
+    "xvmulwev.h.bu.b  $xr7,             $xr3,                 $xr0                   \n\t"
+    "xvmulwev.h.bu.b  $xr8,             $xr4,                 $xr0                   \n\t"
+    "xvmulwod.h.bu.b  $xr3,             $xr3,                 $xr0                   \n\t"
+    "xvmulwod.h.bu.b  $xr4,             $xr4,                 $xr0                   \n\t"
+    "xvsadd.h         $xr7,             $xr7,                 $xr1                   \n\t"
+    "xvsadd.h         $xr8,             $xr8,                 $xr1                   \n\t"
+    "xvsadd.h         $xr3,             $xr3,                 $xr1                   \n\t"
+    "xvsadd.h         $xr4,             $xr4,                 $xr1                   \n\t"
+    "xvmaxi.h         $xr7,             $xr7,                 0                      \n\t"
+    "xvmaxi.h         $xr8,             $xr8,                 0                      \n\t"
+    "xvmaxi.h         $xr3,             $xr3,                 0                      \n\t"
+    "xvmaxi.h         $xr4,             $xr4,                 0                      \n\t"
+    "xvssrlrn.bu.h    $xr7,             $xr7,                 $xr2                   \n\t"
+    "xvssrlrn.bu.h    $xr8,             $xr8,                 $xr2                   \n\t"
+    "xvssrlrn.bu.h    $xr3,             $xr3,                 $xr2                   \n\t"
+    "xvssrlrn.bu.h    $xr4,             $xr4,                 $xr2                   \n\t"
+    "xvilvl.b         $xr3,             $xr3,                 $xr7                   \n\t"
+    "xvilvl.b         $xr4,             $xr4,                 $xr8                   \n\t"
+    "vst              $vr3,             %[dst],               0                      \n\t"
+    "xvstelm.w        $xr3,             %[dst],               16,          4         \n\t"
+    "add.d            %[dst],           %[dst],               %[dst_stride]          \n\t"
+    "vst              $vr4,             %[dst],               0                      \n\t"
+    "xvstelm.w        $xr4,             %[dst],               16,          4         \n\t"
+    "add.d            %[src],           %[src],               %[src_stride2]         \n\t"
+    "add.d            %[dst],           %[dst],               %[dst_stride]          \n\t"
+    "blt              %[zero],          %[height],            2b                     \n\t"
+    "3:                                                                              \n\t"
+    : [height]"+&r"(height), [src]"+&r"(src), [dst]"+&r"(dst), [src_stride2]"=&r"(src_stride2),
+      [src_stride3]"=&r"(src_stride3), [src_stride4]"=&r"(src_stride4)
+    : [dst_stride]"r"((int64_t) dst_stride), [src_stride]"r"((int64_t) src_stride), [i_4]"r"(i_4),
+      [zero]"r"(zero), [i_denom]"r"(i_denom), [i_offset]"r"(i_offset), [i_scale]"r"(i_scale)
+    : "memory"
+    );
+}
+
+static void mc_weight_w16_lasx( uint8_t *dst, intptr_t dst_stride, uint8_t *src, intptr_t src_stride,
+                                const x264_weight_t *weight, int height )
+{
+    int i_denom = weight->i_denom, i_scale = weight->i_scale, i_offset = weight->i_offset, i_4 = 4;
+    int zero = 0, src_stride2, src_stride3, src_stride4, dst_stride2, dst_stride3, dst_stride4;
+
+    i_offset <<= i_denom;
+
+    __asm__ volatile(
+    "slli.d           %[src_stride2],   %[src_stride],        1                      \n\t"
+    "add.d            %[src_stride3],   %[src_stride2],       %[src_stride]          \n\t"
+    "slli.d           %[src_stride4],   %[src_stride2],       1                      \n\t"
+    "slli.d           %[dst_stride2],   %[dst_stride],        1                      \n\t"
+    "add.d            %[dst_stride3],   %[dst_stride2],       %[dst_stride]          \n\t"
+    "slli.d           %[dst_stride4],   %[dst_stride2],       1                      \n\t"
+    "xvreplgr2vr.h    $xr2,             %[i_denom]                                   \n\t"
+    "xvreplgr2vr.h    $xr0,             %[i_scale]                                   \n\t"
+    "xvreplgr2vr.h    $xr1,             %[i_offset]                                  \n\t"
+    "1:                                                                              \n\t"
+    "addi.d           %[height],        %[height],            -4                     \n\t"
+    "vld              $vr3,             %[src],               0                      \n\t"
+    "vldx             $vr4,             %[src],               %[src_stride]          \n\t"
+    "vldx             $vr5,             %[src],               %[src_stride2]         \n\t"
+    "vldx             $vr6,             %[src],               %[src_stride3]         \n\t"
+    "vext2xv.hu.bu    $xr3,             $xr3                                         \n\t"
+    "vext2xv.hu.bu    $xr4,             $xr4                                         \n\t"
+    "vext2xv.hu.bu    $xr5,             $xr5                                         \n\t"
+    "vext2xv.hu.bu    $xr6,             $xr6                                         \n\t"
+    "xvmul.h          $xr3,             $xr3,                 $xr0                   \n\t"
+    "xvmul.h          $xr4,             $xr4,                 $xr0                   \n\t"
+    "xvmul.h          $xr5,             $xr5,                 $xr0                   \n\t"
+    "xvmul.h          $xr6,             $xr6,                 $xr0                   \n\t"
+    "xvsadd.h         $xr3,             $xr3,                 $xr1                   \n\t"
+    "xvsadd.h         $xr4,             $xr4,                 $xr1                   \n\t"
+    "xvsadd.h         $xr5,             $xr5,                 $xr1                   \n\t"
+    "xvsadd.h         $xr6,             $xr6,                 $xr1                   \n\t"
+    "xvmaxi.h         $xr3,             $xr3,                 0                      \n\t"
+    "xvmaxi.h         $xr4,             $xr4,                 0                      \n\t"
+    "xvmaxi.h         $xr5,             $xr5,                 0                      \n\t"
+    "xvmaxi.h         $xr6,             $xr6,                 0                      \n\t"
+    "xvssrlrn.bu.h    $xr3,             $xr3,                 $xr2                   \n\t"
+    "xvssrlrn.bu.h    $xr4,             $xr4,                 $xr2                   \n\t"
+    "xvssrlrn.bu.h    $xr5,             $xr5,                 $xr2                   \n\t"
+    "xvssrlrn.bu.h    $xr6,             $xr6,                 $xr2                   \n\t"
+    "xvpermi.d        $xr3,             $xr3,                 8                      \n\n"
+    "xvpermi.d        $xr4,             $xr4,                 8                      \n\n"
+    "xvpermi.d        $xr5,             $xr5,                 8                      \n\n"
+    "xvpermi.d        $xr6,             $xr6,                 8                      \n\n"
+    "vst              $vr3,             %[dst],               0                      \n\t"
+    "vstx             $vr4,             %[dst],               %[dst_stride]          \n\t"
+    "vstx             $vr5,             %[dst],               %[dst_stride2]         \n\t"
+    "vstx             $vr6,             %[dst],               %[dst_stride3]         \n\t"
+    "add.d            %[src],           %[src],               %[src_stride4]         \n\t"
+    "add.d            %[dst],           %[dst],               %[dst_stride4]         \n\t"
+    "bge              %[height],        %[i_4],               1b                     \n\t"
+    "beqz             %[height],        3f                                           \n\t"
+    "2:                                                                              \n\t"
+    "addi.d           %[height],        %[height],            -2                     \n\t"
+    "vld              $vr3,             %[src],               0                      \n\t"
+    "vldx             $vr4,             %[src],               %[src_stride]          \n\t"
+    "vext2xv.hu.bu    $xr3,             $xr3                                         \n\t"
+    "vext2xv.hu.bu    $xr4,             $xr4                                         \n\t"
+    "xvmul.h          $xr3,             $xr3,                 $xr0                   \n\t"
+    "xvmul.h          $xr4,             $xr4,                 $xr0                   \n\t"
+    "xvsadd.h         $xr3,             $xr3,                 $xr1                   \n\t"
+    "xvsadd.h         $xr4,             $xr4,                 $xr1                   \n\t"
+    "xvmaxi.h         $xr3,             $xr3,                 0                      \n\t"
+    "xvmaxi.h         $xr4,             $xr4,                 0                      \n\t"
+    "xvssrlrn.bu.h    $xr3,             $xr3,                 $xr2                   \n\t"
+    "xvssrlrn.bu.h    $xr4,             $xr4,                 $xr2                   \n\t"
+    "xvpermi.d        $xr3,             $xr3,                 8                      \n\n"
+    "xvpermi.d        $xr4,             $xr4,                 8                      \n\n"
+    "vst              $vr3,             %[dst],               0                      \n\t"
+    "vstx             $vr4,             %[dst],               %[dst_stride]          \n\t"
+    "add.d            %[src],           %[src],               %[src_stride2]         \n\t"
+    "add.d            %[dst],           %[dst],               %[dst_stride2]         \n\t"
+    "blt              %[zero],          %[height],            2b                     \n\t"
+    "3:                                                                              \n\t"
+    : [height]"+&r"(height), [src]"+&r"(src), [dst]"+&r"(dst), [src_stride2]"=&r"(src_stride2),
+      [src_stride3]"=&r"(src_stride3), [src_stride4]"=&r"(src_stride4), [dst_stride2]"=&r"(dst_stride2),
+      [dst_stride3]"=&r"(dst_stride3), [dst_stride4]"=&r"(dst_stride4)
+    : [dst_stride]"r"((int64_t) dst_stride), [src_stride]"r"((int64_t) src_stride), [i_4]"r"(i_4),
+      [zero]"r"(zero), [i_denom]"r"(i_denom), [i_offset]"r"(i_offset), [i_scale]"r"(i_scale)
+    : "memory"
+    );
+}
+
+static void mc_weight_w8_lasx( uint8_t *dst, intptr_t dst_stride, uint8_t *src, intptr_t src_stride,
+                               const x264_weight_t *weight, int height )
+{
+    int i_4 = 4;
+    int i_denom = weight->i_denom, i_scale = weight->i_scale, i_offset = weight->i_offset;
+    int zero = 0, src_stride2, src_stride3, src_stride4;
+
+    i_offset <<= i_denom;
+    i_offset += (1 << ( i_denom -1 ));
+
+    __asm__ volatile(
+    "slli.d           %[src_stride2],   %[src_stride],        1                      \n\t"
+    "add.d            %[src_stride3],   %[src_stride2],       %[src_stride]          \n\t"
+    "slli.d           %[src_stride4],   %[src_stride2],       1                      \n\t"
+    "xvreplgr2vr.h    $xr2,             %[i_denom]                                   \n\t"
+    "xvreplgr2vr.h    $xr0,             %[i_scale]                                   \n\t"
+    "xvreplgr2vr.h    $xr1,             %[i_offset]                                  \n\t"
+    "1:                                                                              \n\t"
+    "addi.d           %[height],        %[height],            -4                     \n\t"
+    "vld              $vr3,             %[src],               0                      \n\t"
+    "vldx             $vr4,             %[src],               %[src_stride]          \n\t"
+    "vldx             $vr5,             %[src],               %[src_stride2]         \n\t"
+    "vldx             $vr6,             %[src],               %[src_stride3]         \n\t"
+    "add.d            %[src],           %[src],               %[src_stride4]         \n\t"
+    "vilvl.d          $vr3,             $vr4,                 $vr3                   \n\t"
+    "vilvl.d          $vr4,             $vr6,                 $vr5                   \n\t"
+    "vext2xv.hu.bu    $xr3,             $xr3                                         \n\t"
+    "vext2xv.hu.bu    $xr4,             $xr4                                         \n\t"
+    "xvmul.h          $xr3,             $xr3,                 $xr0                   \n\t"
+    "xvmul.h          $xr4,             $xr4,                 $xr0                   \n\t"
+    "xvsadd.h         $xr3,             $xr3,                 $xr1                   \n\t"
+    "xvsadd.h         $xr4,             $xr4,                 $xr1                   \n\t"
+    "xvmaxi.h         $xr3,             $xr3,                 0                      \n\t"
+    "xvmaxi.h         $xr4,             $xr4,                 0                      \n\t"
+    "xvssrln.bu.h     $xr3,             $xr3,                 $xr2                   \n\t"
+    "xvssrln.bu.h     $xr4,             $xr4,                 $xr2                   \n\t"
+    "xvstelm.d        $xr3,             %[dst],               0,            0        \n\t"
+    "add.d            %[dst],           %[dst],               %[dst_stride]          \n\t"
+    "xvstelm.d        $xr3,             %[dst],               0,            2        \n\t"
+    "add.d            %[dst],           %[dst],               %[dst_stride]          \n\t"
+    "xvstelm.d        $xr4,             %[dst],               0,            0        \n\t"
+    "add.d            %[dst],           %[dst],               %[dst_stride]          \n\t"
+    "xvstelm.d        $xr4,             %[dst],               0,            2        \n\t"
+    "add.d            %[dst],           %[dst],               %[dst_stride]          \n\t"
+    "bge              %[height],        %[i_4],               1b                     \n\t"
+    "beqz             %[height],        3f                                           \n\t"
+    "2:                                                                              \n\t"
+    "addi.d           %[height],        %[height],            -2                     \n\t"
+    "vld              $vr3,             %[src],               0                      \n\t"
+    "vldx             $vr4,             %[src],               %[src_stride]          \n\t"
+    "vilvl.d          $vr3,             $vr4,                 $vr3                   \n\t"
+    "vext2xv.hu.bu    $xr3,             $xr3                                         \n\t"
+    "xvmul.h          $xr3,             $xr3,                 $xr0                   \n\t"
+    "xvsadd.h         $xr3,             $xr3,                 $xr1                   \n\t"
+    "xvmaxi.h         $xr3,             $xr3,                 0                      \n\t"
+    "xvssrln.bu.h     $xr3,             $xr3,                 $xr2                   \n\t"
+    "xvstelm.d        $xr3,             %[dst],               0,           0         \n\t"
+    "add.d            %[dst],           %[dst],               %[dst_stride]          \n\t"
+    "xvstelm.d        $xr3,             %[dst],               0,           2         \n\t"
+    "add.d            %[dst],           %[dst],               %[dst_stride]          \n\t"
+    "add.d            %[src],           %[src],               %[src_stride2]         \n\t"
+    "blt              %[zero],          %[height],            2b                     \n\t"
+    "3:                                                                              \n\t"
+    : [height]"+&r"(height), [src]"+&r"(src), [dst]"+&r"(dst), [src_stride2]"=&r"(src_stride2),
+      [src_stride3]"=&r"(src_stride3), [src_stride4]"=&r"(src_stride4)
+    : [dst_stride]"r"((int64_t) dst_stride), [src_stride]"r"((int64_t) src_stride), [i_4]"r"(i_4),
+      [zero]"r"(zero), [i_denom]"r"(i_denom), [i_offset]"r"(i_offset), [i_scale]"r"(i_scale)
+    : "memory"
+    );
+}
+
+static void mc_weight_w4_lasx( uint8_t *dst, intptr_t dst_stride, uint8_t *src, intptr_t src_stride,
+                               const x264_weight_t *weight, int height )
+{
+    int i_denom = weight->i_denom, i_scale = weight->i_scale, i_offset = weight->i_offset;
+    int zero = 0, i_4 = 4;
+
+    i_offset <<= i_denom;
+    i_offset += (1 << ( i_denom -1 ));
+
+    __asm__ volatile(
+    "xvreplgr2vr.h    $xr2,             %[i_denom]                                   \n\t"
+    "xvreplgr2vr.h    $xr0,             %[i_scale]                                   \n\t"
+    "xvreplgr2vr.h    $xr1,             %[i_offset]                                  \n\t"
+    "1:                                                                              \n\t"
+    "addi.d           %[height],        %[height],            -4                     \n\t"
+    "vldrepl.w        $vr3,             %[src],               0                      \n\t"
+    "add.d            %[src],           %[src],               %[src_stride]          \n\t"
+    "vldrepl.w        $vr4,             %[src],               0                      \n\t"
+    "add.d            %[src],           %[src],               %[src_stride]          \n\t"
+    "vldrepl.w        $vr5,             %[src],               0                      \n\t"
+    "add.d            %[src],           %[src],               %[src_stride]          \n\t"
+    "vldrepl.w        $vr6,             %[src],               0                      \n\t"
+    "add.d            %[src],           %[src],               %[src_stride]          \n\t"
+    "vilvl.w          $vr3,             $vr4,                 $vr3                   \n\t"
+    "vilvl.w          $vr4,             $vr6,                 $vr5                   \n\t"
+    "vilvl.d          $vr3,             $vr4,                 $vr3                   \n\t"
+    "vext2xv.hu.bu    $xr3,             $xr3                                         \n\t"
+    "xvmul.h          $xr3,             $xr3,                 $xr0                   \n\t"
+    "xvsadd.h         $xr3,             $xr3,                 $xr1                   \n\t"
+    "xvmaxi.h         $xr3,             $xr3,                 0                      \n\t"
+    "xvssrln.bu.h     $xr3,             $xr3,                 $xr2                   \n\t"
+    "xvstelm.w        $xr3,             %[dst],               0,           0         \n\t"
+    "add.d            %[dst],           %[dst],               %[dst_stride]          \n\t"
+    "xvstelm.w        $xr3,             %[dst],               0,           1         \n\t"
+    "add.d            %[dst],           %[dst],               %[dst_stride]          \n\t"
+    "xvstelm.w        $xr3,             %[dst],               0,           4         \n\t"
+    "add.d            %[dst],           %[dst],               %[dst_stride]          \n\t"
+    "xvstelm.w        $xr3,             %[dst],               0,           5         \n\t"
+    "add.d            %[dst],           %[dst],               %[dst_stride]          \n\t"
+    "bge              %[height],        %[i_4],               1b                     \n\t"
+    "beqz             %[height],        3f                                           \n\t"
+    "2:                                                                              \n\t"
+    "addi.d           %[height],        %[height],            -2                     \n\t"
+    "vldrepl.w        $vr3,             %[src],               0                      \n\t"
+    "add.d            %[src],           %[src],               %[src_stride]          \n\t"
+    "vldrepl.w        $vr4,             %[src],               0                      \n\t"
+    "add.d            %[src],           %[src],               %[src_stride]          \n\t"
+    "vilvl.w          $vr3,             $vr4,                 $vr3                   \n\t"
+    "vext2xv.hu.bu    $xr3,             $xr3                                         \n\t"
+    "xvmul.h          $xr3,             $xr3,                 $xr0                   \n\t"
+    "xvsadd.h         $xr3,             $xr3,                 $xr1                   \n\t"
+    "xvmaxi.h         $xr3,             $xr3,                 0                      \n\t"
+    "xvssrln.bu.h     $xr3,             $xr3,                 $xr2                   \n\t"
+    "xvstelm.w        $xr3,             %[dst],               0,           0         \n\t"
+    "add.d            %[dst],           %[dst],               %[dst_stride]          \n\t"
+    "xvstelm.w        $xr3,             %[dst],               0,           1         \n\t"
+    "add.d            %[dst],           %[dst],               %[dst_stride]          \n\t"
+    "blt              %[zero],          %[height],            2b                     \n\t"
+    "3:                                                                              \n\t"
+    : [height]"+&r"(height), [src]"+&r"(src), [dst]"+&r"(dst)
+    : [dst_stride]"r"((int64_t) dst_stride), [src_stride]"r"((int64_t) src_stride), [i_4]"r"(i_4),
+      [zero]"r"(zero), [i_denom]"r"(i_denom), [i_offset]"r"(i_offset), [i_scale]"r"(i_scale)
+    : "memory"
+    );
+}
+
+static void mc_weight_w4_noden_lasx( uint8_t *dst, intptr_t dst_stride, uint8_t *src, intptr_t src_stride,
+                                     const x264_weight_t *weight, int height )
+{
+    int i_scale = weight->i_scale, i_offset = weight->i_offset;
+    int zero = 0, i_4 = 4;
+
+    __asm__ volatile(
+    "xvreplgr2vr.h    $xr0,             %[i_scale]                                   \n\t"
+    "xvreplgr2vr.h    $xr1,             %[i_offset]                                  \n\t"
+    "1:                                                                              \n\t"
+    "addi.d           %[height],        %[height],            -4                     \n\t"
+    "vldrepl.w        $vr3,             %[src],               0                      \n\t"
+    "add.d            %[src],           %[src],               %[src_stride]          \n\t"
+    "vldrepl.w        $vr4,             %[src],               0                      \n\t"
+    "add.d            %[src],           %[src],               %[src_stride]          \n\t"
+    "vldrepl.w        $vr5,             %[src],               0                      \n\t"
+    "add.d            %[src],           %[src],               %[src_stride]          \n\t"
+    "vldrepl.w        $vr6,             %[src],               0                      \n\t"
+    "add.d            %[src],           %[src],               %[src_stride]          \n\t"
+    "vilvl.w          $vr3,             $vr4,                 $vr3                   \n\t"
+    "vilvl.w          $vr4,             $vr6,                 $vr5                   \n\t"
+    "vilvl.d          $vr3,             $vr4,                 $vr3                   \n\t"
+    "vext2xv.hu.bu    $xr3,             $xr3                                         \n\t"
+    "xvmul.h          $xr3,             $xr3,                 $xr0                   \n\t"
+    "xvsadd.h         $xr3,             $xr3,                 $xr1                   \n\t"
+    "xvmaxi.h         $xr3,             $xr3,                 0                      \n\t"
+    "xvssrlni.bu.h    $xr3,             $xr3,                 0                      \n\t"
+    "xvstelm.w        $xr3,             %[dst],               0,           0         \n\t"
+    "add.d            %[dst],           %[dst],               %[dst_stride]          \n\t"
+    "xvstelm.w        $xr3,             %[dst],               0,           1         \n\t"
+    "add.d            %[dst],           %[dst],               %[dst_stride]          \n\t"
+    "xvstelm.w        $xr3,             %[dst],               0,           4         \n\t"
+    "add.d            %[dst],           %[dst],               %[dst_stride]          \n\t"
+    "xvstelm.w        $xr3,             %[dst],               0,           5         \n\t"
+    "add.d            %[dst],           %[dst],               %[dst_stride]          \n\t"
+    "bge              %[height],        %[i_4],               1b                     \n\t"
+    "beqz             %[height],        3f                                           \n\t"
+    "2:                                                                              \n\t"
+    "addi.d           %[height],        %[height],            -2                     \n\t"
+    "vldrepl.w        $vr3,             %[src],               0                      \n\t"
+    "add.d            %[src],           %[src],               %[src_stride]          \n\t"
+    "vldrepl.w        $vr4,             %[src],               0                      \n\t"
+    "add.d            %[src],           %[src],               %[src_stride]          \n\t"
+    "vilvl.w          $vr3,             $vr4,                 $vr3                   \n\t"
+    "vext2xv.hu.bu    $xr3,             $xr3                                         \n\t"
+    "xvmul.h          $xr3,             $xr3,                 $xr0                   \n\t"
+    "xvsadd.h         $xr3,             $xr3,                 $xr1                   \n\t"
+    "xvmaxi.h         $xr3,             $xr3,                 0                      \n\t"
+    "xvssrlni.bu.h    $xr3,             $xr3,                 0                      \n\t"
+    "xvstelm.w        $xr3,             %[dst],               0,           0         \n\t"
+    "add.d            %[dst],           %[dst],               %[dst_stride]          \n\t"
+    "xvstelm.w        $xr3,             %[dst],               0,           1         \n\t"
+    "add.d            %[dst],           %[dst],               %[dst_stride]          \n\t"
+    "blt              %[zero],          %[height],            2b                     \n\t"
+    "3:                                                                              \n\t"
+    : [height]"+&r"(height), [src]"+&r"(src), [dst]"+&r"(dst)
+    : [dst_stride]"r"((int64_t) dst_stride), [src_stride]"r"((int64_t) src_stride),
+      [zero]"r"(zero), [i_offset]"r"(i_offset), [i_scale]"r"(i_scale), [i_4]"r"(i_4)
+    : "memory"
+    );
+}
+
+static void mc_weight_w8_noden_lasx( uint8_t *dst, intptr_t dst_stride, uint8_t *src, intptr_t src_stride,
+                                     const x264_weight_t *weight, int height )
+{
+    int i_4 = 4;
+    int i_scale = weight->i_scale, i_offset = weight->i_offset;
+    int zero = 0, src_stride2, src_stride3, src_stride4;
+
+    __asm__ volatile(
+    "slli.d           %[src_stride2],   %[src_stride],        1                      \n\t"
+    "add.d            %[src_stride3],   %[src_stride2],       %[src_stride]          \n\t"
+    "slli.d           %[src_stride4],   %[src_stride2],       1                      \n\t"
+    "xvreplgr2vr.h    $xr0,             %[i_scale]                                   \n\t"
+    "xvreplgr2vr.h    $xr1,             %[i_offset]                                  \n\t"
+    "1:                                                                              \n\t"
+    "addi.d           %[height],        %[height],            -4                     \n\t"
+    "vld              $vr3,             %[src],               0                      \n\t"
+    "vldx             $vr4,             %[src],               %[src_stride]          \n\t"
+    "vldx             $vr5,             %[src],               %[src_stride2]         \n\t"
+    "vldx             $vr6,             %[src],               %[src_stride3]         \n\t"
+    "add.d            %[src],           %[src],               %[src_stride4]         \n\t"
+    "vilvl.d          $vr3,             $vr4,                 $vr3                   \n\t"
+    "vilvl.d          $vr4,             $vr6,                 $vr5                   \n\t"
+    "vext2xv.hu.bu    $xr3,             $xr3                                         \n\t"
+    "vext2xv.hu.bu    $xr4,             $xr4                                         \n\t"
+    "xvmul.h          $xr3,             $xr3,                 $xr0                   \n\t"
+    "xvmul.h          $xr4,             $xr4,                 $xr0                   \n\t"
+    "xvsadd.h         $xr3,             $xr3,                 $xr1                   \n\t"
+    "xvsadd.h         $xr4,             $xr4,                 $xr1                   \n\t"
+    "xvmaxi.h         $xr3,             $xr3,                 0                      \n\t"
+    "xvmaxi.h         $xr4,             $xr4,                 0                      \n\t"
+    "xvssrlni.bu.h    $xr4,             $xr3,                 0                      \n\t"
+    "xvstelm.d        $xr4,             %[dst],               0,            0        \n\t"
+    "add.d            %[dst],           %[dst],               %[dst_stride]          \n\t"
+    "xvstelm.d        $xr4,             %[dst],               0,            2        \n\t"
+    "add.d            %[dst],           %[dst],               %[dst_stride]          \n\t"
+    "xvstelm.d        $xr4,             %[dst],               0,            1        \n\t"
+    "add.d            %[dst],           %[dst],               %[dst_stride]          \n\t"
+    "xvstelm.d        $xr4,             %[dst],               0,            3        \n\t"
+    "add.d            %[dst],           %[dst],               %[dst_stride]          \n\t"
+    "bge              %[height],        %[i_4],               1b                     \n\t"
+    "beqz             %[height],        3f                                           \n\t"
+    "2:                                                                              \n\t"
+    "addi.d           %[height],        %[height],            -2                     \n\t"
+    "vld              $vr3,             %[src],               0                      \n\t"
+    "vldx             $vr4,             %[src],               %[src_stride]          \n\t"
+    "vilvl.d          $vr3,             $vr4,                 $vr3                   \n\t"
+    "vext2xv.hu.bu    $xr3,             $xr3                                         \n\t"
+    "xvmul.h          $xr3,             $xr3,                 $xr0                   \n\t"
+    "xvsadd.h         $xr3,             $xr3,                 $xr1                   \n\t"
+    "xvmaxi.h         $xr3,             $xr3,                 0                      \n\t"
+    "xvssrlni.bu.h    $xr3,             $xr3,                 0                      \n\t"
+    "xvstelm.d        $xr3,             %[dst],               0,            0        \n\t"
+    "add.d            %[dst],           %[dst],               %[dst_stride]          \n\t"
+    "xvstelm.d        $xr3,             %[dst],               0,            2        \n\t"
+    "add.d            %[dst],           %[dst],               %[dst_stride]          \n\t"
+    "add.d            %[src],           %[src],               %[src_stride2]         \n\t"
+    "blt              %[zero],          %[height],            2b                     \n\t"
+    "3:                                                                              \n\t"
+    : [height]"+&r"(height), [src]"+&r"(src), [dst]"+&r"(dst), [src_stride2]"=&r"(src_stride2),
+      [src_stride3]"=&r"(src_stride3), [src_stride4]"=&r"(src_stride4)
+    : [dst_stride]"r"((int64_t) dst_stride), [src_stride]"r"((int64_t) src_stride),
+      [zero]"r"(zero), [i_offset]"r"(i_offset), [i_scale]"r"(i_scale), [i_4]"r"(i_4)
+    : "memory"
+    );
+}
+
+static void mc_weight_w16_noden_lasx( uint8_t *dst, intptr_t dst_stride, uint8_t *src, intptr_t src_stride,
+                                      const x264_weight_t *weight, int height )
+{
+    int i_4 = 4;
+    int i_scale = weight->i_scale, i_offset = weight->i_offset;
+    int zero = 0, src_stride2, src_stride3, src_stride4, dst_stride2, dst_stride3, dst_stride4;
+
+    __asm__ volatile(
+    "slli.d           %[src_stride2],   %[src_stride],        1                      \n\t"
+    "add.d            %[src_stride3],   %[src_stride2],       %[src_stride]          \n\t"
+    "slli.d           %[src_stride4],   %[src_stride2],       1                      \n\t"
+    "slli.d           %[dst_stride2],   %[dst_stride],        1                      \n\t"
+    "add.d            %[dst_stride3],   %[dst_stride2],       %[dst_stride]          \n\t"
+    "slli.d           %[dst_stride4],   %[dst_stride2],       1                      \n\t"
+    "xvreplgr2vr.h    $xr0,             %[i_scale]                                   \n\t"
+    "xvreplgr2vr.h    $xr1,             %[i_offset]                                  \n\t"
+    "1:                                                                              \n\t"
+    "addi.d           %[height],        %[height],            -4                     \n\t"
+    "vld              $vr3,             %[src],               0                      \n\t"
+    "vldx             $vr4,             %[src],               %[src_stride]          \n\t"
+    "vldx             $vr5,             %[src],               %[src_stride2]         \n\t"
+    "vldx             $vr6,             %[src],               %[src_stride3]         \n\t"
+    "vext2xv.hu.bu    $xr3,             $xr3                                         \n\t"
+    "vext2xv.hu.bu    $xr4,             $xr4                                         \n\t"
+    "vext2xv.hu.bu    $xr5,             $xr5                                         \n\t"
+    "vext2xv.hu.bu    $xr6,             $xr6                                         \n\t"
+    "xvmul.h          $xr3,             $xr3,                 $xr0                   \n\t"
+    "xvmul.h          $xr4,             $xr4,                 $xr0                   \n\t"
+    "xvmul.h          $xr5,             $xr5,                 $xr0                   \n\t"
+    "xvmul.h          $xr6,             $xr6,                 $xr0                   \n\t"
+    "xvsadd.h         $xr3,             $xr3,                 $xr1                   \n\t"
+    "xvsadd.h         $xr4,             $xr4,                 $xr1                   \n\t"
+    "xvsadd.h         $xr5,             $xr5,                 $xr1                   \n\t"
+    "xvsadd.h         $xr6,             $xr6,                 $xr1                   \n\t"
+    "xvmaxi.h         $xr3,             $xr3,                 0                      \n\t"
+    "xvmaxi.h         $xr4,             $xr4,                 0                      \n\t"
+    "xvmaxi.h         $xr5,             $xr5,                 0                      \n\t"
+    "xvmaxi.h         $xr6,             $xr6,                 0                      \n\t"
+    "xvssrlni.bu.h    $xr4,             $xr3,                 0                      \n\t"
+    "xvssrlni.bu.h    $xr6,             $xr5,                 0                      \n\t"
+    "xvpermi.d        $xr3,             $xr4,                 8                      \n\t"
+    "xvpermi.d        $xr4,             $xr4,                 13                     \n\t"
+    "xvpermi.d        $xr5,             $xr6,                 8                      \n\t"
+    "xvpermi.d        $xr6,             $xr6,                 13                     \n\t"
+    "vst              $vr3,             %[dst],               0                      \n\t"
+    "vstx             $vr4,             %[dst],               %[dst_stride]          \n\t"
+    "vstx             $vr5,             %[dst],               %[dst_stride2]         \n\t"
+    "vstx             $vr6,             %[dst],               %[dst_stride3]         \n\t"
+    "add.d            %[src],           %[src],               %[src_stride4]         \n\t"
+    "add.d            %[dst],           %[dst],               %[dst_stride4]         \n\t"
+    "bge              %[height],        %[i_4],               1b                     \n\t"
+    "beqz             %[height],        3f                                           \n\t"
+    "2:                                                                              \n\t"
+    "addi.d           %[height],        %[height],            -2                     \n\t"
+    "vld              $vr3,             %[src],               0                      \n\t"
+    "vldx             $vr4,             %[src],               %[src_stride]          \n\t"
+    "vext2xv.hu.bu    $xr3,             $xr3                                         \n\t"
+    "vext2xv.hu.bu    $xr4,             $xr4                                         \n\t"
+    "xvmul.h          $xr3,             $xr3,                 $xr0                   \n\t"
+    "xvmul.h          $xr4,             $xr4,                 $xr0                   \n\t"
+    "xvsadd.h         $xr3,             $xr3,                 $xr1                   \n\t"
+    "xvsadd.h         $xr4,             $xr4,                 $xr1                   \n\t"
+    "xvmaxi.h         $xr3,             $xr3,                 0                      \n\t"
+    "xvmaxi.h         $xr4,             $xr4,                 0                      \n\t"
+    "xvssrlni.bu.h    $xr4,             $xr3,                 0                      \n\t"
+    "xvpermi.d        $xr3,             $xr4,                 8                      \n\t"
+    "xvpermi.d        $xr4,             $xr4,                 13                     \n\t"
+    "vst              $vr3,             %[dst],               0                      \n\t"
+    "vstx             $vr4,             %[dst],               %[dst_stride]          \n\t"
+    "add.d            %[src],           %[src],               %[src_stride2]         \n\t"
+    "add.d            %[dst],           %[dst],               %[dst_stride2]         \n\t"
+    "blt              %[zero],          %[height],            2b                     \n\t"
+    "3:                                                                              \n\t"
+    : [height]"+&r"(height), [src]"+&r"(src), [dst]"+&r"(dst), [src_stride2]"=&r"(src_stride2),
+      [src_stride3]"=&r"(src_stride3), [src_stride4]"=&r"(src_stride4), [dst_stride2]"=&r"(dst_stride2),
+      [dst_stride3]"=&r"(dst_stride3), [dst_stride4]"=&r"(dst_stride4)
+    : [dst_stride]"r"((int64_t) dst_stride), [src_stride]"r"((int64_t) src_stride),
+      [zero]"r"(zero), [i_offset]"r"(i_offset), [i_scale]"r"(i_scale), [i_4]"r"(i_4)
+    : "memory"
+    );
+}
+
+static void mc_weight_w20_noden_lasx( uint8_t *dst, intptr_t dst_stride, uint8_t *src, intptr_t src_stride,
+                                      const x264_weight_t *weight, int height )
+{
+    int i_scale = weight->i_scale, i_offset = weight->i_offset;
+    int zero = 0, i_4 = 4, src_stride2, src_stride3, src_stride4;
+
+    __asm__ volatile(
+    "slli.d           %[src_stride2],   %[src_stride],        1                      \n\t"
+    "add.d            %[src_stride3],   %[src_stride2],       %[src_stride]          \n\t"
+    "slli.d           %[src_stride4],   %[src_stride2],       1                      \n\t"
+    "xvreplgr2vr.b    $xr0,             %[i_scale]                                   \n\t"
+    "xvreplgr2vr.h    $xr1,             %[i_offset]                                  \n\t"
+    "1:                                                                              \n\t"
+    "addi.d           %[height],        %[height],            -4                     \n\t"
+    "xvld             $xr3,             %[src],               0                      \n\t"
+    "xvldx            $xr4,             %[src],               %[src_stride]          \n\t"
+    "xvldx            $xr5,             %[src],               %[src_stride2]         \n\t"
+    "xvldx            $xr6,             %[src],               %[src_stride3]         \n\t"
+    "xvmulwev.h.bu.b  $xr7,             $xr3,                 $xr0                   \n\t"
+    "xvmulwev.h.bu.b  $xr8,             $xr4,                 $xr0                   \n\t"
+    "xvmulwev.h.bu.b  $xr9,             $xr5,                 $xr0                   \n\t"
+    "xvmulwev.h.bu.b  $xr10,            $xr6,                 $xr0                   \n\t"
+    "xvmulwod.h.bu.b  $xr3,             $xr3,                 $xr0                   \n\t"
+    "xvmulwod.h.bu.b  $xr4,             $xr4,                 $xr0                   \n\t"
+    "xvmulwod.h.bu.b  $xr5,             $xr5,                 $xr0                   \n\t"
+    "xvmulwod.h.bu.b  $xr6,             $xr6,                 $xr0                   \n\t"
+    "xvsadd.h         $xr7,             $xr7,                 $xr1                   \n\t"
+    "xvsadd.h         $xr8,             $xr8,                 $xr1                   \n\t"
+    "xvsadd.h         $xr9,             $xr9,                 $xr1                   \n\t"
+    "xvsadd.h         $xr10,            $xr10,                $xr1                   \n\t"
+    "xvsadd.h         $xr3,             $xr3,                 $xr1                   \n\t"
+    "xvsadd.h         $xr4,             $xr4,                 $xr1                   \n\t"
+    "xvsadd.h         $xr5,             $xr5,                 $xr1                   \n\t"
+    "xvsadd.h         $xr6,             $xr6,                 $xr1                   \n\t"
+    "xvmaxi.h         $xr7,             $xr7,                 0                      \n\t"
+    "xvmaxi.h         $xr8,             $xr8,                 0                      \n\t"
+    "xvmaxi.h         $xr9,             $xr9,                 0                      \n\t"
+    "xvmaxi.h         $xr10,            $xr10,                0                      \n\t"
+    "xvmaxi.h         $xr3,             $xr3,                 0                      \n\t"
+    "xvmaxi.h         $xr4,             $xr4,                 0                      \n\t"
+    "xvmaxi.h         $xr5,             $xr5,                 0                      \n\t"
+    "xvmaxi.h         $xr6,             $xr6,                 0                      \n\t"
+    "xvssrlni.bu.h    $xr8,             $xr7,                 0                      \n\t"
+    "xvssrlni.bu.h    $xr10,            $xr9,                 0                      \n\t"
+    "xvssrlni.bu.h    $xr4,             $xr3,                 0                      \n\t"
+    "xvssrlni.bu.h    $xr6,             $xr5,                 0                      \n\t"
+    "xvilvl.b         $xr3,             $xr4,                 $xr8                   \n\t"
+    "xvilvh.b         $xr4,             $xr4,                 $xr8                   \n\t"
+    "xvilvl.b         $xr5,             $xr6,                 $xr10                  \n\t"
+    "xvilvh.b         $xr6,             $xr6,                 $xr10                  \n\t"
+    "vst              $vr3,             %[dst],               0                      \n\t"
+    "xvstelm.w        $xr3,             %[dst],               16,          4         \n\t"
+    "add.d            %[dst],           %[dst],               %[dst_stride]          \n\t"
+    "vst              $vr4,             %[dst],               0                      \n\t"
+    "xvstelm.w        $xr4,             %[dst],               16,          4         \n\t"
+    "add.d            %[dst],           %[dst],               %[dst_stride]          \n\t"
+    "vst              $vr5,             %[dst],               0                      \n\t"
+    "xvstelm.w        $xr5,             %[dst],               16,          4         \n\t"
+    "add.d            %[dst],           %[dst],               %[dst_stride]          \n\t"
+    "vst              $vr6,             %[dst],               0                      \n\t"
+    "xvstelm.w        $xr6,             %[dst],               16,          4         \n\t"
+    "add.d            %[src],           %[src],               %[src_stride4]         \n\t"
+    "add.d            %[dst],           %[dst],               %[dst_stride]          \n\t"
+    "bge              %[height],        %[i_4],               1b                     \n\t"
+    "beqz             %[height],        3f                                           \n\t"
+    "2:                                                                              \n\t"
+    "addi.d           %[height],        %[height],            -2                     \n\t"
+    "xvld             $xr3,             %[src],               0                      \n\t"
+    "xvldx            $xr4,             %[src],               %[src_stride]          \n\t"
+    "xvmulwev.h.bu.b  $xr7,             $xr3,                 $xr0                   \n\t"
+    "xvmulwev.h.bu.b  $xr8,             $xr4,                 $xr0                   \n\t"
+    "xvmulwod.h.bu.b  $xr3,             $xr3,                 $xr0                   \n\t"
+    "xvmulwod.h.bu.b  $xr4,             $xr4,                 $xr0                   \n\t"
+    "xvsadd.h         $xr7,             $xr7,                 $xr1                   \n\t"
+    "xvsadd.h         $xr8,             $xr8,                 $xr1                   \n\t"
+    "xvsadd.h         $xr3,             $xr3,                 $xr1                   \n\t"
+    "xvsadd.h         $xr4,             $xr4,                 $xr1                   \n\t"
+    "xvmaxi.h         $xr7,             $xr7,                 0                      \n\t"
+    "xvmaxi.h         $xr8,             $xr8,                 0                      \n\t"
+    "xvmaxi.h         $xr3,             $xr3,                 0                      \n\t"
+    "xvmaxi.h         $xr4,             $xr4,                 0                      \n\t"
+    "xvssrlni.bu.h    $xr8,             $xr7,                 0                      \n\t"
+    "xvssrlni.bu.h    $xr4,             $xr3,                 0                      \n\t"
+    "xvilvl.b         $xr3,             $xr4,                 $xr8                   \n\t"
+    "xvilvh.b         $xr4,             $xr4,                 $xr8                   \n\t"
+    "vst              $vr3,             %[dst],               0                      \n\t"
+    "xvstelm.w        $xr3,             %[dst],               16,          4         \n\t"
+    "add.d            %[dst],           %[dst],               %[dst_stride]          \n\t"
+    "vst              $vr4,             %[dst],               0                      \n\t"
+    "xvstelm.w        $xr4,             %[dst],               16,          4         \n\t"
+    "add.d            %[src],           %[src],               %[src_stride2]         \n\t"
+    "add.d            %[dst],           %[dst],               %[dst_stride]          \n\t"
+    "blt              %[zero],          %[height],            2b                     \n\t"
+    "3:                                                                              \n\t"
+    : [height]"+&r"(height), [src]"+&r"(src), [dst]"+&r"(dst), [src_stride2]"=&r"(src_stride2),
+      [src_stride3]"=&r"(src_stride3), [src_stride4]"=&r"(src_stride4)
+    : [dst_stride]"r"((int64_t) dst_stride), [src_stride]"r"((int64_t) src_stride),
+      [zero]"r"(zero), [i_4]"r"(i_4), [i_offset]"r"(i_offset), [i_scale]"r"(i_scale)
+    : "memory"
+    );
+}
+
+#define MC_WEIGHT(func)                                                                                             \
+static void (* mc##func##_wtab_lasx[6])( uint8_t *, intptr_t, uint8_t *, intptr_t, const x264_weight_t *, int ) =   \
+{                                                                                                                   \
+    mc_weight_w4##func##_lasx,                                                                                 \
+    mc_weight_w4##func##_lasx,                                                                                 \
+    mc_weight_w8##func##_lasx,                                                                                 \
+    mc_weight_w16##func##_lasx,                                                                                \
+    mc_weight_w16##func##_lasx,                                                                                \
+    mc_weight_w20##func##_lasx,                                                                                \
+};
+
+#if !HIGH_BIT_DEPTH
+MC_WEIGHT()
+MC_WEIGHT(_noden)
+#endif
+
+static void weight_cache_lasx( x264_t *h, x264_weight_t *w )
+{
+    if ( w->i_denom >= 1)
+    {
+        w->weightfn = mc_wtab_lasx;
+    }
+    else
+        w->weightfn = mc_noden_wtab_lasx;
+}
+
+static weight_fn_t mc_weight_wtab_lasx[6] =
+{
+    mc_weight_w4_lasx,
+    mc_weight_w4_lasx,
+    mc_weight_w8_lasx,
+    mc_weight_w16_lasx,
+    mc_weight_w16_lasx,
+    mc_weight_w20_lasx,
+};
+
+static void avc_biwgt_opscale_4x2_nw_lasx( uint8_t *p_src1,
+                                           int32_t i_src1_stride,
+                                           uint8_t *p_src2,
+                                           int32_t i_src2_stride,
+                                           uint8_t *p_dst, int32_t i_dst_stride,
+                                           int32_t i_log2_denom,
+                                           int32_t i_src1_weight,
+                                           int32_t i_src2_weight )
+{
+    __m256i src1_wgt, src2_wgt, wgt;
+    __m256i src0, src1, src2;
+    __m256i denom;
+
+    src1_wgt = __lasx_xvreplgr2vr_b( i_src1_weight );
+    src2_wgt = __lasx_xvreplgr2vr_b( i_src2_weight );
+    denom = __lasx_xvreplgr2vr_h( i_log2_denom + 1 );
+
+    wgt = __lasx_xvpackev_b( src2_wgt, src1_wgt );
+
+    src0 = __lasx_xvldrepl_w( p_src1, 0 );
+    p_src1 += i_src1_stride;
+    src1 = __lasx_xvldrepl_w( p_src1, 0 );
+    src2 = __lasx_xvpackev_w( src1, src0 );
+
+    src0 = __lasx_xvldrepl_w( p_src2, 0 );
+    p_src2 += i_src2_stride;
+    src1 = __lasx_xvldrepl_w( p_src2, 0 );
+    src0 = __lasx_xvpackev_w( src1, src0 );
+
+    src0 = __lasx_xvilvl_b( src0, src2 );
+
+    src2 = __lasx_xvmulwev_h_bu_b(src0, wgt);
+    src0 = __lasx_xvmaddwod_h_bu_b(src2, src0, wgt);
+    src0 = __lasx_xvmaxi_h( src0, 0 );
+    src0 = __lasx_xvssrlrn_bu_h(src0, denom);
+
+    __lasx_xvstelm_w(src0, p_dst, 0, 0);
+    __lasx_xvstelm_w(src0, p_dst + i_dst_stride, 0, 1);
+}
+
+static void avc_biwgt_opscale_4x4multiple_nw_lasx( uint8_t *p_src1,
+                                                   int32_t i_src1_stride,
+                                                   uint8_t *p_src2,
+                                                   int32_t i_src2_stride,
+                                                   uint8_t *p_dst,
+                                                   int32_t i_dst_stride,
+                                                   int32_t i_height,
+                                                   int32_t i_log2_denom,
+                                                   int32_t i_src1_weight,
+                                                   int32_t i_src2_weight )
+{
+    uint8_t u_cnt;
+    __m256i src1_wgt, src2_wgt, wgt;
+    __m256i src0, src1, src2, src3, tmp0;
+    __m256i denom;
+    int32_t i_dst_stride_x2 = i_dst_stride << 1;
+    int32_t i_dst_stride_x4 = i_dst_stride << 2;
+    int32_t i_dst_stride_x3 = i_dst_stride_x2 + i_dst_stride;
+
+    src1_wgt = __lasx_xvreplgr2vr_b( i_src1_weight );
+    src2_wgt = __lasx_xvreplgr2vr_b( i_src2_weight );
+    denom = __lasx_xvreplgr2vr_h( i_log2_denom + 1 );
+
+    wgt = __lasx_xvpackev_b( src2_wgt, src1_wgt );
+
+    for( u_cnt = ( i_height >> 2 ); u_cnt--; )
+    {
+        src0 = __lasx_xvldrepl_w( p_src1, 0 );
+        p_src1 += i_src1_stride;
+        src1 = __lasx_xvldrepl_w( p_src1, 0 );
+        p_src1 += i_src1_stride;
+        src2 = __lasx_xvldrepl_w( p_src1, 0 );
+        p_src1 += i_src1_stride;
+        src3 = __lasx_xvldrepl_w( p_src1, 0 );
+        p_src1 += i_src1_stride;
+        src0 = __lasx_xvpackev_w( src1, src0 );
+        src1 = __lasx_xvpackev_w( src3, src2 );
+        tmp0 = __lasx_xvpermi_q( src0, src1, 0x02 );
+
+        src0 = __lasx_xvldrepl_w( p_src2, 0 );
+        p_src2 += i_src2_stride;
+        src1 = __lasx_xvldrepl_w( p_src2, 0 );
+        p_src2 += i_src2_stride;
+        src2 = __lasx_xvldrepl_w( p_src2, 0 );
+        p_src2 += i_src2_stride;
+        src3 = __lasx_xvldrepl_w( p_src2, 0 );
+        p_src2 += i_src2_stride;
+        src0 = __lasx_xvpackev_w( src1, src0 );
+        src1 = __lasx_xvpackev_w( src3, src2 );
+        src0 = __lasx_xvpermi_q( src0, src1, 0x02 );
+
+        src0 = __lasx_xvilvl_b( src0, tmp0 );
+
+        src2 = __lasx_xvmulwev_h_bu_b(src0, wgt);
+        src0 = __lasx_xvmaddwod_h_bu_b(src2, src0, wgt);
+        src0 = __lasx_xvmaxi_h( src0, 0 );
+        src0 = __lasx_xvssrlrn_bu_h(src0, denom);
+
+        __lasx_xvstelm_w(src0, p_dst, 0, 0);
+        __lasx_xvstelm_w(src0, p_dst + i_dst_stride, 0, 1);
+        __lasx_xvstelm_w(src0, p_dst + i_dst_stride_x2, 0, 4);
+        __lasx_xvstelm_w(src0, p_dst + i_dst_stride_x3, 0, 5);
+        p_dst += i_dst_stride_x4;
+    }
+}
+
+static void avc_biwgt_opscale_4width_nw_lasx( uint8_t *p_src1,
+                                              int32_t i_src1_stride,
+                                              uint8_t *p_src2,
+                                              int32_t i_src2_stride,
+                                              uint8_t *p_dst,
+                                              int32_t i_dst_stride,
+                                              int32_t i_height,
+                                              int32_t i_log2_denom,
+                                              int32_t i_src1_weight,
+                                              int32_t i_src2_weight )
+{
+    if( 2 == i_height )
+    {
+        avc_biwgt_opscale_4x2_nw_lasx( p_src1, i_src1_stride,
+                                       p_src2, i_src2_stride,
+                                       p_dst, i_dst_stride,
+                                       i_log2_denom, i_src1_weight,
+                                       i_src2_weight );
+    }
+    else
+    {
+        avc_biwgt_opscale_4x4multiple_nw_lasx( p_src1, i_src1_stride,
+                                               p_src2, i_src2_stride,
+                                               p_dst, i_dst_stride,
+                                               i_height, i_log2_denom,
+                                               i_src1_weight,
+                                               i_src2_weight );
+    }
+}
+
+static void avc_biwgt_opscale_8width_nw_lasx( uint8_t *p_src1,
+                                              int32_t i_src1_stride,
+                                              uint8_t *p_src2,
+                                              int32_t i_src2_stride,
+                                              uint8_t *p_dst,
+                                              int32_t i_dst_stride,
+                                              int32_t i_height,
+                                              int32_t i_log2_denom,
+                                              int32_t i_src1_weight,
+                                              int32_t i_src2_weight )
+{
+    uint8_t u_cnt;
+    __m256i src1_wgt, src2_wgt, wgt;
+    __m256i src0, src1, src2, src3;
+    __m256i denom;
+    int32_t i_dst_stride_x2 = i_dst_stride << 1;
+
+    src1_wgt = __lasx_xvreplgr2vr_b( i_src1_weight );
+    src2_wgt = __lasx_xvreplgr2vr_b( i_src2_weight );
+    denom = __lasx_xvreplgr2vr_h( i_log2_denom + 1 );
+
+    wgt = __lasx_xvpackev_b( src2_wgt, src1_wgt );
+
+#define BIWGT_OPSCALE_8W_NW                              \
+    src0 = __lasx_xvldrepl_d( p_src1, 0 );               \
+    p_src1 += i_src1_stride;                             \
+    src1 = __lasx_xvldrepl_d( p_src1, 0 );               \
+    p_src1 += i_src1_stride;                             \
+                                                         \
+    src2 = __lasx_xvldrepl_d( p_src2, 0 );               \
+    p_src2 += i_src2_stride;                             \
+    src3 = __lasx_xvldrepl_d( p_src2, 0 );               \
+    p_src2 += i_src2_stride;                             \
+                                                         \
+    src0 = __lasx_xvpermi_q( src0, src1, 0x02 );         \
+    src1 = __lasx_xvpermi_q( src2, src3, 0x02 );         \
+    src0 = __lasx_xvilvl_b( src1, src0 );                \
+                                                         \
+    src2 = __lasx_xvmulwev_h_bu_b(src0, wgt);            \
+    src0 = __lasx_xvmaddwod_h_bu_b(src2, src0, wgt);     \
+    src0 = __lasx_xvmaxi_h( src0, 0 );                   \
+    src0 = __lasx_xvssrlrn_bu_h(src0, denom);            \
+                                                         \
+    __lasx_xvstelm_d(src0, p_dst, 0, 0);                 \
+    __lasx_xvstelm_d(src0, p_dst + i_dst_stride, 0, 2);  \
+    p_dst += i_dst_stride_x2;                            \
+
+    for( u_cnt = ( i_height >> 2 ); u_cnt--; )
+    {
+        BIWGT_OPSCALE_8W_NW;
+        BIWGT_OPSCALE_8W_NW;
+    }
+
+#undef BIWGT_OPSCALE_8W_NW
+
+}
+static void avc_biwgt_opscale_16width_nw_lasx( uint8_t *p_src1,
+                                               int32_t i_src1_stride,
+                                               uint8_t *p_src2,
+                                               int32_t i_src2_stride,
+                                               uint8_t *p_dst,
+                                               int32_t i_dst_stride,
+                                               int32_t i_height,
+                                               int32_t i_log2_denom,
+                                               int32_t i_src1_weight,
+                                               int32_t i_src2_weight )
+{
+    uint8_t u_cnt;
+    __m256i src1_wgt, src2_wgt, wgt;
+    __m256i src0, src1, src2, src3, src4, src5, src6, src7;
+    __m256i denom;
+    int32_t i_src1_stride_x2 = i_src1_stride << 1;
+    int32_t i_src1_stride_x4 = i_src1_stride << 2;
+    int32_t i_src2_stride_x2 = i_src2_stride << 1;
+    int32_t i_src2_stride_x4 = i_src2_stride << 2;
+    int32_t i_src1_stride_x3 = i_src1_stride_x2 + i_src1_stride;
+    int32_t i_src2_stride_x3 = i_src2_stride_x2 + i_src2_stride;
+
+    src1_wgt = __lasx_xvreplgr2vr_b( i_src1_weight );
+    src2_wgt = __lasx_xvreplgr2vr_b( i_src2_weight );
+    denom = __lasx_xvreplgr2vr_h( i_log2_denom + 1 );
+
+    wgt = __lasx_xvpackev_b( src2_wgt, src1_wgt );
+
+#define BIWGT_OPSCALE_16W_NW( srcA, srcB )           \
+    srcA = __lasx_xvpermi_d( srcA, 0x50 );           \
+    srcB = __lasx_xvpermi_d( srcB, 0x50 );           \
+    srcA = __lasx_xvilvl_b( srcB, srcA );            \
+                                                     \
+    srcB = __lasx_xvmulwev_h_bu_b(srcA, wgt);        \
+    srcA = __lasx_xvmaddwod_h_bu_b(srcB, srcA, wgt); \
+    srcA = __lasx_xvmaxi_h( srcA, 0 );               \
+    srcA = __lasx_xvssrlrn_bu_h(srcA, denom);        \
+                                                     \
+    __lasx_xvstelm_d(srcA, p_dst, 0, 0);             \
+    __lasx_xvstelm_d(srcA, p_dst + 8, 0, 2);         \
+    p_dst += i_dst_stride;
+
+    for( u_cnt = ( i_height >> 2 ); u_cnt--; )
+    {
+        DUP4_ARG2( __lasx_xvldx, p_src1, 0, p_src1, i_src1_stride, p_src1,
+                   i_src1_stride_x2, p_src1, i_src1_stride_x3, src0, src1, src2, src3 );
+        p_src1 += i_src1_stride_x4;
+
+        DUP4_ARG2( __lasx_xvldx, p_src2, 0, p_src2, i_src2_stride, p_src2,
+                   i_src2_stride_x2, p_src2, i_src2_stride_x3, src4, src5, src6, src7 );
+        p_src2 += i_src2_stride_x4;
+
+        BIWGT_OPSCALE_16W_NW( src0, src4 );
+        BIWGT_OPSCALE_16W_NW( src1, src5 );
+        BIWGT_OPSCALE_16W_NW( src2, src6 );
+        BIWGT_OPSCALE_16W_NW( src3, src7 );
+    }
+
+#undef BIWGT_OPSCALE_16W_NW
+
+}
+
+static void avc_biwgt_opscale_4x2_lasx( uint8_t *p_src1,
+                                        int32_t i_src1_stride,
+                                        uint8_t *p_src2,
+                                        int32_t i_src2_stride,
+                                        uint8_t *p_dst, int32_t i_dst_stride,
+                                        int32_t i_log2_denom,
+                                        int32_t i_src1_weight,
+                                        int32_t i_src2_weight,
+                                        int32_t i_offset_in )
+{
+    __m256i src1_wgt, src2_wgt, wgt;
+    __m256i src0, src1, src2;
+    __m256i denom, offset;
+
+    i_offset_in = ( ( i_offset_in + 1 ) | 1 ) << i_log2_denom;
+
+    src1_wgt = __lasx_xvreplgr2vr_b( i_src1_weight );
+    src2_wgt = __lasx_xvreplgr2vr_b( i_src2_weight );
+    denom = __lasx_xvreplgr2vr_h( i_log2_denom + 1 );
+    offset = __lasx_xvreplgr2vr_h( i_offset_in );
+
+    wgt = __lasx_xvpackev_b( src2_wgt, src1_wgt );
+
+    src0 = __lasx_xvldrepl_w( p_src1, 0 );
+    p_src1 += i_src1_stride;
+    src1 = __lasx_xvldrepl_w( p_src1, 0 );
+    src2 = __lasx_xvpackev_w( src1, src0 );
+
+    src0 = __lasx_xvldrepl_w( p_src2, 0 );
+    p_src2 += i_src2_stride;
+    src1 = __lasx_xvldrepl_w( p_src2, 0 );
+    src0 = __lasx_xvpackev_w( src1, src0 );
+
+    src0 = __lasx_xvilvl_b( src0, src2 );
+
+    src0 = __lasx_xvdp2_h_bu( src0, wgt );
+    src0 = __lasx_xvsadd_h( src0, offset );
+    src0 = __lasx_xvmaxi_h( src0, 0 );
+    src0 = __lasx_xvssrln_bu_h(src0, denom);
+
+    __lasx_xvstelm_w(src0, p_dst, 0, 0);
+    __lasx_xvstelm_w(src0, p_dst + i_dst_stride, 0, 1);
+}
+
+static void avc_biwgt_opscale_4x4multiple_lasx( uint8_t *p_src1,
+                                                int32_t i_src1_stride,
+                                                uint8_t *p_src2,
+                                                int32_t i_src2_stride,
+                                                uint8_t *p_dst,
+                                                int32_t i_dst_stride,
+                                                int32_t i_height,
+                                                int32_t i_log2_denom,
+                                                int32_t i_src1_weight,
+                                                int32_t i_src2_weight,
+                                                int32_t i_offset_in )
+{
+    uint8_t u_cnt;
+    __m256i src1_wgt, src2_wgt, wgt;
+    __m256i src0, src1, src2, src3, tmp0;
+    __m256i denom, offset;
+    int32_t i_dst_stride_x2 = i_dst_stride << 1;
+    int32_t i_dst_stride_x4 = i_dst_stride << 2;
+    int32_t i_dst_stride_x3 = i_dst_stride_x2 + i_dst_stride;
+
+    i_offset_in = ( ( i_offset_in + 1 ) | 1 ) << i_log2_denom;
+
+    src1_wgt = __lasx_xvreplgr2vr_b( i_src1_weight );
+    src2_wgt = __lasx_xvreplgr2vr_b( i_src2_weight );
+    denom = __lasx_xvreplgr2vr_h( i_log2_denom + 1 );
+    offset = __lasx_xvreplgr2vr_h( i_offset_in );
+
+    wgt = __lasx_xvpackev_b( src2_wgt, src1_wgt );
+
+    for( u_cnt = ( i_height >> 2 ); u_cnt--; )
+    {
+        src0 = __lasx_xvldrepl_w( p_src1, 0 );
+        p_src1 += i_src1_stride;
+        src1 = __lasx_xvldrepl_w( p_src1, 0 );
+        p_src1 += i_src1_stride;
+        src2 = __lasx_xvldrepl_w( p_src1, 0 );
+        p_src1 += i_src1_stride;
+        src3 = __lasx_xvldrepl_w( p_src1, 0 );
+        p_src1 += i_src1_stride;
+        src0 = __lasx_xvpackev_w( src1, src0 );
+        src1 = __lasx_xvpackev_w( src3, src2 );
+        tmp0 = __lasx_xvpermi_q( src0, src1, 0x02 );
+
+        src0 = __lasx_xvldrepl_w( p_src2, 0 );
+        p_src2 += i_src2_stride;
+        src1 = __lasx_xvldrepl_w( p_src2, 0 );
+        p_src2 += i_src2_stride;
+        src2 = __lasx_xvldrepl_w( p_src2, 0 );
+        p_src2 += i_src2_stride;
+        src3 = __lasx_xvldrepl_w( p_src2, 0 );
+        p_src2 += i_src2_stride;
+        src0 = __lasx_xvpackev_w( src1, src0 );
+        src1 = __lasx_xvpackev_w( src3, src2 );
+        src0 = __lasx_xvpermi_q( src0, src1, 0x02 );
+
+        src0 = __lasx_xvilvl_b( src0, tmp0 );
+
+        src0 = __lasx_xvdp2_h_bu( src0, wgt );
+        src0 = __lasx_xvsadd_h( src0, offset );
+        src0 = __lasx_xvmaxi_h( src0, 0 );
+        src0 = __lasx_xvssrln_bu_h(src0, denom);
+
+        __lasx_xvstelm_w(src0, p_dst, 0, 0);
+        __lasx_xvstelm_w(src0, p_dst + i_dst_stride, 0, 1);
+        __lasx_xvstelm_w(src0, p_dst + i_dst_stride_x2, 0, 4);
+        __lasx_xvstelm_w(src0, p_dst + i_dst_stride_x3, 0, 5);
+        p_dst += i_dst_stride_x4;
+    }
+}
+
+static void avc_biwgt_opscale_4width_lasx( uint8_t *p_src1,
+                                           int32_t i_src1_stride,
+                                           uint8_t *p_src2,
+                                           int32_t i_src2_stride,
+                                           uint8_t *p_dst,
+                                           int32_t i_dst_stride,
+                                           int32_t i_height,
+                                           int32_t i_log2_denom,
+                                           int32_t i_src1_weight,
+                                           int32_t i_src2_weight,
+                                           int32_t i_offset_in )
+{
+    if( 2 == i_height )
+    {
+        avc_biwgt_opscale_4x2_lasx( p_src1, i_src1_stride,
+                                    p_src2, i_src2_stride,
+                                    p_dst, i_dst_stride,
+                                    i_log2_denom, i_src1_weight,
+                                    i_src2_weight, i_offset_in );
+    }
+    else
+    {
+        avc_biwgt_opscale_4x4multiple_lasx( p_src1, i_src1_stride,
+                                            p_src2, i_src2_stride,
+                                            p_dst, i_dst_stride,
+                                            i_height, i_log2_denom,
+                                            i_src1_weight,
+                                            i_src2_weight, i_offset_in );
+    }
+}
+
+static void avc_biwgt_opscale_8width_lasx( uint8_t *p_src1,
+                                           int32_t i_src1_stride,
+                                           uint8_t *p_src2,
+                                           int32_t i_src2_stride,
+                                           uint8_t *p_dst,
+                                           int32_t i_dst_stride,
+                                           int32_t i_height,
+                                           int32_t i_log2_denom,
+                                           int32_t i_src1_weight,
+                                           int32_t i_src2_weight,
+                                           int32_t i_offset_in )
+{
+    uint8_t u_cnt;
+    __m256i src1_wgt, src2_wgt, wgt;
+    __m256i src0, src1, src2, src3;
+    __m256i denom, offset;
+    int32_t i_dst_stride_x2 = ( i_dst_stride << 1 );
+
+    i_offset_in = ( ( i_offset_in + 1 ) | 1 ) << i_log2_denom;
+
+    src1_wgt = __lasx_xvreplgr2vr_b( i_src1_weight );
+    src2_wgt = __lasx_xvreplgr2vr_b( i_src2_weight );
+    denom = __lasx_xvreplgr2vr_h( i_log2_denom + 1 );
+    offset = __lasx_xvreplgr2vr_h( i_offset_in );
+
+    wgt = __lasx_xvpackev_b( src2_wgt, src1_wgt );
+
+#define BIWGT_OPSCALE_8W                                 \
+    src0 = __lasx_xvldrepl_d( p_src1, 0 );               \
+    p_src1 += i_src1_stride;                             \
+    src1 = __lasx_xvldrepl_d( p_src1, 0 );               \
+    p_src1 += i_src1_stride;                             \
+                                                         \
+    src2 = __lasx_xvldrepl_d( p_src2, 0 );               \
+    p_src2 += i_src2_stride;                             \
+    src3 = __lasx_xvldrepl_d( p_src2, 0 );               \
+    p_src2 += i_src2_stride;                             \
+                                                         \
+    src0 = __lasx_xvpermi_q( src0, src1, 0x02 );         \
+    src1 = __lasx_xvpermi_q( src2, src3, 0x02 );         \
+    src0 = __lasx_xvilvl_b( src1, src0 );                \
+                                                         \
+    src0 = __lasx_xvdp2_h_bu( src0, wgt );               \
+    src0 = __lasx_xvsadd_h( src0, offset );              \
+    src0 = __lasx_xvmaxi_h( src0, 0 );                   \
+    src0 = __lasx_xvssrln_bu_h(src0, denom);             \
+                                                         \
+    __lasx_xvstelm_d(src0, p_dst, 0, 0);                 \
+    __lasx_xvstelm_d(src0, p_dst + i_dst_stride, 0, 2);  \
+    p_dst += i_dst_stride_x2;
+
+    for( u_cnt = ( i_height >> 2 ); u_cnt--; )
+    {
+        BIWGT_OPSCALE_8W;
+        BIWGT_OPSCALE_8W;
+    }
+
+#undef BIWGT_OPSCALE_8W
+
+}
+
+static void avc_biwgt_opscale_16width_lasx( uint8_t *p_src1,
+                                            int32_t i_src1_stride,
+                                            uint8_t *p_src2,
+                                            int32_t i_src2_stride,
+                                            uint8_t *p_dst,
+                                            int32_t i_dst_stride,
+                                            int32_t i_height,
+                                            int32_t i_log2_denom,
+                                            int32_t i_src1_weight,
+                                            int32_t i_src2_weight,
+                                            int32_t i_offset_in )
+{
+    uint8_t u_cnt;
+    __m256i src1_wgt, src2_wgt, wgt;
+    __m256i src0, src1, src2, src3, src4, src5, src6, src7;
+    __m256i denom, offset;
+    int32_t i_src1_stride_x2 = i_src1_stride << 1;
+    int32_t i_src1_stride_x4 = i_src1_stride << 2;
+    int32_t i_src2_stride_x2 = i_src2_stride << 1;
+    int32_t i_src2_stride_x4 = i_src2_stride << 2;
+    int32_t i_src1_stride_x3 = i_src1_stride_x2 + i_src1_stride;
+    int32_t i_src2_stride_x3 = i_src2_stride_x2 + i_src2_stride;
+
+    i_offset_in = ( ( i_offset_in + 1 ) | 1 ) << i_log2_denom;
+
+    src1_wgt = __lasx_xvreplgr2vr_b( i_src1_weight );
+    src2_wgt = __lasx_xvreplgr2vr_b( i_src2_weight );
+    denom = __lasx_xvreplgr2vr_h( i_log2_denom + 1 );
+    offset = __lasx_xvreplgr2vr_h( i_offset_in );
+
+    wgt = __lasx_xvpackev_b( src2_wgt, src1_wgt );
+
+#define BIWGT_OPSCALE_16W( srcA, srcB )          \
+    srcA = __lasx_xvpermi_d( srcA, 0x50 );       \
+    srcB = __lasx_xvpermi_d( srcB, 0x50 );       \
+    srcA = __lasx_xvilvl_b( srcB, srcA );        \
+                                                 \
+    srcA = __lasx_xvdp2_h_bu( srcA, wgt );       \
+    srcA = __lasx_xvsadd_h( srcA, offset );      \
+    srcA = __lasx_xvmaxi_h( srcA, 0 );           \
+    srcA = __lasx_xvssrln_bu_h(srcA, denom);     \
+                                                 \
+    __lasx_xvstelm_d(srcA, p_dst, 0, 0);         \
+    __lasx_xvstelm_d(srcA, p_dst + 8, 0, 2);     \
+    p_dst += i_dst_stride;
+
+    for( u_cnt = ( i_height >> 2 ); u_cnt--; )
+    {
+        DUP4_ARG2( __lasx_xvldx, p_src1, 0, p_src1, i_src1_stride, p_src1,
+                   i_src1_stride_x2, p_src1, i_src1_stride_x3, src0, src1, src2, src3 );
+        p_src1 += i_src1_stride_x4;
+
+        DUP4_ARG2( __lasx_xvldx, p_src2, 0, p_src2, i_src2_stride, p_src2,
+                   i_src2_stride_x2, p_src2, i_src2_stride_x3, src4, src5, src6, src7 );
+        p_src2 += i_src2_stride_x4;
+
+        BIWGT_OPSCALE_16W( src0, src4 );
+        BIWGT_OPSCALE_16W( src1, src5 );
+        BIWGT_OPSCALE_16W( src2, src6 );
+        BIWGT_OPSCALE_16W( src3, src7 );
+    }
+
+#undef BIWGT_OPSCALE_16W
+
+}
+
+static void avg_src_width4_lasx( uint8_t *p_src1, int32_t i_src1_stride,
+                                 uint8_t *p_src2, int32_t i_src2_stride,
+                                 uint8_t *p_dst, int32_t i_dst_stride,
+                                 int32_t i_height )
+{
+    int32_t i_cnt;
+    __m256i src0, src1;
+    __m256i dst0, dst1;
+    int32_t i_src1_stride_x2 = i_src1_stride << 1;
+    int32_t i_src2_stride_x2 = i_src2_stride << 1;
+
+    for( i_cnt = ( i_height >> 1 ); i_cnt--; )
+    {
+        DUP2_ARG2(__lasx_xvldx, p_src1, 0, p_src1, i_src1_stride, src0, src1);
+        p_src1 += i_src1_stride_x2;
+        DUP2_ARG2(__lasx_xvldx, p_src2, 0, p_src2, i_src2_stride, dst0, dst1);
+        p_src2 += i_src2_stride_x2;
+
+        DUP2_ARG2( __lasx_xvavgr_bu, src0, dst0, src1, dst1, dst0, dst1 );
+        __lasx_xvstelm_w( dst0, p_dst, 0, 0 );
+        p_dst += i_dst_stride;
+        __lasx_xvstelm_w( dst1, p_dst, 0, 0 );
+        p_dst += i_dst_stride;
+    }
+}
+
+static void avg_src_width8_lasx( uint8_t *p_src1, int32_t i_src1_stride,
+                                 uint8_t *p_src2, int32_t i_src2_stride,
+                                 uint8_t *p_dst, int32_t i_dst_stride,
+                                 int32_t i_height )
+{
+    int32_t i_cnt = i_height >> 2;
+    int32_t i_src1_stride_x2, i_src1_stride_x3, i_src1_stride_x4;
+    int32_t i_src2_stride_x2, i_src2_stride_x3, i_src2_stride_x4;
+
+    __asm__ volatile(
+    "slli.w    %[src1_stride2],  %[src1_stride1],  1                    \n\t"
+    "add.w     %[src1_stride3],  %[src1_stride2],  %[src1_stride1]      \n\t"
+    "slli.w    %[src1_stride4],  %[src1_stride1],  2                    \n\t"
+    "slli.w    %[src2_stride2],  %[src2_stride1],  1                    \n\t"
+    "add.w     %[src2_stride3],  %[src2_stride2],  %[src2_stride1]      \n\t"
+    "slli.w    %[src2_stride4],  %[src2_stride1],  2                    \n\t"
+    "beqz      %[cnt],           2f                                     \n\t"
+    "1:                                                                 \n\t"
+    "addi.w    %[cnt],           %[cnt],           -1                   \n\t"
+    "vld       $vr0,             %[src1],          0                    \n\t"
+    "vldx      $vr1,             %[src1],          %[src1_stride1]      \n\t"
+    "vldx      $vr2,             %[src1],          %[src1_stride2]      \n\t"
+    "vldx      $vr3,             %[src1],          %[src1_stride3]      \n\t"
+    "vld       $vr4,             %[src2],          0                    \n\t"
+    "vldx      $vr5,             %[src2],          %[src2_stride1]      \n\t"
+    "vldx      $vr6,             %[src2],          %[src2_stride2]      \n\t"
+    "vldx      $vr7,             %[src2],          %[src2_stride3]      \n\t"
+    "vavgr.bu  $vr0,             $vr0,             $vr4                 \n\t"
+    "vavgr.bu  $vr1,             $vr1,             $vr5                 \n\t"
+    "vavgr.bu  $vr2,             $vr2,             $vr6                 \n\t"
+    "vavgr.bu  $vr3,             $vr3,             $vr7                 \n\t"
+    "vstelm.d  $vr0,             %[dst],           0,              0    \n\t"
+    "add.d     %[dst],           %[dst],           %[dst_stride1]       \n\t"
+    "vstelm.d  $vr1,             %[dst],           0,              0    \n\t"
+    "add.d     %[dst],           %[dst],           %[dst_stride1]       \n\t"
+    "vstelm.d  $vr2,             %[dst],           0,              0    \n\t"
+    "add.d     %[dst],           %[dst],           %[dst_stride1]       \n\t"
+    "vstelm.d  $vr3,             %[dst],           0,              0    \n\t"
+    "add.d     %[dst],           %[dst],           %[dst_stride1]       \n\t"
+    "add.d     %[src1],          %[src1],          %[src1_stride4]      \n\t"
+    "add.d     %[src2],          %[src2],          %[src2_stride4]      \n\t"
+    "bnez      %[cnt],           1b                                     \n\t"
+    "2:                                                                 \n\t"
+     : [src1]"+&r"(p_src1),
+       [src2]"+&r"(p_src2),
+       [src1_stride2]"=&r"(i_src1_stride_x2),
+       [src1_stride3]"=&r"(i_src1_stride_x3),
+       [src1_stride4]"=&r"(i_src1_stride_x4),
+       [src2_stride2]"=&r"(i_src2_stride_x2),
+       [src2_stride3]"=&r"(i_src2_stride_x3),
+       [src2_stride4]"=&r"(i_src2_stride_x4),
+       [dst]"+&r"(p_dst), [cnt]"+&r"(i_cnt)
+     : [src1_stride1]"r"(i_src1_stride),
+       [src2_stride1]"r"(i_src2_stride),
+       [dst_stride1]"r"(i_dst_stride)
+     : "memory"
+    );
+}
+
+static void avg_src_width16_lasx( uint8_t *p_src1, int32_t i_src1_stride,
+                                  uint8_t *p_src2, int32_t i_src2_stride,
+                                  uint8_t *p_dst, int32_t i_dst_stride,
+                                  int32_t i_height )
+{
+    int32_t i_cnt = i_height >> 3;
+    int32_t i_src1_stride_x2, i_src1_stride_x3, i_src1_stride_x4;
+    int32_t i_src2_stride_x2, i_src2_stride_x3, i_src2_stride_x4;
+    int32_t i_dst_stride_x2, i_dst_stride_x3, i_dst_stride_x4;
+
+    __asm__ volatile(
+    "slli.w    %[src1_stride2],  %[src1_stride1],  1                    \n\t"
+    "add.w     %[src1_stride3],  %[src1_stride2],  %[src1_stride1]      \n\t"
+    "slli.w    %[src1_stride4],  %[src1_stride1],  2                    \n\t"
+    "slli.w    %[src2_stride2],  %[src2_stride1],  1                    \n\t"
+    "add.w     %[src2_stride3],  %[src2_stride2],  %[src2_stride1]      \n\t"
+    "slli.w    %[src2_stride4],  %[src2_stride1],  2                    \n\t"
+    "slli.w    %[dst_stride2],   %[dst_stride1],   1                    \n\t"
+    "add.w     %[dst_stride3],   %[dst_stride2],   %[dst_stride1]       \n\t"
+    "slli.w    %[dst_stride4],   %[dst_stride1],   2                    \n\t"
+    "beqz      %[cnt],           2f                                     \n\t"
+    "1:                                                                 \n\t"
+    "addi.w    %[cnt],           %[cnt],           -1                   \n\t"
+    "vld       $vr0,             %[src1],          0                    \n\t"
+    "vldx      $vr1,             %[src1],          %[src1_stride1]      \n\t"
+    "vldx      $vr2,             %[src1],          %[src1_stride2]      \n\t"
+    "vldx      $vr3,             %[src1],          %[src1_stride3]      \n\t"
+    "vld       $vr4,             %[src2],          0                    \n\t"
+    "vldx      $vr5,             %[src2],          %[src2_stride1]      \n\t"
+    "vldx      $vr6,             %[src2],          %[src2_stride2]      \n\t"
+    "vldx      $vr7,             %[src2],          %[src2_stride3]      \n\t"
+    "vavgr.bu  $vr0,             $vr0,             $vr4                 \n\t"
+    "vavgr.bu  $vr1,             $vr1,             $vr5                 \n\t"
+    "vavgr.bu  $vr2,             $vr2,             $vr6                 \n\t"
+    "vavgr.bu  $vr3,             $vr3,             $vr7                 \n\t"
+    "vst       $vr0,             %[dst],           0                    \n\t"
+    "vstx      $vr1,             %[dst],           %[dst_stride1]       \n\t"
+    "vstx      $vr2,             %[dst],           %[dst_stride2]       \n\t"
+    "vstx      $vr3,             %[dst],           %[dst_stride3]       \n\t"
+    "add.d     %[dst],           %[dst],           %[dst_stride4]       \n\t"
+    "add.d     %[src1],          %[src1],          %[src1_stride4]      \n\t"
+    "add.d     %[src2],          %[src2],          %[src2_stride4]      \n\t"
+
+    "vld       $vr0,             %[src1],          0                    \n\t"
+    "vldx      $vr1,             %[src1],          %[src1_stride1]      \n\t"
+    "vldx      $vr2,             %[src1],          %[src1_stride2]      \n\t"
+    "vldx      $vr3,             %[src1],          %[src1_stride3]      \n\t"
+    "vld       $vr4,             %[src2],          0                    \n\t"
+    "vldx      $vr5,             %[src2],          %[src2_stride1]      \n\t"
+    "vldx      $vr6,             %[src2],          %[src2_stride2]      \n\t"
+    "vldx      $vr7,             %[src2],          %[src2_stride3]      \n\t"
+    "vavgr.bu  $vr0,             $vr0,             $vr4                 \n\t"
+    "vavgr.bu  $vr1,             $vr1,             $vr5                 \n\t"
+    "vavgr.bu  $vr2,             $vr2,             $vr6                 \n\t"
+    "vavgr.bu  $vr3,             $vr3,             $vr7                 \n\t"
+    "vst       $vr0,             %[dst],           0                    \n\t"
+    "vstx      $vr1,             %[dst],           %[dst_stride1]       \n\t"
+    "vstx      $vr2,             %[dst],           %[dst_stride2]       \n\t"
+    "vstx      $vr3,             %[dst],           %[dst_stride3]       \n\t"
+    "add.d     %[dst],           %[dst],           %[dst_stride4]       \n\t"
+    "add.d     %[src1],          %[src1],          %[src1_stride4]      \n\t"
+    "add.d     %[src2],          %[src2],          %[src2_stride4]      \n\t"
+
+    "bnez      %[cnt],           1b                                     \n\t"
+    "2:                                                                 \n\t"
+     : [src1]"+&r"(p_src1),
+       [src2]"+&r"(p_src2),
+       [src1_stride2]"=&r"(i_src1_stride_x2),
+       [src1_stride3]"=&r"(i_src1_stride_x3),
+       [src1_stride4]"=&r"(i_src1_stride_x4),
+       [src2_stride2]"=&r"(i_src2_stride_x2),
+       [src2_stride3]"=&r"(i_src2_stride_x3),
+       [src2_stride4]"=&r"(i_src2_stride_x4),
+       [dst_stride2]"=&r"(i_dst_stride_x2),
+       [dst_stride3]"=&r"(i_dst_stride_x3),
+       [dst_stride4]"=&r"(i_dst_stride_x4),
+       [dst]"+&r"(p_dst), [cnt]"+&r"(i_cnt)
+     : [src1_stride1]"r"(i_src1_stride),
+       [src2_stride1]"r"(i_src2_stride),
+       [dst_stride1]"r"(i_dst_stride)
+     : "memory"
+    );
+}
+
+static void *x264_memcpy_aligned_lasx(void *dst, const void *src, size_t n)
+{
+    int64_t zero = 0, d;
+
+    __asm__ volatile(
+    "andi      %[d],            %[n],              16                   \n\t"
+    "beqz      %[d],            2f                                      \n\t"
+    "addi.d    %[n],            %[n],              -16                  \n\t"
+    "vld       $vr0,            %[src],            0                    \n\t"
+    "vst       $vr0,            %[dst],            0                    \n\t"
+    "addi.d    %[src],          %[src],            16                   \n\t"
+    "addi.d    %[dst],          %[dst],            16                   \n\t"
+    "2:                                                                 \n\t"
+    "andi      %[d],            %[n],              32                   \n\t"
+    "beqz      %[d],            3f                                      \n\t"
+    "addi.d    %[n],            %[n],              -32                  \n\t"
+    "xvld      $xr0,            %[src],            0                    \n\t"
+    "xvst      $xr0,            %[dst],            0                    \n\t"
+    "addi.d    %[src],          %[src],            32                   \n\t"
+    "addi.d    %[dst],          %[dst],            32                   \n\t"
+    "3:                                                                 \n\t"
+    "beqz      %[n],            5f                                      \n\t"
+    "4:                                                                 \n\t"
+    "addi.d    %[n],            %[n],              -64                  \n\t"
+    "xvld      $xr0,            %[src],            32                   \n\t"
+    "xvld      $xr1,            %[src],            0                    \n\t"
+    "xvst      $xr0,            %[dst],            32                   \n\t"
+    "xvst      $xr1,            %[dst],            0                    \n\t"
+    "addi.d    %[src],          %[src],            64                   \n\t"
+    "addi.d    %[dst],          %[dst],            64                   \n\t"
+    "blt       %[zero],         %[n],              4b                   \n\t"
+    "5:                                                                 \n\t"
+    : [dst]"+&r"(dst), [src]"+&r"(src), [n]"+&r"(n),
+      [d]"=&r"(d)
+    : [zero]"r"(zero)
+    : "memory"
+    );
+    return NULL;
+}
+
+static void pixel_avg_16x16_lasx( uint8_t *p_pix1, intptr_t pix1_stride,
+                                  uint8_t *p_pix2, intptr_t pix2_stride,
+                                  uint8_t *p_pix3, intptr_t pix3_stride,
+                                  int32_t i_weight )
+{
+    if( 32 == i_weight )
+    {
+        avg_src_width16_lasx( p_pix2, pix2_stride, p_pix3, pix3_stride,
+                              p_pix1, pix1_stride, 16 );
+    }
+    else if( i_weight < 0 || i_weight > 63 )
+    {
+        avc_biwgt_opscale_16width_nw_lasx( p_pix2, pix2_stride,
+                                           p_pix3, pix3_stride,
+                                           p_pix1, pix1_stride,
+                                           16, 5, i_weight,
+                                           ( 64 - i_weight ) );
+    }
+    else
+    {
+        avc_biwgt_opscale_16width_lasx( p_pix2, pix2_stride,
+                                        p_pix3, pix3_stride,
+                                        p_pix1, pix1_stride,
+                                        16, 5, i_weight,
+                                        ( 64 - i_weight ), 0 );
+    }
+}
+
+static void pixel_avg_16x8_lasx( uint8_t *p_pix1, intptr_t pix1_stride,
+                                 uint8_t *p_pix2, intptr_t pix2_stride,
+                                 uint8_t *p_pix3, intptr_t pix3_stride,
+                                 int32_t i_weight )
+{
+    if( 32 == i_weight )
+    {
+        avg_src_width16_lasx( p_pix2, pix2_stride, p_pix3, pix3_stride,
+                              p_pix1, pix1_stride, 8 );
+    }
+    else if( i_weight < 0 || i_weight > 63 )
+    {
+        avc_biwgt_opscale_16width_nw_lasx( p_pix2, pix2_stride,
+                                           p_pix3, pix3_stride,
+                                           p_pix1, pix1_stride,
+                                           8, 5, i_weight,
+                                           ( 64 - i_weight ) );
+    }
+    else
+    {
+        avc_biwgt_opscale_16width_lasx( p_pix2, pix2_stride,
+                                        p_pix3, pix3_stride,
+                                        p_pix1, pix1_stride,
+                                        8, 5, i_weight,
+                                        ( 64 - i_weight ), 0 );
+    }
+}
+
+static void pixel_avg_8x16_lasx( uint8_t *p_pix1, intptr_t pix1_stride,
+                                 uint8_t *p_pix2, intptr_t pix2_stride,
+                                 uint8_t *p_pix3, intptr_t pix3_stride,
+                                 int32_t i_weight )
+{
+    if( 32 == i_weight )
+    {
+        avg_src_width8_lasx( p_pix2, pix2_stride, p_pix3, pix3_stride,
+                             p_pix1, pix1_stride, 16 );
+    }
+    else if( i_weight < 0 || i_weight > 63 )
+    {
+        avc_biwgt_opscale_8width_nw_lasx( p_pix2, pix2_stride,
+                                          p_pix3, pix3_stride,
+                                          p_pix1, pix1_stride, 16, 5, i_weight,
+                                          ( 64 - i_weight ) );
+    }
+    else
+    {
+        avc_biwgt_opscale_8width_lasx( p_pix2, pix2_stride,
+                                       p_pix3, pix3_stride,
+                                       p_pix1, pix1_stride, 16, 5, i_weight,
+                                       ( 64 - i_weight ), 0 );
+    }
+}
+
+static void pixel_avg_8x8_lasx( uint8_t *p_pix1, intptr_t pix1_stride,
+                                uint8_t *p_pix2, intptr_t pix2_stride,
+                                uint8_t *p_pix3, intptr_t pix3_stride,
+                                int32_t i_weight )
+{
+    if( 32 == i_weight )
+    {
+        avg_src_width8_lasx( p_pix2, pix2_stride, p_pix3, pix3_stride,
+                             p_pix1, pix1_stride, 8 );
+    }
+    else if( i_weight < 0 || i_weight > 63 )
+    {
+        avc_biwgt_opscale_8width_nw_lasx( p_pix2, pix2_stride,
+                                          p_pix3, pix3_stride,
+                                          p_pix1, pix1_stride, 8, 5, i_weight,
+                                          ( 64 - i_weight ) );
+    }
+    else
+    {
+        avc_biwgt_opscale_8width_lasx( p_pix2, pix2_stride,
+                                       p_pix3, pix3_stride,
+                                       p_pix1, pix1_stride, 8, 5, i_weight,
+                                       ( 64 - i_weight ), 0 );
+    }
+}
+
+static void pixel_avg_8x4_lasx( uint8_t *p_pix1, intptr_t pix1_stride,
+                                uint8_t *p_pix2, intptr_t pix2_stride,
+                                uint8_t *p_pix3, intptr_t pix3_stride,
+                                int32_t i_weight )
+{
+    if( 32 == i_weight )
+    {
+        avg_src_width8_lasx( p_pix2, pix2_stride, p_pix3, pix3_stride,
+                             p_pix1, pix1_stride, 4 );
+    }
+    else if( i_weight < 0 || i_weight > 63 )
+    {
+        avc_biwgt_opscale_8width_nw_lasx( p_pix2, pix2_stride,
+                                          p_pix3, pix3_stride,
+                                          p_pix1, pix1_stride, 4, 5, i_weight,
+                                          ( 64 - i_weight ) );
+    }
+    else
+    {
+        avc_biwgt_opscale_8width_lasx( p_pix2, pix2_stride,
+                                       p_pix3, pix3_stride,
+                                       p_pix1, pix1_stride, 4, 5, i_weight,
+                                       ( 64 - i_weight ), 0 );
+    }
+}
+
+static void pixel_avg_4x16_lasx( uint8_t *p_pix1, intptr_t pix1_stride,
+                                 uint8_t *p_pix2, intptr_t pix2_stride,
+                                 uint8_t *p_pix3, intptr_t pix3_stride,
+                                 int32_t i_weight )
+{
+    if( 32 == i_weight )
+    {
+        avg_src_width4_lasx( p_pix2, pix2_stride, p_pix3, pix3_stride,
+                             p_pix1, pix1_stride, 16 );
+    }
+    else if( i_weight < 0 || i_weight > 63 )
+    {
+        avc_biwgt_opscale_4width_nw_lasx( p_pix2, pix2_stride,
+                                          p_pix3, pix3_stride,
+                                          p_pix1, pix1_stride, 16, 5, i_weight,
+                                          ( 64 - i_weight ) );
+    }
+    else
+    {
+        avc_biwgt_opscale_4width_lasx( p_pix2, pix2_stride,
+                                       p_pix3, pix3_stride,
+                                       p_pix1, pix1_stride, 16, 5, i_weight,
+                                       ( 64 - i_weight ), 0 );
+    }
+}
+
+static void pixel_avg_4x8_lasx( uint8_t *p_pix1, intptr_t pix1_stride,
+                                uint8_t *p_pix2, intptr_t pix2_stride,
+                                uint8_t *p_pix3, intptr_t pix3_stride,
+                                int32_t i_weight )
+{
+    if( 32 == i_weight )
+    {
+        avg_src_width4_lasx( p_pix2, pix2_stride, p_pix3, pix3_stride,
+                             p_pix1, pix1_stride, 8 );
+    }
+    else if( i_weight < 0 || i_weight > 63 )
+    {
+        avc_biwgt_opscale_4width_nw_lasx( p_pix2, pix2_stride,
+                                          p_pix3, pix3_stride,
+                                          p_pix1, pix1_stride, 8, 5, i_weight,
+                                          ( 64 - i_weight ) );
+    }
+    else
+    {
+        avc_biwgt_opscale_4width_lasx( p_pix2, pix2_stride,
+                                       p_pix3, pix3_stride,
+                                       p_pix1, pix1_stride, 8, 5, i_weight,
+                                       ( 64 - i_weight ), 0 );
+    }
+}
+
+static void pixel_avg_4x4_lasx( uint8_t *p_pix1, intptr_t pix1_stride,
+                                uint8_t *p_pix2, intptr_t pix2_stride,
+                                uint8_t *p_pix3, intptr_t pix3_stride,
+                                int32_t i_weight )
+{
+    if( 32 == i_weight )
+    {
+        avg_src_width4_lasx( p_pix2, pix2_stride, p_pix3, pix3_stride,
+                             p_pix1, pix1_stride, 4 );
+    }
+    else if( i_weight < 0 || i_weight > 63 )
+    {
+        avc_biwgt_opscale_4width_nw_lasx( p_pix2, pix2_stride,
+                                          p_pix3, pix3_stride,
+                                          p_pix1, pix1_stride, 4, 5, i_weight,
+                                          ( 64 - i_weight ) );
+    }
+    else
+    {
+        avc_biwgt_opscale_4width_lasx( p_pix2, pix2_stride,
+                                       p_pix3, pix3_stride,
+                                       p_pix1, pix1_stride, 4, 5, i_weight,
+                                       ( 64 - i_weight ), 0 );
+    }
+}
+
+static void pixel_avg_4x2_lasx( uint8_t *p_pix1, intptr_t pix1_stride,
+                                uint8_t *p_pix2, intptr_t pix2_stride,
+                                uint8_t *p_pix3, intptr_t pix3_stride,
+                                int32_t i_weight )
+{
+    if( 32 == i_weight )
+    {
+        avg_src_width4_lasx( p_pix2, pix2_stride, p_pix3, pix3_stride,
+                             p_pix1, pix1_stride, 2 );
+    }
+    else if( i_weight < 0 || i_weight > 63 )
+    {
+        avc_biwgt_opscale_4x2_nw_lasx( p_pix2, pix2_stride,
+                                       p_pix3, pix3_stride,
+                                       p_pix1, pix1_stride, 5, i_weight,
+                                       ( 64 - i_weight ) );
+    }
+    else
+    {
+        avc_biwgt_opscale_4x2_lasx( p_pix2, pix2_stride,
+                                    p_pix3, pix3_stride,
+                                    p_pix1, pix1_stride, 5, i_weight,
+                                    ( 64 - i_weight ), 0 );
+    }
+}
+
+static inline void avg_src_width16_no_align_lasx( uint8_t *p_src1,
+                                                  int32_t i_src1_stride,
+                                                  uint8_t *p_src2,
+                                                  int32_t i_src2_stride,
+                                                  uint8_t *p_dst,
+                                                  int32_t i_dst_stride,
+                                                  int32_t i_height )
+{
+    int32_t i_cnt;
+    __m256i src0, src1;
+
+    for( i_cnt = i_height; i_cnt--; )
+    {
+        src0 = __lasx_xvld( p_src1, 0 );
+        p_src1 += i_src1_stride;
+        src1 = __lasx_xvld( p_src2, 0 );
+        p_src2 += i_src2_stride;
+
+        src0 = __lasx_xvavgr_bu( src0, src1 );
+        __lasx_xvstelm_d( src0, p_dst, 0, 0 );
+        __lasx_xvstelm_d( src0, p_dst, 0, 1 );
+        p_dst += i_dst_stride;
+    }
+}
+
+static inline void avg_src_width20_no_align_lasx( uint8_t *p_src1,
+                                                  int32_t i_src1_stride,
+                                                  uint8_t *p_src2,
+                                                  int32_t i_src2_stride,
+                                                  uint8_t *p_dst,
+                                                  int32_t i_dst_stride,
+                                                  int32_t i_height )
+{
+    int32_t i_cnt;
+    __m256i src0, src1;
+
+    for( i_cnt = i_height; i_cnt--; )
+    {
+        src0 = __lasx_xvld( p_src1, 0 );
+        p_src1 += i_src1_stride;
+        src1 = __lasx_xvld( p_src2, 0 );
+        p_src2 += i_src2_stride;
+
+        src0 = __lasx_xvavgr_bu( src0, src1 );
+        __lasx_xvstelm_d( src0, p_dst, 0, 0 );
+        __lasx_xvstelm_d( src0, p_dst, 8, 1 );
+        __lasx_xvstelm_w( src0, p_dst, 16, 4 );
+        p_dst += i_dst_stride;
+    }
+}
+
+static inline void avg_src_width12_no_align_lasx( uint8_t *p_src1,
+                                                  int32_t i_src1_stride,
+                                                  uint8_t *p_src2,
+                                                  int32_t i_src2_stride,
+                                                  uint8_t *p_dst,
+                                                  int32_t i_dst_stride,
+                                                  int32_t i_height )
+{
+    int32_t i_cnt;
+    __m256i src0, src1;
+
+    for( i_cnt = i_height; i_cnt--; )
+    {
+        src0 = __lasx_xvld( p_src1, 0 );
+        p_src1 += i_src1_stride;
+        src1 = __lasx_xvld( p_src2, 0 );
+        p_src2 += i_src2_stride;
+
+        src0 = __lasx_xvavgr_bu( src0, src1 );
+        __lasx_xvstelm_d( src0, p_dst, 0, 0 );
+        __lasx_xvstelm_w( src0, p_dst, 8, 2 );
+        p_dst += i_dst_stride;
+    }
+}
+
+static inline void avg_src_width8_no_align_lasx( uint8_t *p_src1,
+                                                 int32_t i_src1_stride,
+                                                 uint8_t *p_src2,
+                                                 int32_t i_src2_stride,
+                                                 uint8_t *p_dst,
+                                                 int32_t i_dst_stride,
+                                                 int32_t i_height )
+{
+    int32_t i_cnt;
+    __m256i src0, src1;
+
+    for( i_cnt = i_height; i_cnt--; )
+    {
+        src0 = __lasx_xvld( p_src1, 0 );
+        p_src1 += i_src1_stride;
+        src1 = __lasx_xvld( p_src2, 0 );
+        p_src2 += i_src2_stride;
+
+        src0 = __lasx_xvavgr_bu( src0, src1 );
+        __lasx_xvstelm_d( src0, p_dst, 0, 0 );
+        p_dst += i_dst_stride;
+    }
+}
+
+static inline void avg_src_width4_no_align_lasx( uint8_t *p_src1,
+                                                 int32_t i_src1_stride,
+                                                 uint8_t *p_src2,
+                                                 int32_t i_src2_stride,
+                                                 uint8_t *p_dst,
+                                                 int32_t i_dst_stride,
+                                                 int32_t i_height )
+{
+    int32_t i_cnt;
+    __m256i src0, src1;
+
+    for( i_cnt = i_height; i_cnt--; )
+    {
+        src0 = __lasx_xvld( p_src1, 0 );
+        p_src1 += i_src1_stride;
+        src1 = __lasx_xvld( p_src2, 0 );
+        p_src2 += i_src2_stride;
+
+        src0 = __lasx_xvavgr_bu( src0, src1 );
+        __lasx_xvstelm_w( src0, p_dst, 0, 0 );
+        p_dst += i_dst_stride;
+    }
+}
+
+static inline void mc_weight_w16_no_align_lasx( uint8_t *p_dst,
+                                                intptr_t i_dst_stride,
+                                                uint8_t *p_src,
+                                                intptr_t i_src_stride,
+                                                const x264_weight_t *pWeight,
+                                                int32_t i_height )
+{
+    int32_t i_log2_denom = pWeight->i_denom;
+    int32_t i_offset = pWeight->i_offset;
+    int32_t i_weight = pWeight->i_scale;
+    uint8_t u_cnt;
+    __m256i zero = __lasx_xvldi( 0 );
+    __m256i src;
+    __m256i wgt, denom, offset;
+
+    i_offset <<= ( i_log2_denom );
+
+    if( i_log2_denom )
+    {
+        i_offset += ( 1 << ( i_log2_denom - 1 ) );
+    }
+
+    wgt =  __lasx_xvreplgr2vr_h( i_weight );
+    offset = __lasx_xvreplgr2vr_h( i_offset );
+    denom = __lasx_xvreplgr2vr_h( i_log2_denom );
+
+    for( u_cnt = i_height; u_cnt--; )
+    {
+        src = __lasx_xvld( p_src, 0 );
+        p_src += i_src_stride;
+
+        src = __lasx_xvpermi_d( src, 0x50 );
+        src = __lasx_xvilvl_b( zero, src );
+
+        src = __lasx_xvmul_h( src, wgt );
+        src = __lasx_xvsadd_h( src, offset );
+        src = __lasx_xvmaxi_h( src, 0 );
+        src = __lasx_xvssrln_bu_h(src, denom);
+
+        __lasx_xvstelm_d( src, p_dst, 0, 0 );
+        __lasx_xvstelm_d( src, p_dst, 8, 2 );
+        p_dst += i_dst_stride;
+    }
+}
+
+static inline void mc_weight_w8_no_align_lasx( uint8_t *p_dst,
+                                               intptr_t i_dst_stride,
+                                               uint8_t *p_src,
+                                               intptr_t i_src_stride,
+                                               const x264_weight_t *pWeight,
+                                               int32_t i_height )
+{
+    int32_t i_log2_denom = pWeight->i_denom;
+    int32_t i_offset = pWeight->i_offset;
+    int32_t i_weight = pWeight->i_scale;
+    uint8_t u_cnt;
+    __m256i zero = __lasx_xvldi( 0 );
+    __m256i src;
+    __m256i wgt, denom, offset;
+
+    i_offset <<= ( i_log2_denom );
+
+    if( i_log2_denom )
+    {
+        i_offset += ( 1 << ( i_log2_denom - 1 ) );
+    }
+
+    wgt =  __lasx_xvreplgr2vr_h( i_weight );
+    offset = __lasx_xvreplgr2vr_h( i_offset );
+    denom = __lasx_xvreplgr2vr_h( i_log2_denom );
+
+    for( u_cnt = i_height; u_cnt--; )
+    {
+        src = __lasx_xvldrepl_d( p_src, 0 );
+        p_src += i_src_stride;
+
+        src = __lasx_xvilvl_b( zero, src );
+
+        src = __lasx_xvmul_h( src, wgt );
+        src = __lasx_xvsadd_h( src, offset );
+        src = __lasx_xvmaxi_h( src, 0 );
+        src = __lasx_xvssrln_bu_h(src, denom);
+
+        __lasx_xvstelm_d( src, p_dst, 0, 0 );
+        p_dst += i_dst_stride;
+    }
+}
+
+static inline void mc_weight_w4_no_align_lasx( uint8_t *p_dst,
+                                               intptr_t i_dst_stride,
+                                               uint8_t *p_src,
+                                               intptr_t i_src_stride,
+                                               const x264_weight_t *pWeight,
+                                               int32_t i_height )
+{
+    int32_t i_log2_denom = pWeight->i_denom;
+    int32_t i_offset = pWeight->i_offset;
+    int32_t i_weight = pWeight->i_scale;
+    uint8_t u_cnt;
+    __m256i zero = __lasx_xvldi( 0 );
+    __m256i src;
+    __m256i wgt, denom, offset;
+
+    i_offset <<= ( i_log2_denom );
+
+    if( i_log2_denom )
+    {
+        i_offset += ( 1 << ( i_log2_denom - 1 ) );
+    }
+
+    wgt =  __lasx_xvreplgr2vr_h( i_weight );
+    offset = __lasx_xvreplgr2vr_h( i_offset );
+    denom = __lasx_xvreplgr2vr_h( i_log2_denom );
+
+    for( u_cnt = i_height; u_cnt--; )
+    {
+        src = __lasx_xvldrepl_w( p_src, 0 );
+        p_src += i_src_stride;
+
+        src = __lasx_xvilvl_b( zero, src );
+
+        src = __lasx_xvmul_h( src, wgt );
+        src = __lasx_xvsadd_h( src, offset );
+        src = __lasx_xvmaxi_h( src, 0 );
+        src = __lasx_xvssrln_bu_h(src, denom);
+
+        __lasx_xvstelm_w( src, p_dst, 0, 0 );
+        p_dst += i_dst_stride;
+    }
+}
+
+static inline void mc_weight_w20_no_align_lasx( uint8_t *p_dst,
+                                                intptr_t i_dst_stride,
+                                                uint8_t *p_src,
+                                                intptr_t i_src_stride,
+                                                const x264_weight_t *pWeight,
+                                                int32_t i_height )
+{
+    mc_weight_w16_no_align_lasx( p_dst, i_dst_stride,
+                                 p_src, i_src_stride,
+                                 pWeight, i_height );
+    mc_weight_w4_no_align_lasx( p_dst + 16, i_dst_stride,
+                                p_src + 16, i_src_stride,
+                                pWeight, i_height );
+}
+
+void x264_pixel_avg2_w4_lasx (uint8_t *dst, intptr_t i_dst_stride, uint8_t *src1,
+                              intptr_t i_src_stride, uint8_t *src2, int i_height)
+{
+    int64_t zero = 2, i_4 = 4;
+
+    __asm__ volatile(
+    "1:                                                                            \n\t"
+    "addi.d         %[i_height],      %[i_height],          -4                     \n\t"
+    "vldrepl.w      $vr0,             %[src1],              0                      \n\t"
+    "vldrepl.w      $vr1,             %[src2],              0                      \n\t"
+    "add.d          %[src1],          %[src1],              %[i_src_stride]        \n\t"
+    "add.d          %[src2],          %[src2],              %[i_src_stride]        \n\t"
+    "vldrepl.w      $vr2,             %[src1],              0                      \n\t"
+    "vldrepl.w      $vr3,             %[src2],              0                      \n\t"
+    "add.d          %[src1],          %[src1],              %[i_src_stride]        \n\t"
+    "add.d          %[src2],          %[src2],              %[i_src_stride]        \n\t"
+    "vldrepl.w      $vr4,             %[src1],              0                      \n\t"
+    "vldrepl.w      $vr5,             %[src2],              0                      \n\t"
+    "add.d          %[src1],          %[src1],              %[i_src_stride]        \n\t"
+    "add.d          %[src2],          %[src2],              %[i_src_stride]        \n\t"
+    "vldrepl.w      $vr6,             %[src1],              0                      \n\t"
+    "vldrepl.w      $vr7,             %[src2],              0                      \n\t"
+    "add.d          %[src1],          %[src1],              %[i_src_stride]        \n\t"
+    "add.d          %[src2],          %[src2],              %[i_src_stride]        \n\t"
+    "vavgr.bu       $vr0,             $vr0,                 $vr1                   \n\t"
+    "vavgr.bu       $vr1,             $vr2,                 $vr3                   \n\t"
+    "vavgr.bu       $vr2,             $vr4,                 $vr5                   \n\t"
+    "vavgr.bu       $vr3,             $vr6,                 $vr7                   \n\t"
+    "vstelm.w       $vr0,             %[dst],               0,           0         \n\t"
+    "add.d          %[dst],           %[dst],               %[i_dst_stride]        \n\t"
+    "vstelm.w       $vr1,             %[dst],               0,           0         \n\t"
+    "add.d          %[dst],           %[dst],               %[i_dst_stride]        \n\t"
+    "vstelm.w       $vr2,             %[dst],               0,           0         \n\t"
+    "add.d          %[dst],           %[dst],               %[i_dst_stride]        \n\t"
+    "vstelm.w       $vr3,             %[dst],               0,           0         \n\t"
+    "add.d          %[dst],           %[dst],               %[i_dst_stride]        \n\t"
+    "bge            %[i_height],      %[i_4],               1b                     \n\t"
+    "beqz           %[i_height],      3f                                           \n\t"
+    "2:                                                                            \n\t"
+    "addi.d         %[i_height],      %[i_height],          -2                     \n\t"
+    "vldrepl.w      $vr0,             %[src1],              0                      \n\t"
+    "vldrepl.w      $vr1,             %[src2],              0                      \n\t"
+    "add.d          %[src1],          %[src1],              %[i_src_stride]        \n\t"
+    "add.d          %[src2],          %[src2],              %[i_src_stride]        \n\t"
+    "vldrepl.w      $vr2,             %[src1],              0                      \n\t"
+    "vldrepl.w      $vr3,             %[src2],              0                      \n\t"
+    "add.d          %[src1],          %[src1],              %[i_src_stride]        \n\t"
+    "add.d          %[src2],          %[src2],              %[i_src_stride]        \n\t"
+    "vavgr.bu       $vr0,             $vr0,                 $vr1                   \n\t"
+    "vavgr.bu       $vr1,             $vr2,                 $vr3                   \n\t"
+    "vstelm.w       $vr0,             %[dst],               0,           0         \n\t"
+    "add.d          %[dst],           %[dst],               %[i_dst_stride]        \n\t"
+    "vstelm.w       $vr1,             %[dst],               0,           0         \n\t"
+    "add.d          %[dst],           %[dst],               %[i_dst_stride]        \n\t"
+    "blt            %[zero],          %[i_height],          2b                     \n\t"
+    "3:                                                                            \n\t"
+    : [i_height]"+&r"(i_height), [src1]"+&r"(src1), [src2]"+&r"(src2),
+      [dst]"+&r"(dst)
+    : [i_dst_stride]"r"((int64_t) i_dst_stride), [i_src_stride]"r"((int64_t) i_src_stride),
+      [zero]"r"(zero), [i_4]"r"(i_4)
+    : "memory"
+    );
+}
+
+void x264_pixel_avg2_w8_lasx (uint8_t *dst, intptr_t i_dst_stride, uint8_t *src1,
+                              intptr_t i_src_stride, uint8_t *src2, int i_height)
+{
+    int64_t zero = 0, i_4 = 4, src_stride2, src_stride3, src_stride4;
+
+    __asm__ volatile(
+    "slli.d         %[src_stride2],   %[i_src_stride],      1                      \n\t"
+    "add.d          %[src_stride3],   %[src_stride2],       %[i_src_stride]        \n\t"
+    "slli.d         %[src_stride4],   %[src_stride2],       1                      \n\t"
+    "1:                                                                            \n\t"
+    "addi.d         %[i_height],      %[i_height],          -4                     \n\t"
+    "vld            $vr0,             %[src1],              0                      \n\t"
+    "vld            $vr1,             %[src2],              0                      \n\t"
+    "vldx           $vr2,             %[src1],              %[i_src_stride]        \n\t"
+    "vldx           $vr3,             %[src2],              %[i_src_stride]        \n\t"
+    "vldx           $vr4,             %[src1],              %[src_stride2]         \n\t"
+    "vldx           $vr5,             %[src2],              %[src_stride2]         \n\t"
+    "vldx           $vr6,             %[src1],              %[src_stride3]         \n\t"
+    "vldx           $vr7,             %[src2],              %[src_stride3]         \n\t"
+    "add.d          %[src1],          %[src1],              %[src_stride4]         \n\t"
+    "add.d          %[src2],          %[src2],              %[src_stride4]         \n\t"
+    "vavgr.bu       $vr0,             $vr0,                 $vr1                   \n\t"
+    "vavgr.bu       $vr1,             $vr2,                 $vr3                   \n\t"
+    "vavgr.bu       $vr2,             $vr4,                 $vr5                   \n\t"
+    "vavgr.bu       $vr3,             $vr6,                 $vr7                   \n\t"
+    "vstelm.d       $vr0,             %[dst],               0,           0         \n\t"
+    "add.d          %[dst],           %[dst],               %[i_dst_stride]        \n\t"
+    "vstelm.d       $vr1,             %[dst],               0,           0         \n\t"
+    "add.d          %[dst],           %[dst],               %[i_dst_stride]        \n\t"
+    "vstelm.d       $vr2,             %[dst],               0,           0         \n\t"
+    "add.d          %[dst],           %[dst],               %[i_dst_stride]        \n\t"
+    "vstelm.d       $vr3,             %[dst],               0,           0         \n\t"
+    "add.d          %[dst],           %[dst],               %[i_dst_stride]        \n\t"
+    "bge            %[i_height],      %[i_4],               1b                     \n\t"
+    "beqz           %[i_height],      3f                                           \n\t"
+    "2:                                                                            \n\t"
+    "addi.d         %[i_height],      %[i_height],          -2                     \n\t"
+    "vld            $vr0,             %[src1],              0                      \n\t"
+    "vld            $vr1,             %[src2],              0                      \n\t"
+    "vldx           $vr2,             %[src1],              %[i_src_stride]        \n\t"
+    "vldx           $vr3,             %[src2],              %[i_src_stride]        \n\t"
+    "add.d          %[src1],          %[src1],              %[src_stride2]         \n\t"
+    "add.d          %[src2],          %[src2],              %[src_stride2]         \n\t"
+    "vavgr.bu       $vr0,             $vr0,                 $vr1                   \n\t"
+    "vavgr.bu       $vr1,             $vr2,                 $vr3                   \n\t"
+    "vstelm.d       $vr0,             %[dst],               0,           0         \n\t"
+    "add.d          %[dst],           %[dst],               %[i_dst_stride]        \n\t"
+    "vstelm.d       $vr1,             %[dst],               0,           0         \n\t"
+    "add.d          %[dst],           %[dst],               %[i_dst_stride]        \n\t"
+    "blt            %[zero],          %[i_height],          2b                     \n\t"
+    "3:                                                                            \n\t"
+    : [i_height]"+&r"(i_height), [src1]"+&r"(src1), [src2]"+&r"(src2),
+      [dst]"+&r"(dst), [src_stride2]"=&r"(src_stride2), [src_stride3]"=&r"(src_stride3),
+      [src_stride4]"=&r"(src_stride4)
+    : [i_dst_stride]"r"((int64_t) i_dst_stride), [i_src_stride]"r"((int64_t) i_src_stride),
+      [zero]"r"(zero), [i_4]"r"(i_4)
+    : "memory"
+    );
+}
+
+void x264_pixel_avg2_w16_lasx (uint8_t *dst, intptr_t i_dst_stride, uint8_t *src1,
+                               intptr_t i_src_stride, uint8_t *src2, int i_height)
+{
+    int64_t src_stride2, dst_stride2, dst_stride3, src_stride3, src_stride4, dst_stride4;
+    int64_t zero = 0, i_4 = 4;
+
+    __asm__ volatile(
+    "slli.d         %[src_stride2],   %[i_src_stride],      1                      \n\t"
+    "slli.d         %[dst_stride2],   %[i_dst_stride],      1                      \n\t"
+    "add.d          %[src_stride3],   %[src_stride2],       %[i_src_stride]        \n\t"
+    "add.d          %[dst_stride3],   %[dst_stride2],       %[i_dst_stride]        \n\t"
+    "slli.d         %[src_stride4],   %[src_stride2],       1                      \n\t"
+    "slli.d         %[dst_stride4],   %[dst_stride2],       1                      \n\t"
+    "1:                                                                            \n\t"
+    "addi.d         %[i_height],      %[i_height],          -4                     \n\t"
+    "vld            $vr0,             %[src1],              0                      \n\t"
+    "vldx           $vr1,             %[src1],              %[i_src_stride]        \n\t"
+    "vldx           $vr2,             %[src1],              %[src_stride2]         \n\t"
+    "vldx           $vr3,             %[src1],              %[src_stride3]         \n\t"
+    "vld            $vr4,             %[src2],              0                      \n\t"
+    "vldx           $vr5,             %[src2],              %[i_src_stride]        \n\t"
+    "vldx           $vr6,             %[src2],              %[src_stride2]         \n\t"
+    "vldx           $vr7,             %[src2],              %[src_stride3]         \n\t"
+    "vavgr.bu       $vr0,             $vr0,                 $vr4                   \n\t"
+    "vavgr.bu       $vr1,             $vr1,                 $vr5                   \n\t"
+    "vavgr.bu       $vr2,             $vr2,                 $vr6                   \n\t"
+    "vavgr.bu       $vr3,             $vr3,                 $vr7                   \n\t"
+    "add.d          %[src1],          %[src1],              %[src_stride4]         \n\t"
+    "add.d          %[src2],          %[src2],              %[src_stride4]         \n\t"
+    "vst            $vr0,             %[dst],               0                      \n\t"
+    "vstx           $vr1,             %[dst],               %[i_dst_stride]        \n\t"
+    "vstx           $vr2,             %[dst],               %[dst_stride2]         \n\t"
+    "vstx           $vr3,             %[dst],               %[dst_stride3]         \n\t"
+    "add.d          %[dst],           %[dst],               %[dst_stride4]         \n\t"
+    "bge            %[i_height],      %[i_4],               1b                     \n\t"
+    "beqz           %[i_height],      3f                                           \n\t"
+    "2:                                                                            \n\t"
+    "addi.d         %[i_height],      %[i_height],          -2                     \n\t"
+    "vld            $vr0,             %[src1],              0                      \n\t"
+    "vldx           $vr1,             %[src1],              %[i_src_stride]        \n\t"
+    "vld            $vr2,             %[src2],              0                      \n\t"
+    "vldx           $vr3,             %[src2],              %[i_src_stride]        \n\t"
+    "add.d          %[src1],          %[src1],              %[src_stride2]         \n\t"
+    "add.d          %[src2],          %[src2],              %[src_stride2]         \n\t"
+    "vavgr.bu       $vr0,             $vr0,                 $vr2                   \n\t"
+    "vavgr.bu       $vr1,             $vr1,                 $vr3                   \n\t"
+    "vst            $vr0,             %[dst],               0                      \n\t"
+    "vstx           $vr1,             %[dst],               %[i_dst_stride]        \n\t"
+    "add.d          %[dst],           %[dst],               %[dst_stride2]         \n\t"
+    "blt            %[zero],          %[i_height],          2b                     \n\t"
+    "3:                                                                            \n\t"
+    : [i_height]"+&r"(i_height), [src1]"+&r"(src1), [src2]"+&r"(src2),
+      [dst]"+&r"(dst), [src_stride2]"=&r"(src_stride2), [dst_stride2]"=&r"(dst_stride2),
+      [src_stride3]"=&r"(src_stride3), [dst_stride3]"=&r"(dst_stride3),
+      [src_stride4]"=&r"(src_stride4), [dst_stride4]"=&r"(dst_stride4)
+    : [i_dst_stride]"r"((int64_t) i_dst_stride), [i_src_stride]"r"((int64_t) i_src_stride),
+      [zero]"r"(zero), [i_4]"r"(i_4)
+    : "memory"
+    );
+}
+
+void x264_pixel_avg2_w20_lasx (uint8_t *dst, intptr_t i_dst_stride, uint8_t *src1,
+                               intptr_t i_src_stride, uint8_t *src2, int i_height)
+{
+    int64_t zero = 0, i_4 = 4;
+    int64_t src_stride2, src_stride3, src_stride4;
+
+    __asm__ volatile(
+    "slli.d         %[src_stride2],   %[i_src_stride],      1                      \n\t"
+    "add.d          %[src_stride3],   %[src_stride2],       %[i_src_stride]        \n\t"
+    "slli.d         %[src_stride4],   %[src_stride2],       1                      \n\t"
+    "1:                                                                            \n\t"
+    "addi.d         %[i_height],      %[i_height],          -4                     \n\t"
+    "xvld           $xr0,             %[src1],              0                      \n\t"
+    "xvldx          $xr1,             %[src1],              %[i_src_stride]        \n\t"
+    "xvldx          $xr2,             %[src1],              %[src_stride2]         \n\t"
+    "xvldx          $xr3,             %[src1],              %[src_stride3]         \n\t"
+    "xvld           $xr4,             %[src2],              0                      \n\t"
+    "xvldx          $xr5,             %[src2],              %[i_src_stride]        \n\t"
+    "xvldx          $xr6,             %[src2],              %[src_stride2]         \n\t"
+    "xvldx          $xr7,             %[src2],              %[src_stride3]         \n\t"
+    "add.d          %[src1],          %[src1],              %[src_stride4]         \n\t"
+    "add.d          %[src2],          %[src2],              %[src_stride4]         \n\t"
+
+    "xvavgr.bu      $xr0,             $xr0,                 $xr4                   \n\t"
+    "xvavgr.bu      $xr1,             $xr1,                 $xr5                   \n\t"
+    "xvavgr.bu      $xr2,             $xr2,                 $xr6                   \n\t"
+    "xvavgr.bu      $xr3,             $xr3,                 $xr7                   \n\t"
+    "vst            $vr0,             %[dst],               0                      \n\t"
+    "xvstelm.w      $xr0,             %[dst],               16,          4         \n\t"
+    "add.d          %[dst],           %[dst],               %[i_dst_stride]        \n\t"
+    "vst            $vr1,             %[dst],               0                      \n\t"
+    "xvstelm.w      $xr1,             %[dst],               16,          4         \n\t"
+    "add.d          %[dst],           %[dst],               %[i_dst_stride]        \n\t"
+    "vst            $vr2,             %[dst],               0                      \n\t"
+    "xvstelm.w      $xr2,             %[dst],               16,          4         \n\t"
+    "add.d          %[dst],           %[dst],               %[i_dst_stride]        \n\t"
+    "vst            $vr3,             %[dst],               0                      \n\t"
+    "xvstelm.w      $xr3,             %[dst],               16,          4         \n\t"
+    "add.d          %[dst],           %[dst],               %[i_dst_stride]        \n\t"
+    "bge            %[i_height],      %[i_4],          1b                     \n\t"
+    "beqz           %[i_height],      3f                                           \n\t"
+    "2:                                                                            \n\t"
+    "addi.d         %[i_height],      %[i_height],          -2                     \n\t"
+    "xvld           $xr0,             %[src1],              0                      \n\t"
+    "xvldx          $xr1,             %[src1],              %[i_src_stride]        \n\t"
+    "xvld           $xr2,             %[src2],              0                      \n\t"
+    "xvldx          $xr3,             %[src2],              %[i_src_stride]        \n\t"
+    "add.d          %[src1],          %[src1],              %[src_stride2]         \n\t"
+    "add.d          %[src2],          %[src2],              %[src_stride2]         \n\t"
+    "xvavgr.bu      $xr0,             $xr0,                 $xr2                   \n\t"
+    "xvavgr.bu      $xr1,             $xr1,                 $xr3                   \n\t"
+    "vst            $vr0,             %[dst],               0                      \n\t"
+    "xvstelm.w      $xr0,             %[dst],               16,          4         \n\t"
+    "add.d          %[dst],           %[dst],               %[i_dst_stride]        \n\t"
+    "vst            $vr1,             %[dst],               0                      \n\t"
+    "xvstelm.w      $xr1,             %[dst],               16,          4         \n\t"
+    "add.d          %[dst],           %[dst],               %[i_dst_stride]        \n\t"
+    "blt            %[zero],          %[i_height],          2b                     \n\t"
+    "3:                                                                            \n\t"
+    : [i_height]"+&r"(i_height), [src1]"+&r"(src1), [src2]"+&r"(src2),
+      [dst]"+&r"(dst), [src_stride2]"=&r"(src_stride2),
+      [src_stride3]"=&r"(src_stride3), [src_stride4]"=&r"(src_stride4)
+    : [i_dst_stride]"r"((int64_t) i_dst_stride), [i_src_stride]"r"((int64_t) i_src_stride),
+      [zero]"r"(zero), [i_4]"r"(i_4)
+    : "memory"
+    );
+}
+
+static void (* const pixel_avg_wtab_lasx[6])(uint8_t *, intptr_t, uint8_t *, intptr_t, uint8_t *, int ) =
+{
+    NULL,
+    x264_pixel_avg2_w4_lasx,
+    x264_pixel_avg2_w8_lasx,
+    x264_pixel_avg2_w16_lasx,
+    x264_pixel_avg2_w16_lasx,
+    x264_pixel_avg2_w20_lasx,
+};
+
+static uint8_t *get_ref_lasx( uint8_t *p_dst, intptr_t *p_dst_stride,
+                              uint8_t *p_src[4], intptr_t i_src_stride,
+                              int32_t m_vx, int32_t m_vy,
+                              int32_t i_width, int32_t i_height,
+                              const x264_weight_t *pWeight )
+{
+    int32_t i_qpel_idx;
+    int32_t i_offset;
+    uint8_t *p_src1;
+    int32_t r_vy = m_vy & 3;
+    int32_t r_vx = m_vx & 3;
+    int32_t width = i_width >> 2;
+
+    i_qpel_idx = ( r_vy << 2 ) + r_vx;
+    i_offset = ( m_vy >> 2 ) * i_src_stride + ( m_vx >> 2 );
+    p_src1 = p_src[x264_hpel_ref0[i_qpel_idx]] + i_offset +
+           ( 3 == r_vy ) * i_src_stride;
+
+    if( i_qpel_idx & 5 )
+    {
+        uint8_t *p_src2 = p_src[x264_hpel_ref1[i_qpel_idx]] +
+                          i_offset + ( 3 == r_vx );
+        pixel_avg_wtab_lasx[width](
+                p_dst, *p_dst_stride, p_src1, i_src_stride,
+                p_src2, i_height );
+
+        if( pWeight->weightfn )
+        {
+            pWeight->weightfn[width](p_dst, *p_dst_stride, p_dst, *p_dst_stride, pWeight, i_height);
+        }
+        return p_dst;
+    }
+    else if ( pWeight->weightfn )
+    {
+        pWeight->weightfn[width]( p_dst, *p_dst_stride, p_src1, i_src_stride, pWeight, i_height );
+        return p_dst;
+    }
+    else
+    {
+        *p_dst_stride = i_src_stride;
+        return p_src1;
+    }
+}
+
+static void copy_width4_lasx( uint8_t *p_src, int32_t i_src_stride,
+                              uint8_t *p_dst, int32_t i_dst_stride,
+                              int32_t i_height )
+{
+    int32_t i_cnt;
+    __m256i src0, src1;
+
+    for( i_cnt = ( i_height >> 1 ); i_cnt--;  )
+    {
+        src0 = __lasx_xvldrepl_w( p_src, 0 );
+        p_src += i_src_stride;
+        src1 = __lasx_xvldrepl_w( p_src, 0 );
+        p_src += i_src_stride;
+
+        __lasx_xvstelm_w( src0, p_dst, 0, 0 );
+        p_dst += i_dst_stride;
+        __lasx_xvstelm_w( src1, p_dst, 0, 0 );
+        p_dst += i_dst_stride;
+    }
+}
+
+static void copy_width8_lasx( uint8_t *p_src, int32_t i_src_stride,
+                              uint8_t *p_dst, int32_t i_dst_stride,
+                              int32_t i_height )
+{
+    int32_t i_cnt;
+    __m256i src0, src1, src2, src3;
+
+#define COPY_W8_H4                                  \
+    src0 = __lasx_xvldrepl_d( p_src, 0 );           \
+    p_src += i_src_stride;                          \
+    src1 = __lasx_xvldrepl_d( p_src, 0 );           \
+    p_src += i_src_stride;                          \
+    src2 = __lasx_xvldrepl_d( p_src, 0 );           \
+    p_src += i_src_stride;                          \
+    src3 = __lasx_xvldrepl_d( p_src, 0 );           \
+    p_src += i_src_stride;                          \
+                                                    \
+    __lasx_xvstelm_d( src0, p_dst, 0, 0 );          \
+    p_dst += i_dst_stride;                          \
+    __lasx_xvstelm_d( src1, p_dst, 0, 0 );          \
+    p_dst += i_dst_stride;                          \
+    __lasx_xvstelm_d( src2, p_dst, 0, 0 );          \
+    p_dst += i_dst_stride;                          \
+    __lasx_xvstelm_d( src3, p_dst, 0, 0 );          \
+    p_dst += i_dst_stride;
+
+    if( 0 == i_height % 12 )
+    {
+        for( i_cnt = i_height; 0 < i_cnt; i_cnt -= 12 )
+        {
+            COPY_W8_H4;
+            COPY_W8_H4;
+            COPY_W8_H4;
+        }
+    }
+    else if( 0 == ( i_height & 7 ) )
+    {
+        for( i_cnt = ( i_height >> 3 ); i_cnt--; )
+        {
+            COPY_W8_H4;
+            COPY_W8_H4;
+        }
+    }
+    else if( 0 == ( i_height & 3 ) )
+    {
+        for( i_cnt = ( i_height >> 2 ); i_cnt--; )
+        {
+            COPY_W8_H4;
+        }
+    }
+
+#undef COPY_W8_H4
+
+}
+
+static void copy_width16_lasx( uint8_t *p_src, int32_t i_src_stride,
+                               uint8_t *p_dst, int32_t i_dst_stride,
+                               int32_t i_height )
+{
+    int32_t i_cnt;
+    __m256i src0, src1, src2, src3;
+
+#define COPY_W16_H4                                 \
+    src0 = __lasx_xvld(p_src, 0);                   \
+    p_src += i_src_stride;                          \
+    src1 = __lasx_xvld(p_src, 0);                   \
+    p_src += i_src_stride;                          \
+    src2 = __lasx_xvld(p_src, 0);                   \
+    p_src += i_src_stride;                          \
+    src3 = __lasx_xvld(p_src, 0);                   \
+    p_src += i_src_stride;                          \
+                                                    \
+    __lasx_xvstelm_d( src0, p_dst, 0, 0 );          \
+    __lasx_xvstelm_d( src0, p_dst, 8, 1 );          \
+    p_dst += i_dst_stride;                          \
+    __lasx_xvstelm_d( src1, p_dst, 0, 0 );          \
+    __lasx_xvstelm_d( src1, p_dst, 8, 1 );          \
+    p_dst += i_dst_stride;                          \
+    __lasx_xvstelm_d( src2, p_dst, 0, 0 );          \
+    __lasx_xvstelm_d( src2, p_dst, 8, 1 );          \
+    p_dst += i_dst_stride;                          \
+    __lasx_xvstelm_d( src3, p_dst, 0, 0 );          \
+    __lasx_xvstelm_d( src3, p_dst, 8, 1 );          \
+    p_dst += i_dst_stride;
+
+    if( 0 == i_height % 12 )
+    {
+        for( i_cnt = i_height; 0 < i_cnt; i_cnt -= 12 )
+        {
+            COPY_W16_H4;
+            COPY_W16_H4;
+            COPY_W16_H4;
+        }
+    }
+    else if( 0 == ( i_height & 7 ) )
+    {
+        for( i_cnt = ( i_height >> 3 ); i_cnt--; )
+        {
+            COPY_W16_H4;
+            COPY_W16_H4;
+        }
+    }
+    else if( 0 == ( i_height & 3 ) )
+    {
+        for( i_cnt = ( i_height >> 2 ); i_cnt--; )
+        {
+            COPY_W16_H4;
+        }
+    }
+
+#undef COPY_W16_H4
+
+}
+
+static void mc_copy_w16_lasx( uint8_t *p_dst, intptr_t i_dst_stride,
+                              uint8_t *p_src, intptr_t i_src_stride,
+                              int32_t i_height )
+{
+    copy_width16_lasx( p_src, i_src_stride, p_dst, i_dst_stride, i_height );
+}
+
+static void mc_copy_w8_lasx( uint8_t *p_dst, intptr_t i_dst_stride,
+                             uint8_t *p_src, intptr_t i_src_stride,
+                             int32_t i_height )
+{
+    copy_width8_lasx( p_src, i_src_stride, p_dst, i_dst_stride, i_height );
+}
+
+static void mc_copy_w4_lasx( uint8_t *p_dst, intptr_t i_dst_stride,
+                             uint8_t *p_src, intptr_t i_src_stride,
+                             int32_t i_height )
+{
+    copy_width4_lasx( p_src, i_src_stride, p_dst, i_dst_stride, i_height );
+}
+
+static void mc_luma_lasx( uint8_t *p_dst, intptr_t i_dst_stride,
+                          uint8_t *p_src[4], intptr_t i_src_stride,
+                          int32_t m_vx, int32_t m_vy,
+                          int32_t i_width, int32_t i_height,
+                          const x264_weight_t *pWeight )
+{
+    int32_t  i_qpel_idx;
+    int32_t  i_offset;
+    uint8_t  *p_src1;
+
+    i_qpel_idx = ( ( m_vy & 3 ) << 2 ) + ( m_vx & 3 );
+    i_offset = ( m_vy >> 2 ) * i_src_stride + ( m_vx >> 2 );
+    p_src1 = p_src[x264_hpel_ref0[i_qpel_idx]] + i_offset +
+             ( 3 == ( m_vy & 3 ) ) * i_src_stride;
+
+    if( i_qpel_idx & 5 )
+    {
+        uint8_t *p_src2 = p_src[x264_hpel_ref1[i_qpel_idx]] +
+                          i_offset + ( 3 == ( m_vx & 3 ) );
+
+        pixel_avg_wtab_lasx[i_width >> 2](
+                p_dst, i_dst_stride, p_src1, i_src_stride,
+                p_src2, i_height );
+
+        if( pWeight->weightfn )
+        {
+            pWeight->weightfn[i_width>>2]( p_dst, i_dst_stride, p_dst, i_dst_stride, pWeight, i_height );
+        }
+    }
+    else if( pWeight->weightfn )
+    {
+        pWeight->weightfn[i_width>>2]( p_dst, i_dst_stride, p_src1, i_src_stride, pWeight, i_height );
+    }
+    else
+    {
+        if( 16 == i_width )
+        {
+            copy_width16_lasx( p_src1, i_src_stride, p_dst, i_dst_stride,
+                               i_height );
+        }
+        else if( 8 == i_width )
+        {
+            copy_width8_lasx( p_src1, i_src_stride, p_dst, i_dst_stride,
+                              i_height );
+        }
+        else if( 4 == i_width )
+        {
+            copy_width4_lasx( p_src1, i_src_stride, p_dst, i_dst_stride,
+                              i_height );
+        }
+    }
+}
+
+static void avc_luma_vt_16w_lasx( uint8_t *p_src, int32_t i_src_stride,
+                                  uint8_t *p_dst, int32_t i_dst_stride,
+                                  int32_t i_height )
+{
+    uint32_t u_loop_cnt, u_h4w;
+    const int16_t i_filt_const0 = 0xfb01;
+    const int16_t i_filt_const1 = 0x1414;
+    const int16_t i_filt_const2 = 0x1fb;
+    __m256i src0, src1, src2, src3, src4, src5, src6, src7, src8;
+    __m256i src10_h, src32_h, src54_h, src76_h;
+    __m256i src21_h, src43_h, src65_h, src87_h;
+    __m256i src10_l, src32_l, src54_l, src76_l;
+    __m256i src21_l, src43_l, src65_l, src87_l;
+    __m256i out10_h, out32_h, out10_l, out32_l;
+    __m256i res10_h, res32_h, res10_l, res32_l;
+    __m256i tmp10_h, tmp32_h, tmp10_l, tmp32_l;
+    __m256i filt0, filt1, filt2;
+    int32_t i_src_stride_x2 = i_src_stride << 1;
+    int32_t i_src_stride_x4 = i_src_stride << 2;
+    int32_t i_dst_stride_x2 = i_dst_stride << 1;
+    int32_t i_src_stride_x3 = i_src_stride_x2 + i_src_stride;
+
+    u_h4w = i_height % 4;
+    filt0 = __lasx_xvreplgr2vr_h( i_filt_const0 );
+    filt1 = __lasx_xvreplgr2vr_h( i_filt_const1 );
+    filt2 = __lasx_xvreplgr2vr_h( i_filt_const2 );
+
+    src0 = __lasx_xvld( p_src, 0 );
+    p_src += i_src_stride;
+    DUP4_ARG2( __lasx_xvldx, p_src, 0, p_src, i_src_stride, p_src, i_src_stride_x2,
+               p_src, i_src_stride_x3, src1, src2, src3, src4 );
+    p_src += i_src_stride_x4;
+
+    src0 = __lasx_xvxori_b( src0, 128 );
+    DUP4_ARG2( __lasx_xvxori_b, src1, 128, src2, 128, src3, 128, src4, 128,
+               src1, src2, src3, src4 );
+
+    src10_l = __lasx_xvilvl_b( src1, src0 );
+    src10_h = __lasx_xvilvh_b( src1, src0 );
+    src21_l = __lasx_xvilvl_b( src2, src1 );
+    src21_h = __lasx_xvilvh_b( src2, src1 );
+    src32_l = __lasx_xvilvl_b( src3, src2 );
+    src32_h = __lasx_xvilvh_b( src3, src2 );
+    src43_l = __lasx_xvilvl_b( src4, src3 );
+    src43_h = __lasx_xvilvh_b( src4, src3 );
+    res10_h = __lasx_xvpermi_q( src21_h, src10_h, 0x20 );
+    res32_h = __lasx_xvpermi_q( src43_h, src32_h, 0x20 );
+    res10_l = __lasx_xvpermi_q( src21_l, src10_l, 0x20 );
+    res32_l = __lasx_xvpermi_q( src43_l, src32_l, 0x20 );
+
+    for( u_loop_cnt = ( i_height >> 2 ); u_loop_cnt--; )
+    {
+        DUP4_ARG2( __lasx_xvldx, p_src, 0, p_src, i_src_stride, p_src, i_src_stride_x2,
+                   p_src, i_src_stride_x3, src5, src6, src7, src8 );
+        p_src += i_src_stride_x4;
+
+        DUP4_ARG2( __lasx_xvxori_b, src5, 128, src6, 128, src7, 128, src8, 128, src5,
+                   src6, src7, src8 );
+        src54_l = __lasx_xvilvl_b( src5, src4 );
+        src54_h = __lasx_xvilvh_b( src5, src4 );
+        src65_l = __lasx_xvilvl_b( src6, src5 );
+        src65_h = __lasx_xvilvh_b( src6, src5 );
+        src76_l = __lasx_xvilvl_b( src7, src6 );
+        src76_h = __lasx_xvilvh_b( src7, src6 );
+        src87_l = __lasx_xvilvl_b( src8, src7 );
+        src87_h = __lasx_xvilvh_b( src8, src7 );
+        tmp10_h = __lasx_xvpermi_q( src65_h, src54_h, 0x20 );
+        tmp32_h = __lasx_xvpermi_q( src87_h, src76_h, 0x20 );
+        tmp10_l = __lasx_xvpermi_q( src65_l, src54_l, 0x20 );
+        tmp32_l = __lasx_xvpermi_q( src87_l, src76_l, 0x20 );
+
+        out10_h = __lasx_xvdp2_h_b( res10_h, filt0 );
+        out10_h = __lasx_xvdp2add_h_b( out10_h, res32_h, filt1 );
+        out10_h = __lasx_xvdp2add_h_b( out10_h, tmp10_h, filt2 );
+
+
+        out32_h = __lasx_xvdp2_h_b( res32_h, filt0 );
+        out32_h = __lasx_xvdp2add_h_b( out32_h, tmp10_h, filt1 );
+        out32_h = __lasx_xvdp2add_h_b( out32_h, tmp32_h, filt2 );
+
+        out10_l = __lasx_xvdp2_h_b( res10_l, filt0 );
+        out10_l = __lasx_xvdp2add_h_b( out10_l, res32_l, filt1 );
+        out10_l = __lasx_xvdp2add_h_b( out10_l, tmp10_l, filt2 );
+
+        out32_l = __lasx_xvdp2_h_b( res32_l, filt0 );
+        out32_l = __lasx_xvdp2add_h_b( out32_l, tmp10_l, filt1 );
+        out32_l = __lasx_xvdp2add_h_b( out32_l, tmp32_l, filt2 );
+
+        out10_l = __lasx_xvssrarni_b_h(out10_h, out10_l, 5);
+        out32_l = __lasx_xvssrarni_b_h(out32_h, out32_l, 5);
+        DUP2_ARG2( __lasx_xvxori_b, out10_l, 128, out32_l, 128, out10_l, out32_l );
+
+        __lasx_xvstelm_d( out10_l, p_dst, 0, 0 );
+        __lasx_xvstelm_d( out10_l, p_dst, 8, 1 );
+        __lasx_xvstelm_d( out10_l, p_dst + i_dst_stride, 0, 2 );
+        __lasx_xvstelm_d( out10_l, p_dst + i_dst_stride, 8, 3 );
+        p_dst += i_dst_stride_x2;
+        __lasx_xvstelm_d( out32_l, p_dst, 0, 0 );
+        __lasx_xvstelm_d( out32_l, p_dst, 8, 1 );
+        __lasx_xvstelm_d( out32_l, p_dst + i_dst_stride, 0, 2 );
+        __lasx_xvstelm_d( out32_l, p_dst + i_dst_stride, 8, 3 );
+        p_dst += i_dst_stride_x2;
+
+        res10_h = tmp10_h;
+        res32_h = tmp32_h;
+        res10_l = tmp10_l;
+        res32_l = tmp32_l;
+        src4 = src8;
+    }
+
+    if (u_h4w >= 2) {
+        src5 = __lasx_xvld( p_src, 0 );
+        src6 = __lasx_xvldx (p_src, i_src_stride);
+        p_src += i_src_stride_x2;
+        src5 = __lasx_xvxori_b( src5, 128 );
+        src6 = __lasx_xvxori_b( src6, 128 );
+        src54_l = __lasx_xvilvl_b( src5, src4 );
+        src54_h = __lasx_xvilvh_b( src5, src4 );
+        src65_l = __lasx_xvilvl_b( src6, src5 );
+        src65_h = __lasx_xvilvh_b( src6, src5 );
+        tmp10_h = __lasx_xvpermi_q( src65_h, src54_h, 0x20 );
+        tmp10_l = __lasx_xvpermi_q( src65_l, src54_l, 0x20 );
+        out10_h = __lasx_xvdp2_h_b( res10_h, filt0 );
+        out10_h = __lasx_xvdp2add_h_b( out10_h, res32_h, filt1 );
+        out10_h = __lasx_xvdp2add_h_b( out10_h, tmp10_h, filt2 );
+        out10_l = __lasx_xvdp2_h_b( res10_l, filt0 );
+        out10_l = __lasx_xvdp2add_h_b( out10_l, res32_l, filt1 );
+        out10_l = __lasx_xvdp2add_h_b( out10_l, tmp10_l, filt2 );
+        out10_l = __lasx_xvssrarni_b_h(out10_h, out10_l, 5);
+        out10_l = __lasx_xvxori_b( out10_l, 128 );
+        __lasx_xvstelm_d( out10_l, p_dst, 0, 0 );
+        __lasx_xvstelm_d( out10_l, p_dst, 8, 1 );
+        p_dst += i_dst_stride;
+        __lasx_xvstelm_d( out10_l, p_dst, 0, 2 );
+        __lasx_xvstelm_d( out10_l, p_dst, 8, 3 );
+        p_dst += i_dst_stride;
+        u_h4w -= 2;
+        res10_l = res32_l;
+        res32_l = tmp10_l;
+        res10_h = res32_h;
+        res32_h = tmp10_h;
+        src4 = src6;
+    }
+
+    if (u_h4w > 0) {
+        src5 = __lasx_xvld( p_src, 0 );
+        src54_l = __lasx_xvilvl_b( src5, src4 );
+        src54_h = __lasx_xvilvh_b( src5, src4 );
+        out10_h = __lasx_xvdp2_h_b( res10_h, filt0 );
+        out10_h = __lasx_xvdp2add_h_b( out10_h, res32_h, filt1 );
+        out10_h = __lasx_xvdp2add_h_b( out10_h, src54_h, filt2 );
+
+        out10_l = __lasx_xvdp2_h_b( res10_l, filt0 );
+        out10_l = __lasx_xvdp2add_h_b( out10_l, res32_l, filt1 );
+        out10_l = __lasx_xvdp2add_h_b( out10_l, src54_l, filt2 );
+        out10_l = __lasx_xvssrarni_b_h(out10_h, out10_l, 5);
+        out10_l = __lasx_xvxori_b( out10_l, 128 );
+        __lasx_xvstelm_d( out10_l, p_dst, 0, 0 );
+        __lasx_xvstelm_d( out10_l, p_dst, 8, 1 );
+    }
+}
+
+#define LASX_HORZ_FILTER_SH( in, mask0, mask1, mask2 )         \
+( {                                                            \
+    __m256i out0_m;                                            \
+    __m256i tmp0_m, tmp1_m;                                    \
+                                                               \
+    tmp0_m = __lasx_xvshuf_b(in, in, mask0);                   \
+    out0_m = __lasx_xvhaddw_h_b( tmp0_m, tmp0_m );             \
+                                                               \
+    tmp0_m = __lasx_xvshuf_b(in, in, mask1);                   \
+    out0_m = __lasx_xvdp2add_h_b( out0_m, minus5b, tmp0_m );   \
+                                                               \
+    tmp1_m = __lasx_xvshuf_b(in, in, mask2);                   \
+    out0_m = __lasx_xvdp2add_h_b( out0_m, plus20b, tmp1_m );   \
+                                                               \
+    out0_m;                                                    \
+} )
+
+#define LASX_CALC_DPADD_H_6PIX_2COEFF_SH( in0, in1, in2, in3, in4, in5 )   \
+( {                                                                        \
+    __m256i tmp0_m, tmp1_m;                                                \
+    __m256i out0_m, out1_m, out2_m, out3_m;                                \
+                                                                           \
+    tmp0_m = __lasx_xvilvh_h( in5, in0 );                                  \
+    tmp1_m = __lasx_xvilvl_h( in5, in0 );                                  \
+                                                                           \
+    tmp0_m = __lasx_xvhaddw_w_h( tmp0_m, tmp0_m );                         \
+    tmp1_m = __lasx_xvhaddw_w_h( tmp1_m, tmp1_m );                         \
+                                                                           \
+    out0_m = __lasx_xvilvh_h( in1, in4 );                                  \
+    out1_m = __lasx_xvilvl_h( in1, in4 );                                  \
+    DUP2_ARG3( __lasx_xvdp2add_w_h, tmp0_m, out0_m, minus5h, tmp1_m,       \
+               out1_m, minus5h, tmp0_m, tmp1_m );                          \
+    out2_m = __lasx_xvilvh_h( in2, in3 );                                  \
+    out3_m = __lasx_xvilvl_h( in2, in3 );                                  \
+    DUP2_ARG3( __lasx_xvdp2add_w_h, tmp0_m, out2_m, plus20h, tmp1_m,       \
+               out3_m, plus20h, tmp0_m, tmp1_m );                          \
+                                                                           \
+    out0_m = __lasx_xvssrarni_h_w(tmp0_m, tmp1_m, 10);                     \
+    out0_m = __lasx_xvsat_h(out0_m, 7);                                    \
+                                                                           \
+    out0_m;                                                                \
+} )
+
+static void avc_luma_mid_16w_lasx( uint8_t *p_src, int32_t i_src_stride,
+                                   uint8_t *p_dst, int32_t i_dst_stride,
+                                   int32_t i_height )
+{
+    uint32_t u_loop_cnt, u_h4w;
+    int32_t minus = -5, plus = 20;
+    __m256i src0, src1, src2, src3, src4;
+    __m256i src5, src6, src7, src8;
+    __m256i mask0, mask1, mask2;
+    __m256i dst0, dst1, dst2, dst3;
+    __m256i out0, out1;
+    __m256i minus5b, plus20b, minus5h, plus20h;
+    int32_t i_src_stride_x2 = i_src_stride << 1;
+    int32_t i_src_stride_x4 = i_src_stride << 2;
+    int32_t i_src_stride_x3 = i_src_stride_x2 + i_src_stride;
+
+    minus5b = __lasx_xvreplgr2vr_b( minus );
+    plus20b = __lasx_xvreplgr2vr_b( plus );
+    minus5h = __lasx_xvreplgr2vr_h( minus );
+    plus20h = __lasx_xvreplgr2vr_h( plus );
+
+    u_h4w = i_height & 3;
+    mask0 = __lasx_xvld( pu_luma_mask_arr, 0 );
+    mask1 = __lasx_xvld( &pu_luma_mask_arr[32], 0 );
+    mask2 = __lasx_xvld( &pu_luma_mask_arr[64], 0 );
+
+    src0 = __lasx_xvld( p_src, 0 );
+    p_src += i_src_stride;
+    DUP4_ARG2( __lasx_xvldx, p_src, 0, p_src, i_src_stride, p_src, i_src_stride_x2,
+               p_src, i_src_stride_x3, src1, src2, src3, src4 );
+    p_src += i_src_stride_x4;
+    src0 = __lasx_xvpermi_d( src0, 0x94);
+    src1 = __lasx_xvpermi_d( src1, 0x94);
+    src2 = __lasx_xvpermi_d( src2, 0x94);
+    src3 = __lasx_xvpermi_d( src3, 0x94);
+    src4 = __lasx_xvpermi_d( src4, 0x94);
+
+    src0 = __lasx_xvxori_b( src0, 128 );
+    DUP4_ARG2( __lasx_xvxori_b, src1, 128, src2, 128, src3, 128, src4, 128, src1, src2,
+               src3, src4 );
+
+    src0 = LASX_HORZ_FILTER_SH( src0, mask0, mask1, mask2 );
+    src1 = LASX_HORZ_FILTER_SH( src1, mask0, mask1, mask2 );
+    src2 = LASX_HORZ_FILTER_SH( src2, mask0, mask1, mask2 );
+    src3 = LASX_HORZ_FILTER_SH( src3, mask0, mask1, mask2 );
+    src4 = LASX_HORZ_FILTER_SH( src4, mask0, mask1, mask2 );
+
+    for( u_loop_cnt = ( i_height >> 2 ); u_loop_cnt--; )
+    {
+        DUP4_ARG2( __lasx_xvldx, p_src, 0, p_src, i_src_stride, p_src,
+                   i_src_stride_x2, p_src, i_src_stride_x3, src5, src6, src7, src8 );
+        p_src += i_src_stride_x4;
+        src5 = __lasx_xvpermi_d( src5, 0x94);
+        src6 = __lasx_xvpermi_d( src6, 0x94);
+        src7 = __lasx_xvpermi_d( src7, 0x94);
+        src8 = __lasx_xvpermi_d( src8, 0x94);
+
+        DUP4_ARG2( __lasx_xvxori_b, src5, 128, src6, 128, src7, 128, src8, 128,
+                   src5, src6, src7, src8 );
+
+        src5 = LASX_HORZ_FILTER_SH( src5, mask0, mask1, mask2 );
+        src6 = LASX_HORZ_FILTER_SH( src6, mask0, mask1, mask2 );
+        src7 = LASX_HORZ_FILTER_SH( src7, mask0, mask1, mask2 );
+        src8 = LASX_HORZ_FILTER_SH( src8, mask0, mask1, mask2 );
+        dst0 = LASX_CALC_DPADD_H_6PIX_2COEFF_SH( src0, src1, src2,
+                                                 src3, src4, src5 );
+        dst1 = LASX_CALC_DPADD_H_6PIX_2COEFF_SH( src1, src2, src3,
+                                                 src4, src5, src6 );
+        dst2 = LASX_CALC_DPADD_H_6PIX_2COEFF_SH( src2, src3, src4,
+                                                 src5, src6, src7 );
+        dst3 = LASX_CALC_DPADD_H_6PIX_2COEFF_SH( src3, src4, src5,
+                                                 src6, src7, src8 );
+        out0 = __lasx_xvpickev_b( dst1, dst0 );
+        out1 = __lasx_xvpickev_b( dst3, dst2 );
+        DUP2_ARG2( __lasx_xvxori_b, out0, 128, out1, 128, out0, out1 );
+        __lasx_xvstelm_d( out0, p_dst, 0, 0 );
+        __lasx_xvstelm_d( out0, p_dst, 8, 2 );
+        p_dst += i_dst_stride;
+        __lasx_xvstelm_d( out0, p_dst, 0, 1 );
+        __lasx_xvstelm_d( out0, p_dst, 8, 3 );
+        p_dst += i_dst_stride;
+        __lasx_xvstelm_d( out1, p_dst, 0, 0 );
+        __lasx_xvstelm_d( out1, p_dst, 8, 2 );
+        p_dst += i_dst_stride;
+        __lasx_xvstelm_d( out1, p_dst, 0, 1 );
+        __lasx_xvstelm_d( out1, p_dst, 8, 3 );
+        p_dst += i_dst_stride;
+
+        src3 = src7;
+        src1 = src5;
+        src5 = src4;
+        src4 = src8;
+        src2 = src6;
+        src0 = src5;
+    }
+
+    for( u_loop_cnt = u_h4w; u_loop_cnt--; )
+    {
+        src5 = __lasx_xvld( p_src, 0 );
+        p_src += i_src_stride;
+        src5 = __lasx_xvpermi_d( src5, 0x94);
+        src5 = __lasx_xvxori_b( src5, 128 );
+
+        src5 = LASX_HORZ_FILTER_SH( src5, mask0, mask1, mask2 );
+        dst0 = LASX_CALC_DPADD_H_6PIX_2COEFF_SH( src0, src1,
+                                                 src2, src3,
+                                                 src4, src5 );
+
+        out0 = __lasx_xvpickev_b( dst0, dst0 );
+        out0 = __lasx_xvxori_b( out0, 128 );
+        __lasx_xvstelm_d( out0, p_dst, 0, 0);
+        __lasx_xvstelm_d( out0, p_dst, 8, 2);
+        p_dst += i_dst_stride;
+
+        src0 = src1;
+        src1 = src2;
+        src2 = src3;
+        src3 = src4;
+        src4 = src5;
+    }
+}
+
+static void avc_luma_hz_16w_lasx( uint8_t *p_src, int32_t i_src_stride,
+                                  uint8_t *p_dst, int32_t i_dst_stride,
+                                  int32_t i_height )
+{
+    uint32_t u_loop_cnt, u_h4w;
+    int32_t minus = -5, plus = 20;
+    __m256i src0, src1, src2, src3;
+    __m256i mask0, mask1, mask2;
+    __m256i minus5b, plus20b;
+    int32_t i_src_stride_x2 = i_src_stride << 1;
+    int32_t i_src_stride_x4 = i_src_stride << 2;
+    int32_t i_src_stride_x3 = i_src_stride_x2 + i_src_stride;
+
+    minus5b = __lasx_xvreplgr2vr_b( minus );
+    plus20b = __lasx_xvreplgr2vr_b( plus );
+
+    u_h4w = i_height & 3;
+    mask0 = __lasx_xvld( pu_luma_mask_arr, 0 );
+    mask1 = __lasx_xvld( &pu_luma_mask_arr[32], 0 );
+    mask2 = __lasx_xvld( &pu_luma_mask_arr[64], 0 );
+
+    for( u_loop_cnt = ( i_height >> 2 ); u_loop_cnt--; )
+    {
+        DUP4_ARG2( __lasx_xvldx, p_src, 0, p_src, i_src_stride, p_src, i_src_stride_x2,
+                   p_src, i_src_stride_x3, src0, src1, src2, src3 );
+        p_src += i_src_stride_x4;
+        src0 = __lasx_xvpermi_d( src0, 0x94);
+        src1 = __lasx_xvpermi_d( src1, 0x94);
+        src2 = __lasx_xvpermi_d( src2, 0x94);
+        src3 = __lasx_xvpermi_d( src3, 0x94);
+
+        DUP4_ARG2( __lasx_xvxori_b, src0, 128, src1, 128, src2, 128, src3, 128,
+                   src0, src1, src2, src3 );
+
+        src0 = LASX_HORZ_FILTER_SH( src0, mask0, mask1, mask2 );
+        src1 = LASX_HORZ_FILTER_SH( src1, mask0, mask1, mask2 );
+        src2 = LASX_HORZ_FILTER_SH( src2, mask0, mask1, mask2 );
+        src3 = LASX_HORZ_FILTER_SH( src3, mask0, mask1, mask2 );
+
+        src0 = __lasx_xvssrarni_b_h(src1, src0, 5);
+        src1 = __lasx_xvssrarni_b_h(src3, src2, 5);
+        DUP2_ARG2( __lasx_xvxori_b, src0, 128, src1, 128, src0, src1);
+        __lasx_xvstelm_d( src0, p_dst, 0, 0 );
+        __lasx_xvstelm_d( src0, p_dst, 8, 2 );
+        p_dst += i_dst_stride;
+        __lasx_xvstelm_d( src0, p_dst, 0, 1);
+        __lasx_xvstelm_d( src0, p_dst, 8, 3);
+        p_dst += i_dst_stride;
+        __lasx_xvstelm_d( src1, p_dst, 0, 0 );
+        __lasx_xvstelm_d( src1, p_dst, 8, 2 );
+        p_dst += i_dst_stride;
+        __lasx_xvstelm_d( src1, p_dst, 0, 1 );
+        __lasx_xvstelm_d( src1, p_dst, 8, 3 );
+        p_dst += i_dst_stride;
+    }
+
+    for( u_loop_cnt = u_h4w; u_loop_cnt--; )
+    {
+        src0 = __lasx_xvld( p_src, 0 );
+        p_src += i_src_stride;
+        src0 = __lasx_xvpermi_d( src0, 0x94);
+
+        src0 = __lasx_xvxori_b( src0, 128 );
+        src0 = LASX_HORZ_FILTER_SH( src0, mask0, mask1, mask2 );
+        src0 = __lasx_xvssrarni_b_h(src0, src0, 5);
+        src0 = __lasx_xvxori_b( src0, 128 );
+        __lasx_xvstelm_d( src0, p_dst, 0, 0 );
+        __lasx_xvstelm_d( src0, p_dst, 8, 2 );
+        p_dst += i_dst_stride;
+    }
+}
+
+static void hpel_filter_lasx( uint8_t *p_dsth, uint8_t *p_dst_v,
+                              uint8_t *p_dstc, uint8_t *p_src,
+                              intptr_t i_stride, int32_t i_width,
+                              int32_t i_height, int16_t *p_buf )
+{
+    for( int32_t i = 0; i < ( i_width >> 4 ); i++ )
+    {
+        avc_luma_vt_16w_lasx( p_src - 2 - ( 2 * i_stride ), i_stride,
+                              p_dst_v - 2, i_stride, i_height );
+        avc_luma_mid_16w_lasx( p_src - 2 - ( 2 * i_stride ) , i_stride,
+                               p_dstc, i_stride, i_height );
+        avc_luma_hz_16w_lasx( p_src - 2, i_stride, p_dsth, i_stride, i_height );
+
+        p_src += 16;
+        p_dst_v += 16;
+        p_dsth += 16;
+        p_dstc += 16;
+    }
+}
+
+static inline void core_frame_init_lowres_core_lasx( uint8_t *p_src,
+                                              int32_t i_src_stride,
+                                              uint8_t *p_dst0,
+                                              uint8_t *p_dst1,
+                                              uint8_t *p_dst2,
+                                              uint8_t *p_dst3,
+                                              int32_t i_dst_stride,
+                                              int32_t i_width,
+                                              int32_t i_height )
+{
+    int32_t i_loop_width, i_loop_height, i_w16_mul;
+    int32_t i_src_stride_x2 = i_src_stride << 1;
+    __m256i src0, src1, src2, src3, src4, src5, src6, src7, src8;
+    __m256i sld1_vec0, sld1_vec1, sld1_vec2, sld1_vec3, sld1_vec4, sld1_vec5;
+    __m256i pckev_vec0, pckev_vec1, pckev_vec2;
+    __m256i pckod_vec0, pckod_vec1, pckod_vec2;
+    __m256i tmp0, tmp1, tmp2, tmp3;
+    __m256i mask;
+
+    mask = __lasx_xvld( pu_core_mask_arr, 0 );
+
+    i_w16_mul = i_width - i_width % 16;
+    for( i_loop_height = i_height; i_loop_height--; )
+    {
+        src0  = __lasx_xvld( p_src, 0 );
+        DUP2_ARG2( __lasx_xvldx, p_src, i_src_stride, p_src, i_src_stride_x2, src1, src2 );
+        p_src += 16;
+        for( i_loop_width = 0; i_loop_width < ( i_w16_mul >> 4 ); i_loop_width++ )
+        {
+            src3  = __lasx_xvld( p_src, 0 );
+            DUP2_ARG2( __lasx_xvldx, p_src, i_src_stride, p_src, i_src_stride_x2, src4, src5 );
+            src6 = __lasx_xvpermi_q( src3, src3, 0x11 );
+            src7 = __lasx_xvpermi_q( src4, src4, 0x11 );
+            src8 = __lasx_xvpermi_q( src5, src5, 0x11 );
+            p_src += 32;
+
+            pckev_vec0 = __lasx_xvpickev_b( src3, src0 );
+            pckod_vec0 = __lasx_xvpickod_b( src3, src0 );
+            pckev_vec1 = __lasx_xvpickev_b( src4, src1 );
+            pckod_vec1 = __lasx_xvpickod_b( src4, src1 );
+            pckev_vec2 = __lasx_xvpickev_b( src5, src2 );
+            pckod_vec2 = __lasx_xvpickod_b( src5, src2 );
+            DUP4_ARG2( __lasx_xvavgr_bu, pckev_vec1, pckev_vec0, pckod_vec1, pckod_vec0,
+                       pckev_vec2, pckev_vec1, pckod_vec2, pckod_vec1, tmp0, tmp1, tmp2,
+                       tmp3 );
+            DUP2_ARG2( __lasx_xvavgr_bu, tmp1, tmp0, tmp3, tmp2, tmp0, tmp1 );
+            __lasx_xvstelm_d( tmp0, p_dst0, 0, 0 );
+            __lasx_xvstelm_d( tmp0, p_dst0, 8, 1 );
+            __lasx_xvstelm_d( tmp1, p_dst2, 0, 0 );
+            __lasx_xvstelm_d( tmp1, p_dst2, 8, 1 );
+
+            DUP2_ARG3( __lasx_xvshuf_b, src3, src0, mask, src4, src1,
+                       mask, sld1_vec0, sld1_vec1 );
+            DUP2_ARG3( __lasx_xvshuf_b, src5, src2, mask, src6, src3,
+                       mask, sld1_vec2, sld1_vec3 );
+            DUP2_ARG3( __lasx_xvshuf_b, src7, src4, mask, src8, src5,
+                       mask, sld1_vec4, sld1_vec5 );
+            pckev_vec0 = __lasx_xvpickod_b( sld1_vec3, sld1_vec0 );
+            pckev_vec1 = __lasx_xvpickod_b( sld1_vec4, sld1_vec1 );
+            pckev_vec2 = __lasx_xvpickod_b( sld1_vec5, sld1_vec2 );
+            DUP4_ARG2( __lasx_xvavgr_bu, pckev_vec1, pckev_vec0, pckod_vec1, pckod_vec0,
+                       pckev_vec2, pckev_vec1, pckod_vec2, pckod_vec1, tmp0, tmp1, tmp2,
+                       tmp3 );
+            DUP2_ARG2( __lasx_xvavgr_bu, tmp1, tmp0, tmp3, tmp2, tmp0, tmp1 );
+            __lasx_xvstelm_d( tmp0, p_dst1, 0, 0 );
+            __lasx_xvstelm_d( tmp0, p_dst1, 8, 1 );
+            __lasx_xvstelm_d( tmp1, p_dst3, 0, 0 );
+            __lasx_xvstelm_d( tmp1, p_dst3, 8, 1 );
+
+            src0 = src6;
+            src1 = src7;
+            src2 = src8;
+            p_dst0 += 16;
+            p_dst1 += 16;
+            p_dst2 += 16;
+            p_dst3 += 16;
+        }
+
+        if (i_w16_mul < i_width)
+        {
+            src3  = __lasx_xvld( p_src, 0 );
+            DUP2_ARG2( __lasx_xvldx, p_src, i_src_stride, p_src, i_src_stride_x2, src4, src5 );
+            src6 = __lasx_xvpermi_q( src3, src3, 0x11 );
+            src7 = __lasx_xvpermi_q( src4, src4, 0x11 );
+            src8 = __lasx_xvpermi_q( src5, src5, 0x11 );
+            p_src += 16;
+
+            pckev_vec0 = __lasx_xvpickev_b( src3, src0 );
+            pckod_vec0 = __lasx_xvpickod_b( src3, src0 );
+            pckev_vec1 = __lasx_xvpickev_b( src4, src1 );
+            pckod_vec1 = __lasx_xvpickod_b( src4, src1 );
+            pckev_vec2 = __lasx_xvpickev_b( src5, src2 );
+            pckod_vec2 = __lasx_xvpickod_b( src5, src2 );
+            DUP4_ARG2( __lasx_xvavgr_bu, pckev_vec1, pckev_vec0, pckod_vec1,
+                       pckod_vec0, pckev_vec2, pckev_vec1, pckod_vec2, pckod_vec1,
+                       tmp0, tmp1, tmp2, tmp3 );
+            DUP2_ARG2( __lasx_xvavgr_bu, tmp1, tmp0, tmp3, tmp2, tmp0, tmp1 );
+            __lasx_xvstelm_d( tmp0, p_dst0, 0, 0 );
+            __lasx_xvstelm_d( tmp1, p_dst2, 0, 0 );
+
+            DUP2_ARG3( __lasx_xvshuf_b, src3, src0, mask, src4, src1,
+                       mask, sld1_vec0, sld1_vec1 );
+            DUP2_ARG3( __lasx_xvshuf_b, src5, src2, mask, src6, src3,
+                       mask, sld1_vec2, sld1_vec3 );
+            DUP2_ARG3( __lasx_xvshuf_b, src7, src4, mask, src8, src5,
+                       mask, sld1_vec4, sld1_vec5 );
+            pckev_vec0 = __lasx_xvpickod_b( sld1_vec3, sld1_vec0 );
+            pckev_vec1 = __lasx_xvpickod_b( sld1_vec4, sld1_vec1 );
+            pckev_vec2 = __lasx_xvpickod_b( sld1_vec5, sld1_vec2 );
+            DUP4_ARG2( __lasx_xvavgr_bu, pckev_vec1, pckev_vec0, pckod_vec1, pckod_vec0,
+                       pckev_vec2, pckev_vec1, pckod_vec2, pckod_vec1, tmp0, tmp1, tmp2,
+                       tmp3 );
+            DUP2_ARG2( __lasx_xvavgr_bu, tmp1, tmp0, tmp3, tmp2, tmp0, tmp1 );
+            __lasx_xvstelm_d( tmp0, p_dst1, 0, 0 );
+            __lasx_xvstelm_d( tmp1, p_dst3, 0, 0 );
+            p_dst0 += 8;
+            p_dst1 += 8;
+            p_dst2 += 8;
+            p_dst3 += 8;
+        }
+
+        p_src += ( ( i_src_stride << 1 ) - ( ( i_width << 1 ) + 16 ) );
+        p_dst0 += ( i_dst_stride - i_width );
+        p_dst1 += ( i_dst_stride - i_width );
+        p_dst2 += ( i_dst_stride - i_width );
+        p_dst3 += ( i_dst_stride - i_width );
+    }
+}
+
+static void frame_init_lowres_core_lasx( uint8_t *p_src, uint8_t *p_dst0,
+                                         uint8_t *p_dst1, uint8_t *p_dst2,
+                                         uint8_t *p_dst3, intptr_t i_src_stride,
+                                         intptr_t i_dst_stride, int32_t i_width,
+                                         int32_t i_height )
+{
+    core_frame_init_lowres_core_lasx( p_src, i_src_stride, p_dst0,
+                                      p_dst1, p_dst2, p_dst3,
+                                      i_dst_stride, i_width, i_height );
+}
+static void core_plane_copy_deinterleave_lasx( uint8_t *p_src,
+                                               int32_t i_src_stride,
+                                               uint8_t *p_dst0,
+                                               int32_t dst0_stride,
+                                               uint8_t *p_dst1,
+                                               int32_t dst1_stride,
+                                               int32_t i_width,
+                                               int32_t i_height )
+{
+    int32_t i_loop_width, i_loop_height;
+    int32_t i_w_mul4, i_w_mul16, i_w_mul32, i_h4w;
+    __m256i in0, in1, in2, in3, in4, in5, in6, in7;
+    __m256i vec_pckev0, vec_pckev1, vec_pckev2, vec_pckev3;
+    __m256i vec_pckev4, vec_pckev5, vec_pckev6, vec_pckev7;
+    __m256i vec_pckod0, vec_pckod1, vec_pckod2, vec_pckod3;
+    __m256i vec_pckod4, vec_pckod5, vec_pckod6, vec_pckod7;
+    uint8_t *p_dst, *p_dstA, *p_dstB, *p_srcA;
+    int32_t dst0_stride_x2 = dst0_stride << 1;
+    int32_t dst0_stride_x3 = dst0_stride_x2 + dst0_stride;
+    int32_t dst0_stride_x4 = dst0_stride << 2;
+    int32_t dst0_stride_x5 = dst0_stride_x4 + dst0_stride;
+    int32_t dst0_stride_x6 = dst0_stride_x4 + dst0_stride_x2;
+    int32_t dst0_stride_x7 = dst0_stride_x4 + dst0_stride_x3;
+    int32_t dst1_stride_x2 = dst1_stride << 1;
+    int32_t dst1_stride_x3 = dst1_stride_x2 + dst1_stride;
+    int32_t dst1_stride_x4 = dst1_stride << 2;
+    int32_t dst1_stride_x5 = dst1_stride_x4 + dst1_stride;
+    int32_t dst1_stride_x6 = dst1_stride_x4 + dst1_stride_x2;
+    int32_t dst1_stride_x7 = dst1_stride_x4 + dst1_stride_x3;
+    int32_t i_src_stride_x2 = i_src_stride << 1;
+    int32_t i_src_stride_x3 = i_src_stride_x2 + i_src_stride;
+    int32_t i_src_stride_x4 = i_src_stride << 2;
+    int32_t i_src_stride_x5 = i_src_stride_x4 + i_src_stride;
+    int32_t i_src_stride_x6 = i_src_stride_x4 + i_src_stride_x2;
+    int32_t i_src_stride_x7 = i_src_stride_x4 + i_src_stride_x3;
+
+    i_w_mul32 = i_width - ( i_width & 31 );
+    i_w_mul16 = i_width - ( i_width & 15 );
+    i_w_mul4 = i_width - ( i_width & 3 );
+    i_h4w = i_height - ( i_height & 7 );
+
+    for( i_loop_height = ( i_h4w >> 3 ); i_loop_height--; )
+    {
+        for( i_loop_width = ( i_w_mul32 >> 5 ); i_loop_width--; )
+        {
+            DUP4_ARG2(__lasx_xvldx, p_src, 0, p_src, i_src_stride, p_src,
+                      i_src_stride_x2, p_src, i_src_stride_x3, in0, in1, in2, in3);
+            DUP4_ARG2(__lasx_xvldx, p_src, i_src_stride_x4, p_src, i_src_stride_x5, p_src,
+                      i_src_stride_x6, p_src, i_src_stride_x7, in4, in5, in6, in7);
+            p_src += 32;
+            DUP4_ARG2( __lasx_xvpickev_b, in1, in0, in3, in2, in5, in4, in7, in6,
+                       vec_pckev0, vec_pckev1, vec_pckev2, vec_pckev3 );
+            DUP4_ARG2( __lasx_xvpickod_b, in1, in0, in3, in2, in5, in4, in7, in6,
+                       vec_pckod0, vec_pckod1, vec_pckod2, vec_pckod3 );
+
+            DUP4_ARG2(__lasx_xvldx, p_src, 0, p_src, i_src_stride, p_src,
+                      i_src_stride_x2, p_src, i_src_stride_x3, in0, in1, in2, in3);
+            DUP4_ARG2(__lasx_xvldx, p_src, i_src_stride_x4, p_src, i_src_stride_x5, p_src,
+                      i_src_stride_x6, p_src, i_src_stride_x7, in4, in5, in6, in7);
+            p_src += 32;
+            DUP4_ARG2( __lasx_xvpickev_b, in1, in0, in3, in2, in5, in4, in7, in6,
+                       vec_pckev4, vec_pckev5, vec_pckev6, vec_pckev7 );
+            DUP4_ARG2( __lasx_xvpickod_b, in1, in0, in3, in2, in5, in4, in7, in6,
+                       vec_pckod4, vec_pckod5, vec_pckod6, vec_pckod7 );
+
+            in0 = __lasx_xvpermi_q( vec_pckev0, vec_pckev4, 0x02 );
+            in1 = __lasx_xvpermi_q( vec_pckev0, vec_pckev4, 0x13 );
+            in2 = __lasx_xvpermi_q( vec_pckev1, vec_pckev5, 0x02 );
+            in3 = __lasx_xvpermi_q( vec_pckev1, vec_pckev5, 0x13 );
+            in4 = __lasx_xvpermi_q( vec_pckev2, vec_pckev6, 0x02 );
+            in5 = __lasx_xvpermi_q( vec_pckev2, vec_pckev6, 0x13 );
+            in6 = __lasx_xvpermi_q( vec_pckev3, vec_pckev7, 0x02 );
+            in7 = __lasx_xvpermi_q( vec_pckev3, vec_pckev7, 0x13 );
+
+            DUP4_ARG2( __lasx_xvilvl_d, in1, in0, in3, in2, in5, in4, in7, in6,
+                       vec_pckev0, vec_pckev1, vec_pckev2, vec_pckev3 );
+            DUP4_ARG2( __lasx_xvilvh_d, in1, in0, in3, in2, in5, in4, in7, in6,
+                       vec_pckev4, vec_pckev5, vec_pckev6, vec_pckev7 );
+
+            __lasx_xvst( vec_pckev0, p_dst0, 0 );
+            __lasx_xvstx( vec_pckev4, p_dst0, dst0_stride );
+            __lasx_xvstx( vec_pckev1, p_dst0, dst0_stride_x2 );
+            __lasx_xvstx( vec_pckev5, p_dst0, dst0_stride_x3 );
+            __lasx_xvstx( vec_pckev2, p_dst0, dst0_stride_x4 );
+            __lasx_xvstx( vec_pckev6, p_dst0, dst0_stride_x5 );
+            __lasx_xvstx( vec_pckev3, p_dst0, dst0_stride_x6 );
+            __lasx_xvstx( vec_pckev7, p_dst0, dst0_stride_x7 );
+
+            in0 = __lasx_xvpermi_q( vec_pckod0, vec_pckod4, 0x02 );
+            in1 = __lasx_xvpermi_q( vec_pckod0, vec_pckod4, 0x13 );
+            in2 = __lasx_xvpermi_q( vec_pckod1, vec_pckod5, 0x02 );
+            in3 = __lasx_xvpermi_q( vec_pckod1, vec_pckod5, 0x13 );
+            in4 = __lasx_xvpermi_q( vec_pckod2, vec_pckod6, 0x02 );
+            in5 = __lasx_xvpermi_q( vec_pckod2, vec_pckod6, 0x13 );
+            in6 = __lasx_xvpermi_q( vec_pckod3, vec_pckod7, 0x02 );
+            in7 = __lasx_xvpermi_q( vec_pckod3, vec_pckod7, 0x13 );
+
+            DUP4_ARG2( __lasx_xvilvl_d, in1, in0, in3, in2, in5, in4, in7, in6,
+                       vec_pckod0, vec_pckod1, vec_pckod2, vec_pckod3 );
+            DUP4_ARG2( __lasx_xvilvh_d, in1, in0, in3, in2, in5, in4, in7, in6,
+                       vec_pckod4, vec_pckod5, vec_pckod6, vec_pckod7 );
+
+            __lasx_xvst( vec_pckod0, p_dst1, 0 );
+            __lasx_xvstx( vec_pckod4, p_dst1, dst1_stride );
+            __lasx_xvstx( vec_pckod1, p_dst1, dst1_stride_x2 );
+            __lasx_xvstx( vec_pckod5, p_dst1, dst1_stride_x3 );
+            __lasx_xvstx( vec_pckod2, p_dst1, dst1_stride_x4 );
+            __lasx_xvstx( vec_pckod6, p_dst1, dst1_stride_x5 );
+            __lasx_xvstx( vec_pckod3, p_dst1, dst1_stride_x6 );
+            __lasx_xvstx( vec_pckod7, p_dst1, dst1_stride_x7 );
+
+            p_dst0 += 32;
+            p_dst1 += 32;
+        }
+
+        for( i_loop_width = ( ( i_width & 31 ) >> 4 ); i_loop_width--; )
+        {
+            DUP4_ARG2(__lasx_xvldx, p_src, 0, p_src, i_src_stride, p_src,
+                      i_src_stride_x2, p_src, i_src_stride_x3, in0, in1, in2, in3);
+            DUP4_ARG2(__lasx_xvldx, p_src, i_src_stride_x4, p_src, i_src_stride_x5, p_src,
+                      i_src_stride_x6, p_src, i_src_stride_x7, in4, in5, in6, in7);
+            p_src += 32;
+            DUP4_ARG2( __lasx_xvpickev_b, in1, in0, in3, in2, in5, in4, in7, in6,
+                       vec_pckev0, vec_pckev1, vec_pckev2, vec_pckev3 );
+            DUP4_ARG2( __lasx_xvpickod_b, in1, in0, in3, in2, in5, in4, in7, in6,
+                       vec_pckod0, vec_pckod1, vec_pckod2, vec_pckod3 );
+
+            __lasx_xvstelm_d( vec_pckev0, p_dst0, 0, 0 );
+            __lasx_xvstelm_d( vec_pckev0, p_dst0, 8, 2 );
+            p_dst = p_dst0 + dst0_stride;
+            __lasx_xvstelm_d( vec_pckev0, p_dst, 0, 1 );
+            __lasx_xvstelm_d( vec_pckev0, p_dst, 8, 3 );
+            p_dst = p_dst + dst0_stride;
+            __lasx_xvstelm_d( vec_pckev1, p_dst, 0, 0 );
+            __lasx_xvstelm_d( vec_pckev1, p_dst, 8, 2 );
+            p_dst = p_dst + dst0_stride;
+            __lasx_xvstelm_d( vec_pckev1, p_dst, 0, 1 );
+            __lasx_xvstelm_d( vec_pckev1, p_dst, 8, 3 );
+            p_dst = p_dst + dst0_stride;
+            __lasx_xvstelm_d( vec_pckev2, p_dst, 0, 0 );
+            __lasx_xvstelm_d( vec_pckev2, p_dst, 8, 2 );
+            p_dst = p_dst + dst0_stride;
+            __lasx_xvstelm_d( vec_pckev2, p_dst, 0, 1 );
+            __lasx_xvstelm_d( vec_pckev2, p_dst, 8, 3 );
+            p_dst = p_dst + dst0_stride;
+            __lasx_xvstelm_d( vec_pckev3, p_dst, 0, 0 );
+            __lasx_xvstelm_d( vec_pckev3, p_dst, 8, 2 );
+            p_dst = p_dst + dst0_stride;
+            __lasx_xvstelm_d( vec_pckev3, p_dst, 0, 1 );
+            __lasx_xvstelm_d( vec_pckev3, p_dst, 8, 3 );
+
+            __lasx_xvstelm_d( vec_pckod0, p_dst1, 0, 0 );
+            __lasx_xvstelm_d( vec_pckod0, p_dst1, 8, 2 );
+            p_dst = p_dst1 + dst0_stride;
+            __lasx_xvstelm_d( vec_pckod0, p_dst, 0, 1 );
+            __lasx_xvstelm_d( vec_pckod0, p_dst, 8, 3 );
+            p_dst = p_dst + dst0_stride;
+            __lasx_xvstelm_d( vec_pckod1, p_dst, 0, 0 );
+            __lasx_xvstelm_d( vec_pckod1, p_dst, 8, 2 );
+            p_dst = p_dst + dst0_stride;
+            __lasx_xvstelm_d( vec_pckod1, p_dst, 0, 1 );
+            __lasx_xvstelm_d( vec_pckod1, p_dst, 8, 3 );
+            p_dst = p_dst + dst0_stride;
+            __lasx_xvstelm_d( vec_pckod2, p_dst, 0, 0 );
+            __lasx_xvstelm_d( vec_pckod2, p_dst, 8, 2 );
+            p_dst = p_dst + dst0_stride;
+            __lasx_xvstelm_d( vec_pckod2, p_dst, 0, 1 );
+            __lasx_xvstelm_d( vec_pckod2, p_dst, 8, 3 );
+            p_dst = p_dst + dst0_stride;
+            __lasx_xvstelm_d( vec_pckod3, p_dst, 0, 0 );
+            __lasx_xvstelm_d( vec_pckod3, p_dst, 8, 2 );
+            p_dst = p_dst + dst0_stride;
+            __lasx_xvstelm_d( vec_pckod3, p_dst, 0, 1 );
+            __lasx_xvstelm_d( vec_pckod3, p_dst, 8, 3 );
+
+            p_dst0 += 16;
+            p_dst1 += 16;
+        }
+
+        for( i_loop_width = ( ( i_width & 15 ) >> 3 ); i_loop_width--; )
+        {
+            DUP4_ARG2(__lasx_xvldx, p_src, 0, p_src, i_src_stride, p_src,
+                      i_src_stride_x2, p_src, i_src_stride_x3, in0, in1, in2, in3);
+            DUP4_ARG2(__lasx_xvldx, p_src, i_src_stride_x4, p_src, i_src_stride_x5, p_src,
+                      i_src_stride_x6, p_src, i_src_stride_x7, in4, in5, in6, in7);
+            p_src += 16;
+            DUP4_ARG2( __lasx_xvpickev_b, in1, in0, in3, in2, in5, in4, in7, in6,
+                       vec_pckev0, vec_pckev1, vec_pckev2, vec_pckev3 );
+            DUP4_ARG2( __lasx_xvpickod_b, in1, in0, in3, in2, in5, in4, in7, in6,
+                       vec_pckod0, vec_pckod1, vec_pckod2, vec_pckod3 );
+
+            __lasx_xvstelm_d( vec_pckev0, p_dst0, 0, 0 );
+            __lasx_xvstelm_d( vec_pckev0, p_dst0 + dst0_stride, 0, 1 );
+            p_dst = p_dst0 + dst0_stride_x2;
+            __lasx_xvstelm_d( vec_pckev1, p_dst, 0, 0 );
+            __lasx_xvstelm_d( vec_pckev1, p_dst + dst0_stride, 0, 1 );
+            p_dst = p_dst + dst0_stride_x2;
+            __lasx_xvstelm_d( vec_pckev2, p_dst, 0, 0 );
+            __lasx_xvstelm_d( vec_pckev2, p_dst + dst0_stride, 0, 1 );
+            p_dst = p_dst + dst0_stride_x2;
+            __lasx_xvstelm_d( vec_pckev3, p_dst, 0, 0 );
+            __lasx_xvstelm_d( vec_pckev3, p_dst + dst0_stride, 0, 1 );
+
+            __lasx_xvstelm_d( vec_pckod0, p_dst1, 0, 0 );
+            __lasx_xvstelm_d( vec_pckod0, p_dst1 + dst0_stride, 0, 1 );
+            p_dst = p_dst1 + dst1_stride_x2;
+            __lasx_xvstelm_d( vec_pckod1, p_dst, 0, 0 );
+            __lasx_xvstelm_d( vec_pckod1, p_dst + dst0_stride, 0, 1 );
+            p_dst = p_dst + dst1_stride_x2;
+            __lasx_xvstelm_d( vec_pckod2, p_dst, 0, 0 );
+            __lasx_xvstelm_d( vec_pckod2, p_dst + dst0_stride, 0, 1 );
+            p_dst = p_dst + dst1_stride_x2;
+            __lasx_xvstelm_d( vec_pckod3, p_dst, 0, 0 );
+            __lasx_xvstelm_d( vec_pckod3, p_dst + dst0_stride, 0, 1 );
+
+            p_dst0 += 8;
+            p_dst1 += 8;
+        }
+
+
+        for( i_loop_width = ( ( i_width & 7 ) >> 2 ); i_loop_width--; )
+        {
+            DUP4_ARG2(__lasx_xvldx, p_src, 0, p_src, i_src_stride, p_src,
+                      i_src_stride_x2, p_src, i_src_stride_x3, in0, in1, in2, in3);
+            DUP4_ARG2(__lasx_xvldx, p_src, i_src_stride_x4, p_src, i_src_stride_x5, p_src,
+                      i_src_stride_x6, p_src, i_src_stride_x7, in4, in5, in6, in7);
+            p_src += 8;
+            DUP4_ARG2( __lasx_xvpickev_b, in1, in0, in3, in2, in5, in4, in7, in6,
+                       vec_pckev0, vec_pckev1, vec_pckev2, vec_pckev3 );
+            DUP4_ARG2( __lasx_xvpickod_b, in1, in0, in3, in2, in5, in4, in7, in6,
+                       vec_pckod0, vec_pckod1, vec_pckod2, vec_pckod3 );
+
+            __lasx_xvstelm_w( vec_pckev0, p_dst0, 0, 0 );
+            __lasx_xvstelm_w( vec_pckev0, p_dst0 + dst0_stride, 0, 2 );
+            p_dst = p_dst0 + dst0_stride_x2;
+            __lasx_xvstelm_w( vec_pckev1, p_dst, 0, 0 );
+            __lasx_xvstelm_w( vec_pckev1, p_dst + dst0_stride, 0, 2 );
+            p_dst = p_dst + dst0_stride_x2;
+            __lasx_xvstelm_w( vec_pckev2, p_dst, 0, 0 );
+            __lasx_xvstelm_w( vec_pckev2, p_dst + dst0_stride, 0, 2 );
+            p_dst = p_dst + dst0_stride_x2;
+            __lasx_xvstelm_w( vec_pckev3, p_dst, 0, 0 );
+            __lasx_xvstelm_w( vec_pckev3, p_dst + dst0_stride, 0, 2 );
+
+            __lasx_xvstelm_w( vec_pckod0, p_dst1, 0, 0 );
+            __lasx_xvstelm_w( vec_pckod0, p_dst1 + dst0_stride, 0, 2 );
+            p_dst = p_dst1 + dst1_stride_x2;
+            __lasx_xvstelm_w( vec_pckod1, p_dst, 0, 0 );
+            __lasx_xvstelm_w( vec_pckod1, p_dst + dst0_stride, 0, 2 );
+            p_dst = p_dst + dst1_stride_x2;
+            __lasx_xvstelm_w( vec_pckod2, p_dst, 0, 0 );
+            __lasx_xvstelm_w( vec_pckod2, p_dst + dst0_stride, 0, 2 );
+            p_dst = p_dst + dst1_stride_x2;
+            __lasx_xvstelm_w( vec_pckod3, p_dst, 0, 0 );
+            __lasx_xvstelm_w( vec_pckod3, p_dst + dst0_stride, 0, 2 );
+
+            p_dst0 += 4;
+            p_dst1 += 4;
+        }
+
+        for( i_loop_width = i_w_mul4; i_loop_width < i_width; i_loop_width++ )
+        {
+            p_dst0[0] = p_src[0];
+            p_dst1[0] = p_src[1];
+
+            p_dstA = p_dst0 + dst0_stride;
+            p_dstB = p_dst1 + dst1_stride;
+            p_srcA = p_src + i_src_stride;
+            p_dstA[0] = p_srcA[0];
+            p_dstB[0] = p_srcA[1];
+
+            p_dstA += dst0_stride;
+            p_dstB += dst1_stride;
+            p_srcA += i_src_stride;
+            p_dstA[0] = p_srcA[0];
+            p_dstB[0] = p_srcA[1];
+
+            p_dstA += dst0_stride;
+            p_dstB += dst1_stride;
+            p_srcA += i_src_stride;
+            p_dstA[0] = p_srcA[0];
+            p_dstB[0] = p_srcA[1];
+
+            p_dstA += dst0_stride;
+            p_dstB += dst1_stride;
+            p_srcA += i_src_stride;
+            p_dstA[0] = p_srcA[0];
+            p_dstB[0] = p_srcA[1];
+
+            p_dstA += dst0_stride;
+            p_dstB += dst1_stride;
+            p_srcA += i_src_stride;
+            p_dstA[0] = p_srcA[0];
+            p_dstB[0] = p_srcA[1];
+
+            p_dstA += dst0_stride;
+            p_dstB += dst1_stride;
+            p_srcA += i_src_stride;
+            p_dstA[0] = p_srcA[0];
+            p_dstB[0] = p_srcA[1];
+
+            p_dstA += dst0_stride;
+            p_dstB += dst1_stride;
+            p_srcA += i_src_stride;
+            p_dstA[0] = p_srcA[0];
+            p_dstB[0] = p_srcA[1];
+
+            p_dst0 += 1;
+            p_dst1 += 1;
+            p_src += 2;
+        }
+
+        p_src += ( ( i_src_stride << 3 ) - ( i_width << 1 ) );
+        p_dst0 += ( ( dst0_stride << 3 ) - i_width );
+        p_dst1 += ( ( dst1_stride << 3) - i_width );
+    }
+
+    for( i_loop_height = i_h4w; i_loop_height < i_height; i_loop_height++ )
+    {
+        for( i_loop_width = ( i_w_mul16 >> 4 ); i_loop_width--; )
+        {
+            in0 = __lasx_xvld( p_src, 0 );
+            p_src += 32;
+            vec_pckev0 = __lasx_xvpickev_b( in0, in0 );
+            vec_pckod0 = __lasx_xvpickod_b( in0, in0 );
+            __lasx_xvstelm_d( vec_pckev0, p_dst0, 0, 0 );
+            __lasx_xvstelm_d( vec_pckev0, p_dst0, 8, 2 );
+            __lasx_xvstelm_d( vec_pckod0, p_dst1, 0, 0 );
+            __lasx_xvstelm_d( vec_pckod0, p_dst1, 8, 2 );
+            p_dst0 += 16;
+            p_dst1 += 16;
+        }
+
+        for( i_loop_width = ( ( i_width & 15 ) >> 3 ); i_loop_width--; )
+        {
+            in0 = __lasx_xvld( p_src, 0 );
+            p_src += 16;
+            vec_pckev0 = __lasx_xvpickev_b( in0, in0 );
+            vec_pckod0 = __lasx_xvpickod_b( in0, in0 );
+            __lasx_xvstelm_d( vec_pckev0, p_dst0, 0, 0 );
+            __lasx_xvstelm_d( vec_pckod0, p_dst1, 0, 0 );
+            p_dst0 += 8;
+            p_dst1 += 8;
+        }
+
+        for( i_loop_width = ( ( i_width & 7 ) >> 2 ); i_loop_width--; )
+        {
+            in0 = __lasx_xvld( p_src, 0 );
+            p_src += 8;
+            vec_pckev0 = __lasx_xvpickev_b( in0, in0 );
+            vec_pckod0 = __lasx_xvpickod_b( in0, in0 );
+            __lasx_xvstelm_w( vec_pckev0, p_dst0, 0, 0 );
+            __lasx_xvstelm_w( vec_pckod0, p_dst1, 0, 0 );
+            p_dst0 += 4;
+            p_dst1 += 4;
+        }
+
+        for( i_loop_width = i_w_mul4; i_loop_width < i_width; i_loop_width++ )
+        {
+            p_dst0[0] = p_src[0];
+            p_dst1[0] = p_src[1];
+            p_dst0 += 1;
+            p_dst1 += 1;
+            p_src += 2;
+        }
+
+        p_src += ( ( i_src_stride ) - ( i_width << 1 ) );
+        p_dst0 += ( ( dst0_stride ) - i_width );
+        p_dst1 += ( ( dst1_stride ) - i_width );
+    }
+}
+
+static void core_plane_copy_interleave_lasx( uint8_t *p_src0,
+                                             int32_t i_src0_stride,
+                                             uint8_t *p_src1,
+                                             int32_t i_src1_stride,
+                                             uint8_t *p_dst,
+                                             int32_t i_dst_stride,
+                                             int32_t i_width, int32_t i_height )
+{
+    int32_t i_loop_width, i_loop_height, i_w_mul8, i_h4w;
+    __m256i src0, src1, src2, src3, src4, src5, src6, src7;
+    __m256i vec_ilv_l0, vec_ilv_l1, vec_ilv_l2, vec_ilv_l3;
+    __m256i vec_ilv_h0, vec_ilv_h1, vec_ilv_h2, vec_ilv_h3;
+    uint8_t *p_dst_t, *p_srcA, *p_srcB;
+    int32_t i_src0_stride_x2 = i_src0_stride << 1;
+    int32_t i_src1_stride_x2 = i_src1_stride << 1;
+    int32_t i_src0_stride_x3 = i_src0_stride_x2 + i_src0_stride;
+    int32_t i_src1_stride_x3 = i_src1_stride_x2 + i_src1_stride;
+    int32_t i_dst_stride_x2 = i_dst_stride << 1;
+    int32_t i_dst_stride_x3 = i_dst_stride_x2 + i_dst_stride;
+
+    i_w_mul8 = i_width - ( i_width & 7 );
+    i_h4w = i_height - ( i_height & 3 );
+
+    for( i_loop_height = ( i_h4w >> 2 ); i_loop_height--; )
+    {
+        for( i_loop_width = ( i_width >> 5 ); i_loop_width--; )
+        {
+            DUP4_ARG2(__lasx_xvldx, p_src0, 0, p_src0, i_src0_stride, p_src0,
+                      i_src0_stride_x2, p_src0, i_src0_stride_x3, src0, src1, src2, src3);
+            DUP4_ARG2(__lasx_xvldx, p_src1, 0, p_src1, i_src1_stride, p_src1,
+                      i_src1_stride_x2, p_src1, i_src1_stride_x3, src4, src5, src6, src7);
+
+            DUP4_ARG2( __lasx_xvilvl_b, src4, src0, src5, src1, src6, src2, src7, src3,
+                       vec_ilv_l0, vec_ilv_l1, vec_ilv_l2, vec_ilv_l3 );
+            DUP4_ARG2( __lasx_xvilvh_b, src4, src0, src5, src1, src6, src2, src7, src3,
+                       vec_ilv_h0, vec_ilv_h1, vec_ilv_h2, vec_ilv_h3 );
+
+            src0 = __lasx_xvpermi_q( vec_ilv_l0, vec_ilv_h0, 0x02 );
+            src1 = __lasx_xvpermi_q( vec_ilv_l1, vec_ilv_h1, 0x02 );
+            src2 = __lasx_xvpermi_q( vec_ilv_l2, vec_ilv_h2, 0x02 );
+            src3 = __lasx_xvpermi_q( vec_ilv_l3, vec_ilv_h3, 0x02 );
+
+            src4 = __lasx_xvpermi_q( vec_ilv_l0, vec_ilv_h0, 0x13 );
+            src5 = __lasx_xvpermi_q( vec_ilv_l1, vec_ilv_h1, 0x13 );
+            src6 = __lasx_xvpermi_q( vec_ilv_l2, vec_ilv_h2, 0x13 );
+            src7 = __lasx_xvpermi_q( vec_ilv_l3, vec_ilv_h3, 0x13 );
+
+            __lasx_xvst( src0, p_dst, 0 );
+            __lasx_xvstx( src1, p_dst, i_dst_stride );
+            __lasx_xvstx( src2, p_dst, i_dst_stride_x2 );
+            __lasx_xvstx( src3, p_dst, i_dst_stride_x3 );
+            __lasx_xvst( src4, p_dst, 32 );
+            __lasx_xvstx( src5, p_dst, 32 + i_dst_stride );
+            __lasx_xvstx( src6, p_dst, 32 + i_dst_stride_x2 );
+            __lasx_xvstx( src7, p_dst, 32 + i_dst_stride_x3 );
+
+            p_src0 += 32;
+            p_src1 += 32;
+            p_dst += 64;
+        }
+
+        for( i_loop_width = ( ( i_width & 31 ) >> 4 ); i_loop_width--; )
+        {
+            DUP4_ARG2(__lasx_xvldx, p_src0, 0, p_src0, i_src0_stride, p_src0,
+                      i_src0_stride_x2, p_src0, i_src0_stride_x3, src0, src1, src2, src3);
+            DUP4_ARG2(__lasx_xvldx, p_src1, 0, p_src1, i_src1_stride, p_src1,
+                      i_src1_stride_x2, p_src1, i_src1_stride_x3, src4, src5, src6, src7);
+            DUP4_ARG2(__lasx_xvilvl_b, src4, src0, src5, src1, src6, src2, src7, src3,
+                      vec_ilv_l0, vec_ilv_l1, vec_ilv_l2, vec_ilv_l3 );
+            DUP4_ARG2(__lasx_xvilvh_b, src4, src0, src5, src1, src6, src2, src7, src3,
+                      vec_ilv_h0, vec_ilv_h1, vec_ilv_h2, vec_ilv_h3 );
+
+            vec_ilv_l0 = __lasx_xvpermi_q( vec_ilv_l0, vec_ilv_h0, 0x02 );
+            vec_ilv_l1 = __lasx_xvpermi_q( vec_ilv_l1, vec_ilv_h1, 0x02 );
+            vec_ilv_l2 = __lasx_xvpermi_q( vec_ilv_l2, vec_ilv_h2, 0x02 );
+            vec_ilv_l3 = __lasx_xvpermi_q( vec_ilv_l3, vec_ilv_h3, 0x02 );
+
+            __lasx_xvst( vec_ilv_l0, p_dst, 0 );
+            __lasx_xvstx( vec_ilv_l1, p_dst, i_dst_stride );
+            __lasx_xvstx( vec_ilv_l2, p_dst, i_dst_stride_x2 );
+            __lasx_xvstx( vec_ilv_l3, p_dst, i_dst_stride_x3 );
+
+            p_src0 += 16;
+            p_src1 += 16;
+            p_dst += 32;
+        }
+
+        for( i_loop_width = ( i_width & 15 ) >> 3; i_loop_width--; )
+        {
+            DUP4_ARG2(__lasx_xvldx, p_src0, 0, p_src0, i_src0_stride, p_src0,
+                      i_src0_stride_x2, p_src0, i_src0_stride_x3, src0, src1, src2, src3);
+            DUP4_ARG2(__lasx_xvldx, p_src1, 0, p_src1, i_src1_stride, p_src1,
+                      i_src1_stride_x2, p_src1, i_src1_stride_x3, src4, src5, src6, src7);
+            DUP4_ARG2(__lasx_xvilvl_b, src4, src0, src5, src1, src6, src2, src7, src3,
+                      vec_ilv_l0, vec_ilv_l1, vec_ilv_l2, vec_ilv_l3 );
+
+            __lasx_xvstelm_d( vec_ilv_l0, p_dst, 0, 0 );
+            __lasx_xvstelm_d( vec_ilv_l0, p_dst, 8, 1 );
+            p_dst_t = p_dst + i_dst_stride;
+            __lasx_xvstelm_d( vec_ilv_l1, p_dst_t, 0, 0 );
+            __lasx_xvstelm_d( vec_ilv_l1, p_dst_t, 8, 1 );
+            p_dst_t = p_dst_t + i_dst_stride;
+            __lasx_xvstelm_d( vec_ilv_l2, p_dst_t, 0, 0 );
+            __lasx_xvstelm_d( vec_ilv_l2, p_dst_t, 8, 1 );
+            p_dst_t = p_dst_t + i_dst_stride;
+            __lasx_xvstelm_d( vec_ilv_l3, p_dst_t, 0, 0 );
+            __lasx_xvstelm_d( vec_ilv_l3, p_dst_t, 8, 1 );
+
+            p_src0 += 8;
+            p_src1 += 8;
+            p_dst += 16;
+        }
+
+        for( i_loop_width = i_w_mul8; i_loop_width < i_width; i_loop_width++ )
+        {
+            p_dst[0] = p_src0[0];
+            p_dst[1] = p_src1[0];
+
+            p_dst_t = p_dst + i_dst_stride;
+            p_srcA = p_src0 + i_src0_stride;
+            p_srcB = p_src1 + i_src1_stride;
+            p_dst_t[0] = p_srcA[0];
+            p_dst_t[1] = p_srcB[0];
+
+            p_dst_t += i_dst_stride;
+            p_srcA += i_src0_stride;
+            p_srcB += i_src1_stride;
+            p_dst_t[0] = p_srcA[0];
+            p_dst_t[1] = p_srcB[0];
+
+            p_dst_t += i_dst_stride;
+            p_srcA += i_src0_stride;
+            p_srcB += i_src1_stride;
+            p_dst_t[0] = p_srcA[0];
+            p_dst_t[1] = p_srcB[0];
+
+            p_src0 += 1;
+            p_src1 += 1;
+            p_dst += 2;
+        }
+
+        p_src0 += ( ( i_src0_stride << 2 ) - i_width );
+        p_src1 += ( ( i_src1_stride << 2 ) - i_width );
+        p_dst += ( ( i_dst_stride << 2 ) - ( i_width << 1 ) );
+    }
+
+    for( i_loop_height = i_h4w; i_loop_height < i_height; i_loop_height++ )
+    {
+        for( i_loop_width = ( i_width >> 5 ); i_loop_width--; )
+        {
+            src0 = __lasx_xvld( p_src0, 0 );
+            src4 = __lasx_xvld( p_src1, 0 );
+            vec_ilv_l0 = __lasx_xvilvl_b( src4, src0 );
+            vec_ilv_h0 = __lasx_xvilvh_b( src4, src0 );
+
+            src0 = __lasx_xvpermi_q( vec_ilv_l0, vec_ilv_h0, 0x02 );
+            src1 = __lasx_xvpermi_q( vec_ilv_l0, vec_ilv_h0, 0x13 );
+            __lasx_xvst( src0, p_dst, 0 );
+            __lasx_xvst( src1, p_dst, 32 );
+
+            p_src0 += 32;
+            p_src1 += 32;
+            p_dst += 64;
+        }
+
+        for( i_loop_width = ( ( i_width &  31 )  >> 4 ); i_loop_width--; )
+        {
+            src0 = __lasx_xvld( p_src0, 0 );
+            src4 = __lasx_xvld( p_src1, 0 );
+            vec_ilv_l0 = __lasx_xvilvl_b( src4, src0 );
+            vec_ilv_h0 = __lasx_xvilvh_b( src4, src0 );
+
+            vec_ilv_l0 = __lasx_xvpermi_q( vec_ilv_l0, vec_ilv_h0, 0x02 );
+            __lasx_xvst( vec_ilv_l0, p_dst, 0 );
+
+            p_src0 += 16;
+            p_src1 += 16;
+            p_dst += 32;
+        }
+
+        for( i_loop_width = ( i_width & 15 ) >> 3; i_loop_width--; )
+        {
+            src0 = __lasx_xvld( p_src0, 0 );
+            src4 = __lasx_xvld( p_src1, 0 );
+            vec_ilv_l0 = __lasx_xvilvl_b( src4, src0 );
+            __lasx_xvstelm_d( vec_ilv_l0, p_dst, 0, 0 );
+            __lasx_xvstelm_d( vec_ilv_l0, p_dst, 8, 1 );
+
+            p_src0 += 8;
+            p_src1 += 8;
+            p_dst += 16;
+        }
+
+        for( i_loop_width = i_w_mul8; i_loop_width < i_width; i_loop_width++ )
+        {
+            p_dst[0] = p_src0[0];
+            p_dst[1] = p_src1[0];
+            p_src0 += 1;
+            p_src1 += 1;
+            p_dst += 2;
+        }
+
+        p_src0 += ( i_src0_stride - i_width );
+        p_src1 += ( i_src1_stride - i_width );
+        p_dst += ( i_dst_stride - ( i_width << 1 ) );
+    }
+}
+
+static void core_store_interleave_chroma_lasx( uint8_t *p_src0,
+                                               int32_t i_src0_stride,
+                                               uint8_t *p_src1,
+                                               int32_t i_src1_stride,
+                                               uint8_t *p_dst,
+                                               int32_t i_dst_stride,
+                                               int32_t i_height )
+{
+    int32_t i_loop_height, i_h4w;
+    __m256i in0, in1, in2, in3, in4, in5, in6, in7;
+    __m256i tmp0, tmp1, tmp2, tmp3;
+    int32_t i_src0_stride_x2 = i_src0_stride << 1;
+    int32_t i_src1_stride_x2 = i_src1_stride << 1;
+    int32_t i_src0_stride_x4 = i_src0_stride << 2;
+    int32_t i_src1_stride_x4 = i_src1_stride << 2;
+    int32_t i_src0_stride_x3 = i_src0_stride_x2 + i_src0_stride;
+    int32_t i_src1_stride_x3 = i_src1_stride_x2 + i_src1_stride;
+
+    i_h4w = i_height & 3;
+    for( i_loop_height = ( i_height >> 2 ); i_loop_height--; )
+    {
+        DUP4_ARG2( __lasx_xvldx, p_src0, 0, p_src0, i_src0_stride, p_src0,
+                   i_src0_stride_x2, p_src0, i_src0_stride_x3, in0, in1, in2, in3 );
+        p_src0 += i_src0_stride_x4;
+        DUP4_ARG2( __lasx_xvldx, p_src1, 0, p_src1, i_src1_stride, p_src1,
+                   i_src1_stride_x2, p_src1, i_src1_stride_x3, in4, in5, in6, in7 );
+        p_src1 += i_src1_stride_x4;
+        DUP4_ARG2( __lasx_xvilvl_b, in4, in0, in5, in1, in6, in2, in7, in3,
+                   tmp0, tmp1, tmp2, tmp3 );
+
+        __lasx_xvstelm_d( tmp0, p_dst, 0, 0 );
+        __lasx_xvstelm_d( tmp0, p_dst, 8, 1 );
+        p_dst += i_dst_stride;
+        __lasx_xvstelm_d( tmp1, p_dst, 0, 0 );
+        __lasx_xvstelm_d( tmp1, p_dst, 8, 1 );
+        p_dst += i_dst_stride;
+        __lasx_xvstelm_d( tmp2, p_dst, 0, 0 );
+        __lasx_xvstelm_d( tmp2, p_dst, 8, 1 );
+        p_dst += i_dst_stride;
+        __lasx_xvstelm_d( tmp3, p_dst, 0, 0 );
+        __lasx_xvstelm_d( tmp3, p_dst, 8, 1 );
+        p_dst += i_dst_stride;
+    }
+
+    for( i_loop_height = i_h4w; i_loop_height--; )
+    {
+        in0 = __lasx_xvld( p_src0, 0 );
+        p_src0 += i_src0_stride;
+        in1 = __lasx_xvld( p_src1, 0 );
+        p_src1 += i_src1_stride;
+
+        tmp0 = __lasx_xvilvl_b( in1, in0 );
+
+        __lasx_xvstelm_d( tmp0, p_dst, 0, 0 );
+        __lasx_xvstelm_d( tmp0, p_dst, 8, 1 );
+        p_dst += i_dst_stride;
+    }
+}
+
+static void plane_copy_deinterleave_lasx( uint8_t *p_dst0,
+                                          intptr_t i_dst_stride0,
+                                          uint8_t *p_dst1,
+                                          intptr_t i_dst_stride1,
+                                          uint8_t *p_src, intptr_t i_src_stride,
+                                          int32_t i_width, int32_t i_height )
+{
+    core_plane_copy_deinterleave_lasx( p_src, i_src_stride,
+                                       p_dst0, i_dst_stride0,
+                                       p_dst1, i_dst_stride1,
+                                       i_width, i_height );
+}
+
+static void load_deinterleave_chroma_fenc_lasx( uint8_t *p_dst, uint8_t *p_src,
+                                                intptr_t i_src_stride,
+                                                int32_t i_height )
+{
+    core_plane_copy_deinterleave_lasx( p_src, i_src_stride, p_dst, FENC_STRIDE,
+                                       ( p_dst + ( FENC_STRIDE / 2 ) ),
+                                       FENC_STRIDE, 8, i_height );
+}
+
+static void plane_copy_interleave_lasx( uint8_t *p_dst, intptr_t i_dst_stride,
+                                       uint8_t *p_src0, intptr_t i_src_stride0,
+                                       uint8_t *p_src1, intptr_t i_src_stride1,
+                                       int32_t i_width, int32_t i_height )
+{
+    core_plane_copy_interleave_lasx( p_src0, i_src_stride0,
+                                     p_src1, i_src_stride1,
+                                     p_dst, i_dst_stride,
+                                     i_width, i_height );
+}
+
+static void load_deinterleave_chroma_fdec_lasx( uint8_t *p_dst, uint8_t *p_src,
+                                                intptr_t i_src_stride,
+                                                int32_t i_height )
+{
+    core_plane_copy_deinterleave_lasx( p_src, i_src_stride, p_dst, FDEC_STRIDE,
+                                       ( p_dst + ( FDEC_STRIDE / 2 ) ),
+                                       FDEC_STRIDE, 8, i_height );
+}
+
+static void store_interleave_chroma_lasx( uint8_t *p_dst, intptr_t i_dst_stride,
+                                          uint8_t *p_src0, uint8_t *p_src1,
+                                          int32_t i_height )
+{
+    core_store_interleave_chroma_lasx( p_src0, FDEC_STRIDE, p_src1, FDEC_STRIDE,
+                                       p_dst, i_dst_stride, i_height );
+}
+
+static void memzero_aligned_lasx( void *p_dst, size_t n )
+{
+    uint32_t i_tot32 = n >> 5;
+    uint32_t i_remain = n - ( i_tot32 << 5 );
+    int8_t i_cnt;
+    __m256i zero = __lasx_xvldi( 0 );
+
+    for ( i_cnt = i_tot32; i_cnt--; )
+    {
+        __lasx_xvst( zero, p_dst, 0 );
+        p_dst += 32;
+    }
+
+    if( i_remain )
+    {
+        memset( p_dst, 0, i_remain );
+    }
+}
+
+static void prefetch_ref_lasx( uint8_t *pix, intptr_t stride, int32_t parity )
+{
+    int32_t tmp = 0;
+    uint8_t *pix_tmp = pix, *pix_tmp2 = pix;
+
+    __asm__ volatile(
+    "addi.d    %[parity],    %[parity],      -1                   \n\t"
+    "addi.d    %[pix],       %[pix],         64                   \n\t"
+    "and       %[parity],    %[parity],      %[stride]            \n\t"
+    "slli.d    %[tmp],       %[parity],      3                    \n\t"
+    "add.d     %[pix_tmp],   %[pix],         %[tmp]               \n\t"
+    "slli.d    %[tmp],       %[stride],      1                    \n\t"
+    "add.d     %[parity],    %[stride],      %[tmp]               \n\t"
+    "preld     0,            %[pix_tmp],     0                    \n\t"
+    "add.d     %[pix_tmp2],  %[pix_tmp],     %[stride]            \n\t"
+    "preld     0,            %[pix_tmp2],    0                    \n\t"
+    "add.d     %[pix_tmp2],  %[pix_tmp2],    %[stride]            \n\t"
+    "preld     0,            %[pix_tmp2],    0                    \n\t"
+    "add.d     %[pix_tmp],   %[pix_tmp],     %[parity]            \n\t"
+    "preld     0,            %[pix_tmp],     0                    \n\t"
+    "add.d     %[pix],       %[pix_tmp2],    %[tmp]               \n\t"
+    "preld     0,            %[pix],         0                    \n\t"
+    "add.d     %[pix_tmp],   %[pix],         %[stride]            \n\t"
+    "preld     0,            %[pix_tmp],     0                    \n\t"
+    "add.d     %[pix_tmp],   %[pix_tmp],     %[stride]            \n\t"
+    "preld     0,            %[pix_tmp],     0                    \n\t"
+    "add.d     %[pix],       %[pix],         %[parity]            \n\t"
+    "preld     0,            %[pix],         0                    \n\t"
+     : [tmp]"+&r"(tmp), [pix_tmp]"+&r"(pix_tmp),
+       [pix_tmp2]"+&r"(pix_tmp2), [pix]"+&r"(pix),
+       [parity]"+&r"(parity)
+     : [stride]"r"(stride)
+     :
+    );
+}
+
+static void prefetch_fenc_422_lasx( uint8_t *pix_y, intptr_t stride_y,
+                                    uint8_t *pix_uv, intptr_t stride_uv,
+                                    int32_t mb_x )
+{
+    int64_t num1 = 0;
+    int64_t num2 = 0;
+    uint8_t *y_tmp = pix_y, *uv_tmp = pix_uv;
+
+    __asm__ volatile(
+    "andi      %[num1],      %[mb_x],         3                  \n\t"
+    "mul.d     %[num1],      %[num1],         %[stride_y]        \n\t"
+    "andi      %[mb_x],      %[mb_x],         6                  \n\t"
+    "mul.d     %[num2],      %[mb_x],         %[stride_uv]       \n\t"
+    "addi.d    %[pix_y],     %[pix_y],        64                 \n\t"
+    "addi.d    %[pix_uv],    %[pix_uv],       64                 \n\t"
+    "slli.d    %[num1],      %[num1],         2                  \n\t"
+    "add.d     %[pix_y],     %[pix_y],        %[num1]            \n\t"
+    "preld     0,            %[pix_y],        0                  \n\t"
+    "add.d     %[y_tmp],     %[pix_y],        %[stride_y]        \n\t"
+    "preld     0,            %[y_tmp],        0                  \n\t"
+    "add.d     %[pix_y],     %[y_tmp],        %[stride_y]        \n\t"
+    "preld     0,            %[pix_y],        0                  \n\t"
+    "slli.d    %[num2],      %[num2],         2                  \n\t"
+    "add.d     %[pix_y],     %[pix_y],        %[stride_y]        \n\t"
+    "preld     0,            %[pix_y],        0                  \n\t"
+    "add.d     %[pix_uv],    %[pix_uv],       %[num2]            \n\t"
+    "preld     0,            %[pix_uv],       0                  \n\t"
+    "add.d     %[uv_tmp],    %[pix_uv],       %[stride_uv]       \n\t"
+    "preld     0,            %[uv_tmp],       0                  \n\t"
+    "add.d     %[pix_uv],    %[uv_tmp],       %[stride_uv]       \n\t"
+    "preld     0,            %[pix_uv],       0                  \n\t"
+    "add.d     %[pix_uv],    %[pix_uv],       %[stride_uv]       \n\t"
+    "preld     0,            %[pix_uv],       0                  \n\t"
+     : [y_tmp]"+&r"(y_tmp),
+       [uv_tmp]"+&r"(uv_tmp),
+       [num2]"+&r"(num2),
+       [num1]"+&r"(num1),
+       [mb_x]"+&r"(mb_x),
+       [pix_y]"+&r"(pix_y),
+       [pix_uv]"+&r"(pix_uv)
+     : [stride_y]"r"(stride_y), [stride_uv]"r"(stride_uv)
+     :
+    );
+}
+
+static void prefetch_fenc_420_lasx( uint8_t *pix_y, intptr_t stride_y,
+                                    uint8_t *pix_uv, intptr_t stride_uv,
+                                    int32_t mb_x )
+{
+    int64_t num1 = 0;
+    int64_t num2 = 0;
+    uint8_t *y_tmp = pix_y;
+
+    __asm__ volatile(
+    "andi      %[num1],      %[mb_x],         3                  \n\t"
+    "mul.d     %[num1],      %[num1],         %[stride_y]        \n\t"
+    "andi      %[mb_x],      %[mb_x],         6                  \n\t"
+    "mul.d     %[num2],      %[mb_x],         %[stride_uv]       \n\t"
+    "addi.d    %[pix_y],     %[pix_y],        64                 \n\t"
+    "addi.d    %[pix_uv],    %[pix_uv],       64                 \n\t"
+    "slli.d    %[num1],      %[num1],         2                  \n\t"
+    "add.d     %[pix_y],     %[pix_y],        %[num1]            \n\t"
+    "preld     0,            %[pix_y],        0                  \n\t"
+    "add.d     %[y_tmp],     %[pix_y],        %[stride_y]        \n\t"
+    "preld     0,            %[y_tmp],        0                  \n\t"
+    "add.d     %[pix_y],     %[y_tmp],        %[stride_y]        \n\t"
+    "preld     0,            %[pix_y],        0                  \n\t"
+    "slli.d    %[num2],      %[num2],         2                  \n\t"
+    "add.d     %[pix_y],     %[pix_y],        %[stride_y]        \n\t"
+    "preld     0,            %[pix_y],        0                  \n\t"
+    "add.d     %[pix_uv],    %[pix_uv],       %[num2]            \n\t"
+    "preld     0,            %[pix_uv],       0                  \n\t"
+    "add.d     %[pix_uv],    %[pix_uv],       %[stride_uv]       \n\t"
+    "preld     0,            %[pix_uv],       0                  \n\t"
+     : [y_tmp]"+&r"(y_tmp),
+       [num2]"+&r"(num2),
+       [num1]"+&r"(num1),
+       [mb_x]"+&r"(mb_x),
+       [pix_y]"+&r"(pix_y),
+       [pix_uv]"+&r"(pix_uv)
+     : [stride_y]"r"(stride_y), [stride_uv]"r"(stride_uv)
+     :
+    );
+}
+
+#define x264_mc_chroma_lasx x264_template(mc_chroma_lasx)
+void x264_mc_chroma_lasx( uint8_t *p_dst_u, uint8_t *p_dst_v,
+                          intptr_t i_dst_stride,
+                          uint8_t *p_src, intptr_t i_src_stride,
+                          int32_t m_vx, int32_t m_vy,
+                          int32_t i_width, int32_t i_height );
+
+#endif // !HIGH_BIT_DEPTH
+
+void x264_mc_init_loongarch( int32_t cpu, x264_mc_functions_t *pf  )
+{
+#if !HIGH_BIT_DEPTH
+    if( cpu & X264_CPU_LASX )
+    {
+        pf->mc_luma = mc_luma_lasx;
+        pf->mc_chroma = x264_mc_chroma_lasx;
+        pf->get_ref = get_ref_lasx;
+
+        pf->avg[PIXEL_16x16]= pixel_avg_16x16_lasx;
+        pf->avg[PIXEL_16x8] = pixel_avg_16x8_lasx;
+        pf->avg[PIXEL_8x16] = pixel_avg_8x16_lasx;
+        pf->avg[PIXEL_8x8] = pixel_avg_8x8_lasx;
+        pf->avg[PIXEL_8x4] = pixel_avg_8x4_lasx;
+        pf->avg[PIXEL_4x16] = pixel_avg_4x16_lasx;
+        pf->avg[PIXEL_4x8] = pixel_avg_4x8_lasx;
+        pf->avg[PIXEL_4x4] = pixel_avg_4x4_lasx;
+        pf->avg[PIXEL_4x2] = pixel_avg_4x2_lasx;
+
+        pf->weight = mc_weight_wtab_lasx;
+        pf->offsetadd = mc_weight_wtab_lasx;
+        pf->offsetsub = mc_weight_wtab_lasx;
+        pf->weight_cache = weight_cache_lasx;
+
+        pf->copy_16x16_unaligned = mc_copy_w16_lasx;
+        pf->copy[PIXEL_16x16] = mc_copy_w16_lasx;
+        pf->copy[PIXEL_8x8] = mc_copy_w8_lasx;
+        pf->copy[PIXEL_4x4] = mc_copy_w4_lasx;
+
+        pf->store_interleave_chroma = store_interleave_chroma_lasx;
+        pf->load_deinterleave_chroma_fenc = load_deinterleave_chroma_fenc_lasx;
+        pf->load_deinterleave_chroma_fdec = load_deinterleave_chroma_fdec_lasx;
+
+        pf->plane_copy_interleave = plane_copy_interleave_lasx;
+        pf->plane_copy_deinterleave = plane_copy_deinterleave_lasx;
+        pf->plane_copy_deinterleave_yuyv = plane_copy_deinterleave_lasx;
+
+        pf->hpel_filter = hpel_filter_lasx;
+        pf->memcpy_aligned = x264_memcpy_aligned_lasx;
+        pf->memzero_aligned = memzero_aligned_lasx;
+        pf->frame_init_lowres_core = frame_init_lowres_core_lasx;
+
+        pf->prefetch_fenc_420 = prefetch_fenc_420_lasx;
+        pf->prefetch_fenc_422 = prefetch_fenc_422_lasx;
+        pf->prefetch_ref  = prefetch_ref_lasx;
+    }
+#endif // !HIGH_BIT_DEPTH
+}
diff --git a/common/loongarch/mc.h b/common/loongarch/mc.h
new file mode 100644
index 0000000..ce1b9a7
--- /dev/null
+++ b/common/loongarch/mc.h
@@ -0,0 +1,33 @@
+/*****************************************************************************
+ * mc.h: loongarch motion compensation
+ *****************************************************************************
+ * Copyright (C) 2015-2018 x264 project
+ * Copyright (C) 2020 Loongson Technology Corporation Limited
+ *
+ * Authors: zhou peng <zhoupeng@loongson.cn>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02111, USA.
+ *
+ * This program is also available under a commercial proprietary license.
+ * For more information, contact us at licensing@x264.com.
+ *****************************************************************************/
+
+#ifndef X264_LOONGARCH_MC_H
+#define X264_LOONGARCH_MC_H
+
+#define x264_mc_init_loongarch x264_template(mc_init_loongarch)
+void x264_mc_init_loongarch( int cpu, x264_mc_functions_t *pf );
+
+#endif
diff --git a/common/loongarch/pixel-c.c b/common/loongarch/pixel-c.c
new file mode 100644
index 0000000..0811887
--- /dev/null
+++ b/common/loongarch/pixel-c.c
@@ -0,0 +1,3204 @@
+/*****************************************************************************
+ * pixel-c.c: loongarch pixel metrics
+ *****************************************************************************
+ * Copyright (C) 2015-2018 x264 project
+ * Copyright (C) 2020 Loongson Technology Corporation Limited
+ *
+ * Authors: zhou peng <zhoupeng@loongson.cn>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02111, USA.
+ *
+ * This program is also available under a commercial proprietary license.
+ * For more information, contact us at licensing@x264.com.
+ *****************************************************************************/
+
+#include "common/common.h"
+#include "loongson_intrinsics.h"
+#include "pixel.h"
+#include "predict.h"
+
+#if !HIGH_BIT_DEPTH
+
+#define LASX_LOAD_4(p_src, _stride, _stride2, _stride3, _src0, _src1, _src2, _src3)      \
+{                                                                                        \
+    _src0 = __lasx_xvld(p_src, 0);                                                       \
+    _src1 = __lasx_xvldx(p_src, _stride);                                                \
+    _src2 = __lasx_xvldx(p_src, _stride2);                                               \
+    _src3 = __lasx_xvldx(p_src, _stride3);                                               \
+}
+
+static inline int32_t pixel_satd_4width_lasx( uint8_t *p_src, int32_t i_src_stride,
+                                              uint8_t *p_ref, int32_t i_ref_stride,
+                                              uint8_t i_height )
+{
+    int32_t cnt;
+    uint32_t u_sum, sum1, sum2;
+    __m256i src0, src1, src2, src3;
+    __m256i ref0, ref1, ref2, ref3;
+    __m256i diff0, diff1, diff2, diff3;
+    __m256i tmp0, tmp1;
+    __m256i sum = __lasx_xvldi(0);
+    int32_t i_src_stride_x2 = i_src_stride << 1;
+    int32_t i_ref_stride_x2 = i_ref_stride << 1;
+    int32_t i_src_stride_x3 = i_src_stride_x2 + i_src_stride;
+    int32_t i_ref_stride_x3 = i_ref_stride_x2 + i_ref_stride;
+    int32_t i_src_stride_x4 = i_src_stride_x2 << 1;
+    int32_t i_ref_stride_x4 = i_ref_stride_x2 << 1;
+
+    for( cnt = i_height >> 3; cnt--; )
+    {
+        LASX_LOAD_4( p_src, i_src_stride, i_src_stride_x2, i_src_stride_x3,
+                     src0, src1, src2, src3 );
+        p_src += i_src_stride_x4;
+        LASX_LOAD_4( p_src, i_src_stride, i_src_stride_x2, i_src_stride_x3,
+                     diff0, diff1, diff2, diff3 );
+        p_src += i_src_stride_x4;
+        src0 = __lasx_xvilvl_w(src1, src0);
+        src1 = __lasx_xvilvl_w(src3, src2);
+        src2 = __lasx_xvilvl_w(diff1, diff0);
+        src3 = __lasx_xvilvl_w(diff3, diff2);
+        src0 = __lasx_xvpermi_q(src0, src2, 0x02);
+        src1 = __lasx_xvpermi_q(src1, src3, 0x02);
+        src0 = __lasx_xvilvl_d(src1, src0);
+
+
+        LASX_LOAD_4( p_ref, i_ref_stride, i_ref_stride_x2, i_ref_stride_x3,
+                     ref0, ref1, ref2, ref3 );
+        p_ref += i_ref_stride_x4;
+        LASX_LOAD_4( p_ref, i_ref_stride, i_ref_stride_x2, i_ref_stride_x3,
+                     diff0, diff1, diff2, diff3 );
+        p_ref += i_ref_stride_x4;
+        ref0 = __lasx_xvilvl_w(ref1, ref0);
+        ref1 = __lasx_xvilvl_w(ref3, ref2);
+        ref2 = __lasx_xvilvl_w(diff1, diff0);
+        ref3 = __lasx_xvilvl_w(diff3, diff2);
+        ref0 = __lasx_xvpermi_q(ref0, ref2, 0x02);
+        ref1 = __lasx_xvpermi_q(ref1, ref3, 0x02);
+        ref0 = __lasx_xvilvl_d(ref1, ref0);
+
+        diff0 = __lasx_xvsubwev_h_bu(src0, ref0);
+        diff1 = __lasx_xvsubwod_h_bu(src0, ref0);
+
+        tmp0 = __lasx_xvadd_h(diff0, diff1);
+        tmp1 = __lasx_xvsub_h(diff0, diff1);
+
+        diff0 = __lasx_xvpackev_h(tmp1, tmp0);
+        diff1 = __lasx_xvpackod_h(tmp1, tmp0);
+
+        tmp0 = __lasx_xvadd_h(diff0, diff1);
+        tmp1 = __lasx_xvsub_h(diff0, diff1);
+
+        diff0 = __lasx_xvpackev_w(tmp1, tmp0);
+        diff1 = __lasx_xvpackod_w(tmp1, tmp0);
+
+        tmp0 = __lasx_xvadd_h(diff0, diff1);
+        tmp1 = __lasx_xvsub_h(diff0, diff1);
+
+        diff0 = __lasx_xvpackev_d(tmp1, tmp0);
+        diff1 = __lasx_xvpackod_d(tmp1, tmp0);
+
+        tmp0 = __lasx_xvadd_h(diff0, diff1);
+        tmp1 = __lasx_xvsub_h(diff0, diff1);
+
+        diff0 = __lasx_xvpackev_d(tmp1, tmp0);
+        diff1 = __lasx_xvpackod_d(tmp1, tmp0);
+
+        diff0 = __lasx_xvadda_h(diff0, diff1);
+        sum = __lasx_xvadd_h(sum, diff0);
+    }
+    sum = __lasx_xvhaddw_wu_hu( sum, sum );
+    sum = __lasx_xvhaddw_du_wu( sum, sum );
+    sum = __lasx_xvhaddw_qu_du( sum, sum );
+    sum1 = __lasx_xvpickve2gr_wu(sum, 0);
+    sum2 = __lasx_xvpickve2gr_wu(sum, 4);
+    u_sum = sum1 + sum2;
+
+    return ( u_sum >> 1 );
+}
+
+int32_t x264_pixel_satd_4x4_lasx( uint8_t *p_pix1, intptr_t i_stride,
+                                  uint8_t *p_pix2, intptr_t i_stride2 )
+{
+    uint32_t sum;
+    intptr_t i_stride_2 = i_stride << 1;
+    intptr_t i_stride2_2 = i_stride2 << 1;
+    intptr_t i_stride_3 = i_stride_2 + i_stride;
+    intptr_t i_stride2_3 = i_stride2_2 + i_stride2;
+    __m256i src0, src1, src2, src3;
+    __m256i ref0, ref1, ref2, ref3;
+    __m256i diff0, diff1, tmp0, tmp1;
+
+    src0 = __lasx_xvld(p_pix1, 0);
+    src1 = __lasx_xvldx(p_pix1, i_stride);
+    src2 = __lasx_xvldx(p_pix1, i_stride_2);
+    src3 = __lasx_xvldx(p_pix1, i_stride_3);
+    ref0 = __lasx_xvld(p_pix2, 0);
+    ref1 = __lasx_xvldx(p_pix2, i_stride2);
+    ref2 = __lasx_xvldx(p_pix2, i_stride2_2);
+    ref3 = __lasx_xvldx(p_pix2, i_stride2_3);
+
+    src0 = __lasx_xvilvl_w(src1, src0);
+    src1 = __lasx_xvilvl_w(src3, src2);
+    ref0 = __lasx_xvilvl_w(ref1, ref0);
+    ref1 = __lasx_xvilvl_w(ref3, ref2);
+    src0 = __lasx_xvilvl_d(src1, src0);
+    ref0 = __lasx_xvilvl_d(ref1, ref0);
+
+    diff0 = __lasx_xvsubwev_h_bu(src0, ref0);
+    diff1 = __lasx_xvsubwod_h_bu(src0, ref0);
+    tmp0  = __lasx_xvadd_h(diff0, diff1);
+    tmp1  = __lasx_xvsub_h(diff0, diff1);
+    diff0 = __lasx_xvpackev_h(tmp1, tmp0);
+    diff1 = __lasx_xvpackod_h(tmp1, tmp0);
+    tmp0  = __lasx_xvadd_h(diff0, diff1);
+    tmp1  = __lasx_xvsub_h(diff0, diff1);
+    diff0 = __lasx_xvpackev_w(tmp1, tmp0);
+    diff1 = __lasx_xvpackod_w(tmp1, tmp0);
+    tmp0  = __lasx_xvadd_h(diff0, diff1);
+    tmp1  = __lasx_xvsub_h(diff0, diff1);
+    diff0 = __lasx_xvpackev_d(tmp1, tmp0);
+    diff1 = __lasx_xvpackod_d(tmp1, tmp0);
+    tmp0  = __lasx_xvadd_h(diff0, diff1);
+    tmp1  = __lasx_xvsub_h(diff0, diff1);
+    diff0 = __lasx_xvpackev_d(tmp1, tmp0);
+    diff1 = __lasx_xvpackod_d(tmp1, tmp0);
+    diff0 = __lasx_xvadda_h(diff0, diff1);
+    diff0 = __lasx_xvhaddw_wu_hu( diff0, diff0 );
+    diff0 = __lasx_xvhaddw_du_wu( diff0, diff0 );
+    diff0 = __lasx_xvhaddw_qu_du( diff0, diff0 );
+    sum   = __lasx_xvpickve2gr_wu(diff0, 0);
+    return ( sum >> 1 );
+}
+
+static inline int32_t pixel_satd_8width_lasx( uint8_t *p_pix1, int32_t i_stride,
+                                              uint8_t *p_pix2, int32_t i_stride2,
+                                              uint8_t i_height )
+{
+    int32_t sum, i_8 = 8;
+    uint32_t sum1, sum2;
+    int64_t stride_2, stride_3, stride_4, stride2_2, stride2_3, stride2_4;
+
+     __asm__ volatile (
+    "slli.d         %[stride_2],      %[i_stride],          1                      \n\t"
+    "slli.d         %[stride2_2],     %[i_stride2],         1                      \n\t"
+    "add.d          %[stride_3],      %[i_stride],          %[stride_2]            \n\t"
+    "add.d          %[stride2_3],     %[i_stride2],         %[stride2_2]           \n\t"
+    "slli.d         %[stride_4],      %[stride_2],          1                      \n\t"
+    "slli.d         %[stride2_4],     %[stride2_2],         1                      \n\t"
+    "xvldi          $xr16,            0                                            \n\t"
+    "1:                                                                            \n\t"
+    "addi.d         %[i_height],      %[i_height],          -8                     \n\t"
+    "vld            $vr0,             %[p_pix1],            0                      \n\t"
+    "vldx           $vr1,             %[p_pix1],            %[i_stride]            \n\t"
+    "vldx           $vr2,             %[p_pix1],            %[stride_2]            \n\t"
+    "vldx           $vr3,             %[p_pix1],            %[stride_3]            \n\t"
+    "add.d          %[p_pix1],        %[p_pix1],            %[stride_4]            \n\t"
+    "vld            $vr4,             %[p_pix1],            0                      \n\t"
+    "vldx           $vr5,             %[p_pix1],            %[i_stride]            \n\t"
+    "vldx           $vr6,             %[p_pix1],            %[stride_2]            \n\t"
+    "vldx           $vr7,             %[p_pix1],            %[stride_3]            \n\t"
+    "add.d          %[p_pix1],        %[p_pix1],            %[stride_4]            \n\t"
+    "vld            $vr8,             %[p_pix2],            0                      \n\t"
+    "vldx           $vr9,             %[p_pix2],            %[i_stride2]           \n\t"
+    "vldx           $vr10,            %[p_pix2],            %[stride2_2]           \n\t"
+    "vldx           $vr11,            %[p_pix2],            %[stride2_3]           \n\t"
+    "add.d          %[p_pix2],        %[p_pix2],            %[stride2_4]           \n\t"
+    "vld            $vr12,            %[p_pix2],            0                      \n\t"
+    "vldx           $vr13,            %[p_pix2],            %[i_stride2]           \n\t"
+    "vldx           $vr14,            %[p_pix2],            %[stride2_2]           \n\t"
+    "vldx           $vr15,            %[p_pix2],            %[stride2_3]           \n\t"
+    "add.d          %[p_pix2],        %[p_pix2],            %[stride2_4]           \n\t"
+    "vilvl.d        $vr0,             $vr1,                 $vr0                   \n\t"
+    "vilvl.d        $vr1,             $vr3,                 $vr2                   \n\t"
+    "vilvl.d        $vr2,             $vr5,                 $vr4                   \n\t"
+    "vilvl.d        $vr3,             $vr7,                 $vr6                   \n\t"
+    "xvpermi.q      $xr0,             $xr2,                 2                      \n\t"
+    "xvpermi.q      $xr1,             $xr3,                 2                      \n\t"
+    "vilvl.d        $vr2,             $vr9,                 $vr8                   \n\t"
+    "vilvl.d        $vr3,             $vr11,                $vr10                  \n\t"
+    "vilvl.d        $vr4,             $vr13,                $vr12                  \n\t"
+    "vilvl.d        $vr5,             $vr15,                $vr14                  \n\t"
+    "xvpermi.q      $xr2,             $xr4,                 2                      \n\t"
+    "xvpermi.q      $xr3,             $xr5,                 2                      \n\t"
+    "xvsubwev.h.bu  $xr4,             $xr0,                 $xr2                   \n\t"
+    "xvsubwod.h.bu  $xr5,             $xr0,                 $xr2                   \n\t"
+    "xvsubwev.h.bu  $xr6,             $xr1,                 $xr3                   \n\t"
+    "xvsubwod.h.bu  $xr7,             $xr1,                 $xr3                   \n\t"
+    "xvadd.h        $xr0,             $xr4,                 $xr5                   \n\t"
+    "xvsub.h        $xr1,             $xr4,                 $xr5                   \n\t"
+    "xvadd.h        $xr2,             $xr6,                 $xr7                   \n\t"
+    "xvsub.h        $xr3,             $xr6,                 $xr7                   \n\t"
+    "xvpackev.h     $xr4,             $xr1,                 $xr0                   \n\t"
+    "xvpackod.h     $xr5,             $xr1,                 $xr0                   \n\t"
+    "xvpackev.h     $xr6,             $xr3,                 $xr2                   \n\t"
+    "xvpackod.h     $xr7,             $xr3,                 $xr2                   \n\t"
+    "xvadd.h        $xr0,             $xr4,                 $xr5                   \n\t"
+    "xvsub.h        $xr1,             $xr4,                 $xr5                   \n\t"
+    "xvadd.h        $xr2,             $xr6,                 $xr7                   \n\t"
+    "xvsub.h        $xr3,             $xr6,                 $xr7                   \n\t"
+    "xvilvl.h       $xr4,             $xr1,                 $xr0                   \n\t"
+    "xvilvh.h       $xr5,             $xr1,                 $xr0                   \n\t"
+    "xvilvl.h       $xr6,             $xr3,                 $xr2                   \n\t"
+    "xvilvh.h       $xr7,             $xr3,                 $xr2                   \n\t"
+    "xvadd.h        $xr0,             $xr4,                 $xr5                   \n\t"
+    "xvadd.h        $xr2,             $xr6,                 $xr7                   \n\t"
+    "xvsub.h        $xr1,             $xr4,                 $xr5                   \n\t"
+    "xvsub.h        $xr3,             $xr6,                 $xr7                   \n\t"
+    "xvadd.h        $xr4,             $xr0,                 $xr2                   \n\t"
+    "xvadd.h        $xr5,             $xr1,                 $xr3                   \n\t"
+    "xvsub.h        $xr6,             $xr0,                 $xr2                   \n\t"
+    "xvsub.h        $xr7,             $xr1,                 $xr3                   \n\t"
+    "xvadda.h       $xr0,             $xr4,                 $xr5                   \n\t"
+    "xvadda.h       $xr1,             $xr6,                 $xr7                   \n\t"
+    "xvadd.h        $xr0,             $xr0,                 $xr1                   \n\t"
+    "xvadd.h        $xr16,            $xr16,                $xr0                   \n\t"
+    "bge            %[i_height],      %[i_8],               1b                     \n\t"
+    "2:                                                                            \n\t"
+    "xvhaddw.wu.hu  $xr16,            $xr16,                $xr16                  \n\t"
+    "xvhaddw.du.wu  $xr16,            $xr16,                $xr16                  \n\t"
+    "xvhaddw.qu.du  $xr16,            $xr16,                $xr16                  \n\t"
+    "xvpickve2gr.wu %[sum1],          $xr16,                0                      \n\t"
+    "xvpickve2gr.wu %[sum2],          $xr16,                4                      \n\t"
+    "add.w          %[sum],           %[sum1],              %[sum2]                \n\t"
+    : [stride_2]"=&r"(stride_2), [stride_3]"=&r"(stride_3), [stride_4]"=&r"(stride_4),
+      [stride2_2]"=&r"(stride2_2), [stride2_3]"=&r"(stride2_3), [stride2_4]"=&r"(stride2_4),
+      [sum1]"=&r"(sum1), [sum2]"=&r"(sum2), [sum]"=&r"(sum), [p_pix1]"+&r"(p_pix1), [p_pix2]"+&r"(p_pix2),
+      [i_height]"+&r"(i_height)
+    : [i_stride]"r"(i_stride), [i_stride2]"r"(i_stride2), [i_8]"r"(i_8)
+    : "memory"
+    );
+
+    return ( sum >> 1 );
+}
+
+int32_t x264_pixel_satd_4x8_lasx( uint8_t *p_pix1, intptr_t i_stride,
+                                  uint8_t *p_pix2, intptr_t i_stride2 )
+{
+    return pixel_satd_4width_lasx( p_pix1, i_stride, p_pix2, i_stride2, 8 );
+}
+
+int32_t x264_pixel_satd_4x16_lasx( uint8_t *p_pix1, intptr_t i_stride,
+                                   uint8_t *p_pix2, intptr_t i_stride2 )
+{
+    return pixel_satd_4width_lasx( p_pix1, i_stride, p_pix2, i_stride2, 16 );
+}
+
+int32_t x264_pixel_satd_8x4_lasx( uint8_t *p_pix1, intptr_t i_stride,
+                                  uint8_t *p_pix2, intptr_t i_stride2 )
+{
+    uint32_t u_sum = 0, sum1, sum2;
+    intptr_t i_stride_2 = i_stride << 1;
+    intptr_t i_stride2_2 = i_stride2 << 1;
+    intptr_t i_stride_3 = i_stride_2 + i_stride;
+    intptr_t i_stride2_3 = i_stride2_2 + i_stride2;
+    __m256i src0, src1, src2, src3;
+    __m256i ref0, ref1, ref2, ref3;
+    __m256i dif0, dif1;
+    __m256i tmp0, tmp1;
+
+    LASX_LOAD_4(p_pix1, i_stride, i_stride_2, i_stride_3, src0, src1, src2, src3);
+    LASX_LOAD_4(p_pix2, i_stride2, i_stride2_2, i_stride2_3, ref0, ref1, ref2, ref3);
+
+    src0 = __lasx_xvilvl_d(src1, src0);
+    src1 = __lasx_xvilvl_d(src3, src2);
+    ref0 = __lasx_xvilvl_d(ref1, ref0);
+    ref1 = __lasx_xvilvl_d(ref3, ref2);
+    src0 = __lasx_xvpermi_q(src0, src1, 2);
+    ref0 = __lasx_xvpermi_q(ref0, ref1, 2);
+    dif0 = __lasx_xvsubwev_h_bu(src0, ref0);
+    dif1 = __lasx_xvsubwod_h_bu(src0, ref0);
+    tmp0 = __lasx_xvadd_h(dif0, dif1);
+    tmp1 = __lasx_xvsub_h(dif0, dif1);
+    dif0 = __lasx_xvpackev_h(tmp1, tmp0);
+    dif1 = __lasx_xvpackod_h(tmp1, tmp0);
+    tmp0 = __lasx_xvadd_h(dif0, dif1);
+    tmp1 = __lasx_xvsub_h(dif0, dif1);
+    dif0 = __lasx_xvpackev_d(tmp1, tmp0);
+    dif1 = __lasx_xvpackod_d(tmp1, tmp0);
+    tmp0 = __lasx_xvadd_h(dif0, dif1);
+    tmp1 = __lasx_xvsub_h(dif0, dif1);
+    dif0 = __lasx_xvpermi_q(tmp0, tmp1, 0x02);
+    dif1 = __lasx_xvpermi_q(tmp0, tmp1, 0x13);
+    tmp0 = __lasx_xvadd_h(dif0, dif1);
+    tmp1 = __lasx_xvsub_h(dif0, dif1);
+    dif0 = __lasx_xvpackev_d(tmp1, tmp0);
+    dif1 = __lasx_xvpackod_d(tmp1, tmp0);
+    dif0 = __lasx_xvadda_h(dif0, dif1);
+    dif0 = __lasx_xvhaddw_wu_hu(dif0, dif0);
+    dif0 = __lasx_xvhaddw_du_wu(dif0, dif0);
+    dif0 = __lasx_xvhaddw_qu_du(dif0, dif0);
+    sum1 = __lasx_xvpickve2gr_wu(dif0, 0);
+    sum2 = __lasx_xvpickve2gr_wu(dif0, 4);
+    u_sum = sum1 + sum2;
+
+    return ( u_sum >> 1 );
+}
+
+int32_t x264_pixel_satd_8x8_lasx( uint8_t *p_pix1, intptr_t i_stride,
+                                  uint8_t *p_pix2, intptr_t i_stride2 )
+{
+    uint32_t sum;
+    uint32_t sum1, sum2;
+    int64_t stride_2, stride_3, stride_4, stride2_2, stride2_3, stride2_4;
+    uint8_t *pix1, *pix2;
+
+    __asm__ volatile (
+    "slli.d         %[stride_2],      %[i_stride],          1                      \n\t"
+    "slli.d         %[stride2_2],     %[i_stride2],         1                      \n\t"
+    "add.d          %[stride_3],      %[i_stride],          %[stride_2]            \n\t"
+    "add.d          %[stride2_3],     %[i_stride2],         %[stride2_2]           \n\t"
+    "slli.d         %[stride_4],      %[stride_2],          1                      \n\t"
+    "slli.d         %[stride2_4],     %[stride2_2],         1                      \n\t"
+    "add.d          %[pix1],          %[p_pix1],            %[stride_4]            \n\t"
+    "add.d          %[pix2],          %[p_pix2],            %[stride2_4]           \n\t"
+    "vld            $vr0,             %[p_pix1],            0                      \n\t"
+    "vldx           $vr1,             %[p_pix1],            %[i_stride]            \n\t"
+    "vldx           $vr2,             %[p_pix1],            %[stride_2]            \n\t"
+    "vldx           $vr3,             %[p_pix1],            %[stride_3]            \n\t"
+    "vld            $vr4,             %[pix1],              0                      \n\t"
+    "vldx           $vr5,             %[pix1],              %[i_stride]            \n\t"
+    "vldx           $vr6,             %[pix1],              %[stride_2]            \n\t"
+    "vldx           $vr7,             %[pix1],              %[stride_3]            \n\t"
+    "vld            $vr8,             %[p_pix2],            0                      \n\t"
+    "vldx           $vr9,             %[p_pix2],            %[i_stride2]           \n\t"
+    "vldx           $vr10,            %[p_pix2],            %[stride2_2]           \n\t"
+    "vldx           $vr11,            %[p_pix2],            %[stride2_3]           \n\t"
+    "vld            $vr12,            %[pix2],              0                      \n\t"
+    "vldx           $vr13,            %[pix2],              %[i_stride2]           \n\t"
+    "vldx           $vr14,            %[pix2],              %[stride2_2]           \n\t"
+    "vldx           $vr15,            %[pix2],              %[stride2_3]           \n\t"
+    "vilvl.d        $vr0,             $vr1,                 $vr0                   \n\t"
+    "vilvl.d        $vr1,             $vr3,                 $vr2                   \n\t"
+    "vilvl.d        $vr2,             $vr5,                 $vr4                   \n\t"
+    "vilvl.d        $vr3,             $vr7,                 $vr6                   \n\t"
+    "xvpermi.q      $xr0,             $xr2,                 2                      \n\t"
+    "xvpermi.q      $xr1,             $xr3,                 2                      \n\t"
+    "vilvl.d        $vr2,             $vr9,                 $vr8                   \n\t"
+    "vilvl.d        $vr3,             $vr11,                $vr10                  \n\t"
+    "vilvl.d        $vr4,             $vr13,                $vr12                  \n\t"
+    "vilvl.d        $vr5,             $vr15,                $vr14                  \n\t"
+    "xvpermi.q      $xr2,             $xr4,                 2                      \n\t"
+    "xvpermi.q      $xr3,             $xr5,                 2                      \n\t"
+    "xvsubwev.h.bu  $xr4,             $xr0,                 $xr2                   \n\t"
+    "xvsubwod.h.bu  $xr5,             $xr0,                 $xr2                   \n\t"
+    "xvsubwev.h.bu  $xr6,             $xr1,                 $xr3                   \n\t"
+    "xvsubwod.h.bu  $xr7,             $xr1,                 $xr3                   \n\t"
+    "xvadd.h        $xr0,             $xr4,                 $xr5                   \n\t"
+    "xvsub.h        $xr1,             $xr4,                 $xr5                   \n\t"
+    "xvadd.h        $xr2,             $xr6,                 $xr7                   \n\t"
+    "xvsub.h        $xr3,             $xr6,                 $xr7                   \n\t"
+    "xvpackev.h     $xr4,             $xr1,                 $xr0                   \n\t"
+    "xvpackod.h     $xr5,             $xr1,                 $xr0                   \n\t"
+    "xvpackev.h     $xr6,             $xr3,                 $xr2                   \n\t"
+    "xvpackod.h     $xr7,             $xr3,                 $xr2                   \n\t"
+    "xvadd.h        $xr0,             $xr4,                 $xr5                   \n\t"
+    "xvsub.h        $xr1,             $xr4,                 $xr5                   \n\t"
+    "xvadd.h        $xr2,             $xr6,                 $xr7                   \n\t"
+    "xvsub.h        $xr3,             $xr6,                 $xr7                   \n\t"
+    "xvilvl.h       $xr4,             $xr1,                 $xr0                   \n\t"
+    "xvilvh.h       $xr5,             $xr1,                 $xr0                   \n\t"
+    "xvilvl.h       $xr6,             $xr3,                 $xr2                   \n\t"
+    "xvilvh.h       $xr7,             $xr3,                 $xr2                   \n\t"
+    "xvadd.h        $xr0,             $xr4,                 $xr5                   \n\t"
+    "xvadd.h        $xr2,             $xr6,                 $xr7                   \n\t"
+    "xvsub.h        $xr1,             $xr4,                 $xr5                   \n\t"
+    "xvsub.h        $xr3,             $xr6,                 $xr7                   \n\t"
+    "xvadd.h        $xr4,             $xr0,                 $xr2                   \n\t"
+    "xvadd.h        $xr5,             $xr1,                 $xr3                   \n\t"
+    "xvsub.h        $xr6,             $xr0,                 $xr2                   \n\t"
+    "xvsub.h        $xr7,             $xr1,                 $xr3                   \n\t"
+    "xvadda.h       $xr0,             $xr4,                 $xr5                   \n\t"
+    "xvadda.h       $xr1,             $xr6,                 $xr7                   \n\t"
+    "xvadd.h        $xr0,             $xr0,                 $xr1                   \n\t"
+    "xvhaddw.wu.hu  $xr0,             $xr0,                 $xr0                   \n\t"
+    "xvhaddw.du.wu  $xr0,             $xr0,                 $xr0                   \n\t"
+    "xvhaddw.qu.du  $xr0,             $xr0,                 $xr0                   \n\t"
+    "xvpickve2gr.wu %[sum1],          $xr0,                 0                      \n\t"
+    "xvpickve2gr.wu %[sum2],          $xr0,                 4                      \n\t"
+    "add.w          %[sum],           %[sum1],              %[sum2]                \n\t"
+    : [stride_2]"=&r"(stride_2), [stride_3]"=&r"(stride_3), [stride_4]"=&r"(stride_4),
+      [stride2_2]"=&r"(stride2_2), [stride2_3]"=&r"(stride2_3), [stride2_4]"=&r"(stride2_4),
+      [pix1]"=&r"(pix1), [pix2]"=&r"(pix2), [sum1]"=&r"(sum1), [sum2]"=&r"(sum2), [sum]"=&r"(sum)
+    : [i_stride]"r"(i_stride), [i_stride2]"r"(i_stride2), [p_pix1]"r"(p_pix1), [p_pix2]"r"(p_pix2)
+    : "memory"
+    );
+
+    return ( sum >> 1 );
+}
+
+int32_t x264_pixel_satd_8x16_lasx( uint8_t *p_pix1, intptr_t i_stride,
+                                   uint8_t *p_pix2, intptr_t i_stride2 )
+{
+    return pixel_satd_8width_lasx( p_pix1, i_stride, p_pix2, i_stride2, 16 );
+}
+
+int32_t x264_pixel_satd_16x8_lasx( uint8_t *p_pix1, intptr_t i_stride,
+                                   uint8_t *p_pix2, intptr_t i_stride2 )
+{
+    int32_t sum;
+    uint32_t sum1, sum2;
+    int64_t stride_2, stride_3, stride_4, stride2_2, stride2_3, stride2_4;
+
+    __asm__ volatile (
+    "slli.d         %[stride_2],      %[i_stride],          1                      \n\t"
+    "slli.d         %[stride2_2],     %[i_stride2],         1                      \n\t"
+    "add.d          %[stride_3],      %[i_stride],          %[stride_2]            \n\t"
+    "add.d          %[stride2_3],     %[i_stride2],         %[stride2_2]           \n\t"
+    "slli.d         %[stride_4],      %[stride_2],          1                      \n\t"
+    "slli.d         %[stride2_4],     %[stride2_2],         1                      \n\t"
+    "vld            $vr0,             %[p_pix1],            0                      \n\t"
+    "vldx           $vr1,             %[p_pix1],            %[i_stride]            \n\t"
+    "vldx           $vr2,             %[p_pix1],            %[stride_2]            \n\t"
+    "vldx           $vr3,             %[p_pix1],            %[stride_3]            \n\t"
+    "add.d          %[p_pix1],        %[p_pix1],            %[stride_4]            \n\t"
+    "vld            $vr4,             %[p_pix1],            0                      \n\t"
+    "vldx           $vr5,             %[p_pix1],            %[i_stride]            \n\t"
+    "vldx           $vr6,             %[p_pix1],            %[stride_2]            \n\t"
+    "vldx           $vr7,             %[p_pix1],            %[stride_3]            \n\t"
+    "vld            $vr8,             %[p_pix2],            0                      \n\t"
+    "vldx           $vr9,             %[p_pix2],            %[i_stride2]           \n\t"
+    "vldx           $vr10,            %[p_pix2],            %[stride2_2]           \n\t"
+    "vldx           $vr11,            %[p_pix2],            %[stride2_3]           \n\t"
+    "add.d          %[p_pix2],        %[p_pix2],            %[stride2_4]           \n\t"
+    "vld            $vr12,            %[p_pix2],            0                      \n\t"
+    "vldx           $vr13,            %[p_pix2],            %[i_stride2]           \n\t"
+    "vldx           $vr14,            %[p_pix2],            %[stride2_2]           \n\t"
+    "vldx           $vr15,            %[p_pix2],            %[stride2_3]           \n\t"
+    "xvpermi.q      $xr0,             $xr4,                 2                      \n\t"
+    "xvpermi.q      $xr1,             $xr5,                 2                      \n\t"
+    "xvpermi.q      $xr2,             $xr6,                 2                      \n\t"
+    "xvpermi.q      $xr3,             $xr7,                 2                      \n\t"
+    "xvpermi.q      $xr8,             $xr12,                2                      \n\t"
+    "xvpermi.q      $xr9,             $xr13,                2                      \n\t"
+    "xvpermi.q      $xr10,            $xr14,                2                      \n\t"
+    "xvpermi.q      $xr11,            $xr15,                2                      \n\t"
+    "xvsubwev.h.bu  $xr4,             $xr0,                 $xr8                   \n\t"
+    "xvsubwod.h.bu  $xr5,             $xr0,                 $xr8                   \n\t"
+    "xvsubwev.h.bu  $xr6,             $xr1,                 $xr9                   \n\t"
+    "xvsubwod.h.bu  $xr7,             $xr1,                 $xr9                   \n\t"
+    "xvsubwev.h.bu  $xr8,             $xr2,                 $xr10                  \n\t"
+    "xvsubwod.h.bu  $xr9,             $xr2,                 $xr10                  \n\t"
+    "xvsubwev.h.bu  $xr12,            $xr3,                 $xr11                  \n\t"
+    "xvsubwod.h.bu  $xr13,            $xr3,                 $xr11                  \n\t"
+
+    "xvadd.h        $xr0,             $xr4,                 $xr5                   \n\t"
+    "xvsub.h        $xr1,             $xr4,                 $xr5                   \n\t"
+    "xvadd.h        $xr2,             $xr6,                 $xr7                   \n\t"
+    "xvsub.h        $xr3,             $xr6,                 $xr7                   \n\t"
+    "xvadd.h        $xr4,             $xr8,                 $xr9                   \n\t"
+    "xvsub.h        $xr5,             $xr8,                 $xr9                   \n\t"
+    "xvadd.h        $xr6,             $xr12,                $xr13                  \n\t"
+    "xvsub.h        $xr7,             $xr12,                $xr13                  \n\t"
+
+    "xvpackev.h     $xr8,             $xr5,                 $xr4                   \n\t"
+    "xvpackod.h     $xr9,             $xr5,                 $xr4                   \n\t"
+    "xvpackev.h     $xr10,            $xr7,                 $xr6                   \n\t"
+    "xvpackod.h     $xr11,            $xr7,                 $xr6                   \n\t"
+    "xvpackev.h     $xr4,             $xr1,                 $xr0                   \n\t"
+    "xvpackod.h     $xr5,             $xr1,                 $xr0                   \n\t"
+    "xvpackev.h     $xr6,             $xr3,                 $xr2                   \n\t"
+    "xvpackod.h     $xr7,             $xr3,                 $xr2                   \n\t"
+
+    "xvadd.h        $xr0,             $xr4,                 $xr5                   \n\t"
+    "xvsub.h        $xr1,             $xr4,                 $xr5                   \n\t"
+    "xvadd.h        $xr2,             $xr6,                 $xr7                   \n\t"
+    "xvsub.h        $xr3,             $xr6,                 $xr7                   \n\t"
+    "xvadd.h        $xr4,             $xr8,                 $xr9                   \n\t"
+    "xvsub.h        $xr5,             $xr8,                 $xr9                   \n\t"
+    "xvadd.h        $xr6,             $xr10,                $xr11                  \n\t"
+    "xvsub.h        $xr7,             $xr10,                $xr11                  \n\t"
+
+    "xvilvl.h       $xr8,             $xr1,                 $xr0                   \n\t"
+    "xvilvl.h       $xr9,             $xr3,                 $xr2                   \n\t"
+    "xvilvl.h       $xr10,            $xr5,                 $xr4                   \n\t"
+    "xvilvl.h       $xr11,            $xr7,                 $xr6                   \n\t"
+    "xvilvh.h       $xr0,             $xr1,                 $xr0                   \n\t"
+    "xvilvh.h       $xr1,             $xr3,                 $xr2                   \n\t"
+    "xvilvh.h       $xr2,             $xr5,                 $xr4                   \n\t"
+    "xvilvh.h       $xr3,             $xr7,                 $xr6                   \n\t"
+
+
+    "xvadd.h        $xr4,             $xr8,                 $xr9                   \n\t"
+    "xvadd.h        $xr6,             $xr10,                $xr11                  \n\t"
+    "xvsub.h        $xr5,             $xr8,                 $xr9                   \n\t"
+    "xvsub.h        $xr7,             $xr10,                $xr11                  \n\t"
+    "xvadd.h        $xr8,             $xr4,                 $xr6                   \n\t"
+    "xvadd.h        $xr9,             $xr5,                 $xr7                   \n\t"
+    "xvsub.h        $xr10,            $xr4,                 $xr6                   \n\t"
+    "xvsub.h        $xr11,            $xr5,                 $xr7                   \n\t"
+
+    "xvadd.h        $xr4,             $xr0,                 $xr1                   \n\t"
+    "xvadd.h        $xr6,             $xr2,                 $xr3                   \n\t"
+    "xvsub.h        $xr5,             $xr0,                 $xr1                   \n\t"
+    "xvsub.h        $xr7,             $xr2,                 $xr3                   \n\t"
+    "xvadd.h        $xr0,             $xr4,                 $xr6                   \n\t"
+    "xvadd.h        $xr1,             $xr5,                 $xr7                   \n\t"
+    "xvsub.h        $xr2,             $xr4,                 $xr6                   \n\t"
+    "xvsub.h        $xr3,             $xr5,                 $xr7                   \n\t"
+
+    "xvadda.h       $xr8,             $xr8,                 $xr9                   \n\t"
+    "xvadda.h       $xr9,             $xr10,                $xr11                  \n\t"
+    "xvadda.h       $xr0,             $xr0,                 $xr1                   \n\t"
+    "xvadda.h       $xr1,             $xr2,                 $xr3                   \n\t"
+
+    "xvadd.h        $xr8,             $xr8,                 $xr9                   \n\t"
+    "xvadd.h        $xr0,             $xr0,                 $xr1                   \n\t"
+    "xvadd.h        $xr16,            $xr0,                 $xr8                   \n\t"
+    "xvhaddw.wu.hu  $xr16,            $xr16,                $xr16                  \n\t"
+    "xvhaddw.du.wu  $xr16,            $xr16,                $xr16                  \n\t"
+    "xvhaddw.qu.du  $xr16,            $xr16,                $xr16                  \n\t"
+    "xvpickve2gr.wu %[sum1],          $xr16,                0                      \n\t"
+    "xvpickve2gr.wu %[sum2],          $xr16,                4                      \n\t"
+    "add.w          %[sum],           %[sum1],              %[sum2]                \n\t"
+    : [stride_2]"=&r"(stride_2), [stride_3]"=&r"(stride_3), [stride_4]"=&r"(stride_4),
+      [stride2_2]"=&r"(stride2_2), [stride2_3]"=&r"(stride2_3), [stride2_4]"=&r"(stride2_4),
+      [sum1]"=&r"(sum1), [sum2]"=&r"(sum2), [sum]"=&r"(sum), [p_pix1]"+&r"(p_pix1), [p_pix2]"+&r"(p_pix2)
+    : [i_stride]"r"(i_stride), [i_stride2]"r"(i_stride2)
+    : "memory"
+    );
+
+    return ( sum >> 1 );
+}
+
+int32_t x264_pixel_satd_16x16_lasx( uint8_t *p_pix1, intptr_t i_stride,
+                                    uint8_t *p_pix2, intptr_t i_stride2 )
+{
+    int32_t sum;
+    uint32_t sum1, sum2;
+    int64_t stride_2, stride_3, stride_4, stride2_2, stride2_3, stride2_4;
+
+    __asm__ volatile (
+    "slli.d         %[stride_2],      %[i_stride],          1                      \n\t"
+    "slli.d         %[stride2_2],     %[i_stride2],         1                      \n\t"
+    "add.d          %[stride_3],      %[i_stride],          %[stride_2]            \n\t"
+    "add.d          %[stride2_3],     %[i_stride2],         %[stride2_2]           \n\t"
+    "slli.d         %[stride_4],      %[stride_2],          1                      \n\t"
+    "slli.d         %[stride2_4],     %[stride2_2],         1                      \n\t"
+    "vld            $vr0,             %[p_pix1],            0                      \n\t"
+    "vldx           $vr1,             %[p_pix1],            %[i_stride]            \n\t"
+    "vldx           $vr2,             %[p_pix1],            %[stride_2]            \n\t"
+    "vldx           $vr3,             %[p_pix1],            %[stride_3]            \n\t"
+    "add.d          %[p_pix1],        %[p_pix1],            %[stride_4]            \n\t"
+    "vld            $vr4,             %[p_pix1],            0                      \n\t"
+    "vldx           $vr5,             %[p_pix1],            %[i_stride]            \n\t"
+    "vldx           $vr6,             %[p_pix1],            %[stride_2]            \n\t"
+    "vldx           $vr7,             %[p_pix1],            %[stride_3]            \n\t"
+    "add.d          %[p_pix1],        %[p_pix1],            %[stride_4]            \n\t"
+    "vld            $vr8,             %[p_pix2],            0                      \n\t"
+    "vldx           $vr9,             %[p_pix2],            %[i_stride2]           \n\t"
+    "vldx           $vr10,            %[p_pix2],            %[stride2_2]           \n\t"
+    "vldx           $vr11,            %[p_pix2],            %[stride2_3]           \n\t"
+    "add.d          %[p_pix2],        %[p_pix2],            %[stride2_4]           \n\t"
+    "vld            $vr12,            %[p_pix2],            0                      \n\t"
+    "vldx           $vr13,            %[p_pix2],            %[i_stride2]           \n\t"
+    "vldx           $vr14,            %[p_pix2],            %[stride2_2]           \n\t"
+    "vldx           $vr15,            %[p_pix2],            %[stride2_3]           \n\t"
+    "add.d          %[p_pix2],        %[p_pix2],            %[stride2_4]           \n\t"
+    "xvpermi.q      $xr0,             $xr4,                 2                      \n\t"
+    "xvpermi.q      $xr1,             $xr5,                 2                      \n\t"
+    "xvpermi.q      $xr2,             $xr6,                 2                      \n\t"
+    "xvpermi.q      $xr3,             $xr7,                 2                      \n\t"
+    "xvpermi.q      $xr8,             $xr12,                2                      \n\t"
+    "xvpermi.q      $xr9,             $xr13,                2                      \n\t"
+    "xvpermi.q      $xr10,            $xr14,                2                      \n\t"
+    "xvpermi.q      $xr11,            $xr15,                2                      \n\t"
+    "xvsubwev.h.bu  $xr4,             $xr0,                 $xr8                   \n\t"
+    "xvsubwod.h.bu  $xr5,             $xr0,                 $xr8                   \n\t"
+    "xvsubwev.h.bu  $xr6,             $xr1,                 $xr9                   \n\t"
+    "xvsubwod.h.bu  $xr7,             $xr1,                 $xr9                   \n\t"
+    "xvsubwev.h.bu  $xr8,             $xr2,                 $xr10                  \n\t"
+    "xvsubwod.h.bu  $xr9,             $xr2,                 $xr10                  \n\t"
+    "xvsubwev.h.bu  $xr12,            $xr3,                 $xr11                  \n\t"
+    "xvsubwod.h.bu  $xr13,            $xr3,                 $xr11                  \n\t"
+
+    "xvadd.h        $xr0,             $xr4,                 $xr5                   \n\t"
+    "xvsub.h        $xr1,             $xr4,                 $xr5                   \n\t"
+    "xvadd.h        $xr2,             $xr6,                 $xr7                   \n\t"
+    "xvsub.h        $xr3,             $xr6,                 $xr7                   \n\t"
+    "xvadd.h        $xr4,             $xr8,                 $xr9                   \n\t"
+    "xvsub.h        $xr5,             $xr8,                 $xr9                   \n\t"
+    "xvadd.h        $xr6,             $xr12,                $xr13                  \n\t"
+    "xvsub.h        $xr7,             $xr12,                $xr13                  \n\t"
+
+    "xvpackev.h     $xr8,             $xr5,                 $xr4                   \n\t"
+    "xvpackod.h     $xr9,             $xr5,                 $xr4                   \n\t"
+    "xvpackev.h     $xr10,            $xr7,                 $xr6                   \n\t"
+    "xvpackod.h     $xr11,            $xr7,                 $xr6                   \n\t"
+    "xvpackev.h     $xr4,             $xr1,                 $xr0                   \n\t"
+    "xvpackod.h     $xr5,             $xr1,                 $xr0                   \n\t"
+    "xvpackev.h     $xr6,             $xr3,                 $xr2                   \n\t"
+    "xvpackod.h     $xr7,             $xr3,                 $xr2                   \n\t"
+
+    "xvadd.h        $xr0,             $xr4,                 $xr5                   \n\t"
+    "xvsub.h        $xr1,             $xr4,                 $xr5                   \n\t"
+    "xvadd.h        $xr2,             $xr6,                 $xr7                   \n\t"
+    "xvsub.h        $xr3,             $xr6,                 $xr7                   \n\t"
+    "xvadd.h        $xr4,             $xr8,                 $xr9                   \n\t"
+    "xvsub.h        $xr5,             $xr8,                 $xr9                   \n\t"
+    "xvadd.h        $xr6,             $xr10,                $xr11                  \n\t"
+    "xvsub.h        $xr7,             $xr10,                $xr11                  \n\t"
+
+    "xvilvl.h       $xr8,             $xr1,                 $xr0                   \n\t"
+    "xvilvl.h       $xr9,             $xr3,                 $xr2                   \n\t"
+    "xvilvl.h       $xr10,            $xr5,                 $xr4                   \n\t"
+    "xvilvl.h       $xr11,            $xr7,                 $xr6                   \n\t"
+    "xvilvh.h       $xr0,             $xr1,                 $xr0                   \n\t"
+    "xvilvh.h       $xr1,             $xr3,                 $xr2                   \n\t"
+    "xvilvh.h       $xr2,             $xr5,                 $xr4                   \n\t"
+    "xvilvh.h       $xr3,             $xr7,                 $xr6                   \n\t"
+
+    "xvadd.h        $xr4,             $xr8,                 $xr9                   \n\t"
+    "xvadd.h        $xr6,             $xr10,                $xr11                  \n\t"
+    "xvsub.h        $xr5,             $xr8,                 $xr9                   \n\t"
+    "xvsub.h        $xr7,             $xr10,                $xr11                  \n\t"
+    "xvadd.h        $xr8,             $xr4,                 $xr6                   \n\t"
+    "xvadd.h        $xr9,             $xr5,                 $xr7                   \n\t"
+    "xvsub.h        $xr10,            $xr4,                 $xr6                   \n\t"
+    "xvsub.h        $xr11,            $xr5,                 $xr7                   \n\t"
+
+    "xvadd.h        $xr4,             $xr0,                 $xr1                   \n\t"
+    "xvadd.h        $xr6,             $xr2,                 $xr3                   \n\t"
+    "xvsub.h        $xr5,             $xr0,                 $xr1                   \n\t"
+    "xvsub.h        $xr7,             $xr2,                 $xr3                   \n\t"
+    "xvadd.h        $xr0,             $xr4,                 $xr6                   \n\t"
+    "xvadd.h        $xr1,             $xr5,                 $xr7                   \n\t"
+    "xvsub.h        $xr2,             $xr4,                 $xr6                   \n\t"
+    "xvsub.h        $xr3,             $xr5,                 $xr7                   \n\t"
+
+    "xvadda.h       $xr8,             $xr8,                 $xr9                   \n\t"
+    "xvadda.h       $xr9,             $xr10,                $xr11                  \n\t"
+    "xvadda.h       $xr0,             $xr0,                 $xr1                   \n\t"
+    "xvadda.h       $xr1,             $xr2,                 $xr3                   \n\t"
+
+    "xvadd.h        $xr8,             $xr8,                 $xr9                   \n\t"
+    "xvadd.h        $xr0,             $xr0,                 $xr1                   \n\t"
+    "xvadd.h        $xr16,            $xr0,                 $xr8                   \n\t"
+
+    "vld            $vr0,             %[p_pix1],            0                      \n\t"
+    "vldx           $vr1,             %[p_pix1],            %[i_stride]            \n\t"
+    "vldx           $vr2,             %[p_pix1],            %[stride_2]            \n\t"
+    "vldx           $vr3,             %[p_pix1],            %[stride_3]            \n\t"
+    "add.d          %[p_pix1],        %[p_pix1],            %[stride_4]            \n\t"
+    "vld            $vr4,             %[p_pix1],            0                      \n\t"
+    "vldx           $vr5,             %[p_pix1],            %[i_stride]            \n\t"
+    "vldx           $vr6,             %[p_pix1],            %[stride_2]            \n\t"
+    "vldx           $vr7,             %[p_pix1],            %[stride_3]            \n\t"
+    "vld            $vr8,             %[p_pix2],            0                      \n\t"
+    "vldx           $vr9,             %[p_pix2],            %[i_stride2]           \n\t"
+    "vldx           $vr10,            %[p_pix2],            %[stride2_2]           \n\t"
+    "vldx           $vr11,            %[p_pix2],            %[stride2_3]           \n\t"
+    "add.d          %[p_pix2],        %[p_pix2],            %[stride2_4]           \n\t"
+    "vld            $vr12,            %[p_pix2],            0                      \n\t"
+    "vldx           $vr13,            %[p_pix2],            %[i_stride2]           \n\t"
+    "vldx           $vr14,            %[p_pix2],            %[stride2_2]           \n\t"
+    "vldx           $vr15,            %[p_pix2],            %[stride2_3]           \n\t"
+    "xvpermi.q      $xr0,             $xr4,                 2                      \n\t"
+    "xvpermi.q      $xr1,             $xr5,                 2                      \n\t"
+    "xvpermi.q      $xr2,             $xr6,                 2                      \n\t"
+    "xvpermi.q      $xr3,             $xr7,                 2                      \n\t"
+    "xvpermi.q      $xr8,             $xr12,                2                      \n\t"
+    "xvpermi.q      $xr9,             $xr13,                2                      \n\t"
+    "xvpermi.q      $xr10,            $xr14,                2                      \n\t"
+    "xvpermi.q      $xr11,            $xr15,                2                      \n\t"
+    "xvsubwev.h.bu  $xr4,             $xr0,                 $xr8                   \n\t"
+    "xvsubwod.h.bu  $xr5,             $xr0,                 $xr8                   \n\t"
+    "xvsubwev.h.bu  $xr6,             $xr1,                 $xr9                   \n\t"
+    "xvsubwod.h.bu  $xr7,             $xr1,                 $xr9                   \n\t"
+    "xvsubwev.h.bu  $xr8,             $xr2,                 $xr10                  \n\t"
+    "xvsubwod.h.bu  $xr9,             $xr2,                 $xr10                  \n\t"
+    "xvsubwev.h.bu  $xr12,            $xr3,                 $xr11                  \n\t"
+    "xvsubwod.h.bu  $xr13,            $xr3,                 $xr11                  \n\t"
+
+    "xvadd.h        $xr0,             $xr4,                 $xr5                   \n\t"
+    "xvsub.h        $xr1,             $xr4,                 $xr5                   \n\t"
+    "xvadd.h        $xr2,             $xr6,                 $xr7                   \n\t"
+    "xvsub.h        $xr3,             $xr6,                 $xr7                   \n\t"
+    "xvadd.h        $xr4,             $xr8,                 $xr9                   \n\t"
+    "xvsub.h        $xr5,             $xr8,                 $xr9                   \n\t"
+    "xvadd.h        $xr6,             $xr12,                $xr13                  \n\t"
+    "xvsub.h        $xr7,             $xr12,                $xr13                  \n\t"
+
+    "xvpackev.h     $xr8,             $xr5,                 $xr4                   \n\t"
+    "xvpackod.h     $xr9,             $xr5,                 $xr4                   \n\t"
+    "xvpackev.h     $xr10,            $xr7,                 $xr6                   \n\t"
+    "xvpackod.h     $xr11,            $xr7,                 $xr6                   \n\t"
+    "xvpackev.h     $xr4,             $xr1,                 $xr0                   \n\t"
+    "xvpackod.h     $xr5,             $xr1,                 $xr0                   \n\t"
+    "xvpackev.h     $xr6,             $xr3,                 $xr2                   \n\t"
+    "xvpackod.h     $xr7,             $xr3,                 $xr2                   \n\t"
+
+    "xvadd.h        $xr0,             $xr4,                 $xr5                   \n\t"
+    "xvsub.h        $xr1,             $xr4,                 $xr5                   \n\t"
+    "xvadd.h        $xr2,             $xr6,                 $xr7                   \n\t"
+    "xvsub.h        $xr3,             $xr6,                 $xr7                   \n\t"
+    "xvadd.h        $xr4,             $xr8,                 $xr9                   \n\t"
+    "xvsub.h        $xr5,             $xr8,                 $xr9                   \n\t"
+    "xvadd.h        $xr6,             $xr10,                $xr11                  \n\t"
+    "xvsub.h        $xr7,             $xr10,                $xr11                  \n\t"
+
+    "xvilvl.h       $xr8,             $xr1,                 $xr0                   \n\t"
+    "xvilvl.h       $xr9,             $xr3,                 $xr2                   \n\t"
+    "xvilvl.h       $xr10,            $xr5,                 $xr4                   \n\t"
+    "xvilvl.h       $xr11,            $xr7,                 $xr6                   \n\t"
+    "xvilvh.h       $xr0,             $xr1,                 $xr0                   \n\t"
+    "xvilvh.h       $xr1,             $xr3,                 $xr2                   \n\t"
+    "xvilvh.h       $xr2,             $xr5,                 $xr4                   \n\t"
+    "xvilvh.h       $xr3,             $xr7,                 $xr6                   \n\t"
+
+    "xvadd.h        $xr4,             $xr8,                 $xr9                   \n\t"
+    "xvadd.h        $xr6,             $xr10,                $xr11                  \n\t"
+    "xvsub.h        $xr5,             $xr8,                 $xr9                   \n\t"
+    "xvsub.h        $xr7,             $xr10,                $xr11                  \n\t"
+    "xvadd.h        $xr8,             $xr4,                 $xr6                   \n\t"
+    "xvadd.h        $xr9,             $xr5,                 $xr7                   \n\t"
+    "xvsub.h        $xr10,            $xr4,                 $xr6                   \n\t"
+    "xvsub.h        $xr11,            $xr5,                 $xr7                   \n\t"
+
+    "xvadd.h        $xr4,             $xr0,                 $xr1                   \n\t"
+    "xvadd.h        $xr6,             $xr2,                 $xr3                   \n\t"
+    "xvsub.h        $xr5,             $xr0,                 $xr1                   \n\t"
+    "xvsub.h        $xr7,             $xr2,                 $xr3                   \n\t"
+    "xvadd.h        $xr0,             $xr4,                 $xr6                   \n\t"
+    "xvadd.h        $xr1,             $xr5,                 $xr7                   \n\t"
+    "xvsub.h        $xr2,             $xr4,                 $xr6                   \n\t"
+    "xvsub.h        $xr3,             $xr5,                 $xr7                   \n\t"
+
+    "xvadda.h       $xr8,             $xr8,                 $xr9                   \n\t"
+    "xvadda.h       $xr9,             $xr10,                $xr11                  \n\t"
+    "xvadda.h       $xr0,             $xr0,                 $xr1                   \n\t"
+    "xvadda.h       $xr1,             $xr2,                 $xr3                   \n\t"
+
+    "xvadd.h        $xr8,             $xr8,                 $xr9                   \n\t"
+    "xvadd.h        $xr0,             $xr0,                 $xr1                   \n\t"
+    "xvadd.h        $xr16,            $xr16,                $xr8                   \n\t"
+    "xvadd.h        $xr16,            $xr16,                $xr0                   \n\t"
+
+    "xvhaddw.wu.hu  $xr16,            $xr16,                $xr16                  \n\t"
+    "xvhaddw.du.wu  $xr16,            $xr16,                $xr16                  \n\t"
+    "xvhaddw.qu.du  $xr16,            $xr16,                $xr16                  \n\t"
+    "xvpickve2gr.wu %[sum1],          $xr16,                0                      \n\t"
+    "xvpickve2gr.wu %[sum2],          $xr16,                4                      \n\t"
+    "add.w          %[sum],           %[sum1],              %[sum2]                \n\t"
+    : [stride_2]"=&r"(stride_2), [stride_3]"=&r"(stride_3), [stride_4]"=&r"(stride_4),
+      [stride2_2]"=&r"(stride2_2), [stride2_3]"=&r"(stride2_3), [stride2_4]"=&r"(stride2_4),
+      [sum1]"=&r"(sum1), [sum2]"=&r"(sum2), [sum]"=&r"(sum), [p_pix1]"+&r"(p_pix1), [p_pix2]"+&r"(p_pix2)
+    : [i_stride]"r"(i_stride), [i_stride2]"r"(i_stride2)
+    : "memory"
+    );
+
+    return ( sum >> 1 );
+}
+
+#define SAD_LOAD                                                        \
+    __m256i src0, src1, src2, src3, src4, src5, src6, src7;             \
+    __m256i ref0, ref1, ref2, ref3, ref4, ref5, ref6, ref7;             \
+    __m256i diff;                                                       \
+    __m256i sad0 = __lasx_xvldi( 0 );                                   \
+    __m256i sad1 = __lasx_xvldi( 0 );                                   \
+    __m256i sad2 = __lasx_xvldi( 0 );                                   \
+    __m256i sad3 = __lasx_xvldi( 0 );                                   \
+    int32_t i_src_stride_x2 = FENC_STRIDE << 1;                         \
+    int32_t i_ref_stride_x2 = i_ref_stride << 1;                        \
+    int32_t i_src_stride_x3 = i_src_stride_x2 + FENC_STRIDE;            \
+    int32_t i_ref_stride_x3 = i_ref_stride_x2 + i_ref_stride;           \
+    int32_t i_src_stride_x4 = i_src_stride_x2 << 1;                     \
+    int32_t i_ref_stride_x4 = i_ref_stride_x2 << 1;                     \
+
+#define LOAD_REF_DATA_16W( p_ref, sad)                                        \
+    LASX_LOAD_4( p_ref, i_ref_stride, i_ref_stride_x2, i_ref_stride_x3,       \
+                 ref0, ref1, ref2, ref3 );                                    \
+    p_ref += i_ref_stride_x4;                                                 \
+    LASX_LOAD_4( p_ref, i_ref_stride, i_ref_stride_x2, i_ref_stride_x3,       \
+                 ref4, ref5, ref6, ref7 );                                    \
+    ref0 = __lasx_xvpermi_q( ref0, ref1, 0x20 );                              \
+    ref1 = __lasx_xvpermi_q( ref2, ref3, 0x20 );                              \
+    ref2 = __lasx_xvpermi_q( ref4, ref5, 0x20 );                              \
+    ref3 = __lasx_xvpermi_q( ref6, ref7, 0x20 );                              \
+    diff = __lasx_xvabsd_bu( src0, ref0 );                                    \
+    diff = __lasx_xvhaddw_hu_bu( diff, diff );                                \
+    sad  = __lasx_xvadd_h(sad, diff);                                         \
+    diff = __lasx_xvabsd_bu( src1, ref1 );                                    \
+    diff = __lasx_xvhaddw_hu_bu( diff, diff );                                \
+    sad  = __lasx_xvadd_h(sad, diff);                                         \
+    diff = __lasx_xvabsd_bu( src2, ref2 );                                    \
+    diff = __lasx_xvhaddw_hu_bu( diff, diff );                                \
+    sad  = __lasx_xvadd_h(sad, diff);                                         \
+    diff = __lasx_xvabsd_bu( src3, ref3 );                                    \
+    diff = __lasx_xvhaddw_hu_bu( diff, diff );                                \
+    sad  = __lasx_xvadd_h(sad, diff);                                         \
+
+#define ST_REF_DATA(sad)                                  \
+    sad = __lasx_xvhaddw_wu_hu(sad, sad);                 \
+    sad = __lasx_xvhaddw_du_wu(sad, sad);                 \
+    sad = __lasx_xvhaddw_qu_du(sad, sad);                 \
+
+void x264_pixel_sad_x4_16x8_lasx( uint8_t *p_src, uint8_t *p_ref0,
+                                  uint8_t *p_ref1, uint8_t *p_ref2,
+                                  uint8_t *p_ref3, intptr_t i_ref_stride,
+                                  int32_t p_sad_array[4] )
+{
+    SAD_LOAD
+
+    src0 = __lasx_xvld(p_src, 0);
+    src1 = __lasx_xvld(p_src, FENC_STRIDE);
+    src2 = __lasx_xvldx(p_src, i_src_stride_x2);
+    src3 = __lasx_xvldx(p_src, i_src_stride_x3);
+    p_src += i_src_stride_x4;
+    src4 = __lasx_xvld(p_src, 0);
+    src5 = __lasx_xvld(p_src, FENC_STRIDE);
+    src6 = __lasx_xvldx(p_src, i_src_stride_x2);
+    src7 = __lasx_xvldx(p_src, i_src_stride_x3);
+
+    src0 = __lasx_xvpermi_q( src0, src1, 0x20 );
+    src1 = __lasx_xvpermi_q( src2, src3, 0x20 );
+    src2 = __lasx_xvpermi_q( src4, src5, 0x20 );
+    src3 = __lasx_xvpermi_q( src6, src7, 0x20 );
+
+    LOAD_REF_DATA_16W( p_ref0, sad0 );
+    LOAD_REF_DATA_16W( p_ref1, sad1 );
+    LOAD_REF_DATA_16W( p_ref2, sad2 );
+    LOAD_REF_DATA_16W( p_ref3, sad3 );
+
+    ST_REF_DATA(sad0);
+    p_sad_array[0] = __lasx_xvpickve2gr_wu(sad0, 0) + __lasx_xvpickve2gr_wu(sad0, 4);
+    ST_REF_DATA(sad1);
+    p_sad_array[1] = __lasx_xvpickve2gr_wu(sad1, 0) + __lasx_xvpickve2gr_wu(sad1, 4);
+    ST_REF_DATA(sad2);
+    p_sad_array[2] = __lasx_xvpickve2gr_wu(sad2, 0) + __lasx_xvpickve2gr_wu(sad2, 4);
+    ST_REF_DATA(sad3);
+    p_sad_array[3] = __lasx_xvpickve2gr_wu(sad3, 0) + __lasx_xvpickve2gr_wu(sad3, 4);
+}
+
+#undef LOAD_REF_DATA_16W
+
+#define LOAD_REF_DATA_8W( p_ref, sad)                                             \
+    LASX_LOAD_4( p_ref, i_ref_stride, i_ref_stride_x2, i_ref_stride_x3,           \
+                 ref0, ref1, ref2, ref3 );                                        \
+    p_ref += i_ref_stride_x4;                                                     \
+    LASX_LOAD_4( p_ref, i_ref_stride, i_ref_stride_x2, i_ref_stride_x3,           \
+                 ref4, ref5, ref6, ref7 );                                        \
+    ref0 = __lasx_xvilvl_d( ref1, ref0 );                                         \
+    ref1 = __lasx_xvilvl_d( ref3, ref2 );                                         \
+    ref2 = __lasx_xvilvl_d( ref5, ref4 );                                         \
+    ref3 = __lasx_xvilvl_d( ref7, ref6 );                                         \
+    ref0 = __lasx_xvpermi_q( ref0, ref1, 0x20 );                                  \
+    ref1 = __lasx_xvpermi_q( ref2, ref3, 0x20 );                                  \
+    diff = __lasx_xvabsd_bu( src0, ref0 );                                        \
+    diff = __lasx_xvhaddw_hu_bu( diff, diff );                                    \
+    sad  = __lasx_xvadd_h( sad, diff );                                           \
+    diff = __lasx_xvabsd_bu( src1, ref1 );                                        \
+    diff = __lasx_xvhaddw_hu_bu( diff, diff );                                    \
+    sad  = __lasx_xvadd_h( sad, diff );
+
+void x264_pixel_sad_x4_8x16_lasx( uint8_t *p_src, uint8_t *p_ref0,
+                                  uint8_t *p_ref1, uint8_t *p_ref2,
+                                  uint8_t *p_ref3, intptr_t i_ref_stride,
+                                  int32_t p_sad_array[4] )
+{
+    SAD_LOAD
+
+    src0 = __lasx_xvld(p_src, 0);
+    src1 = __lasx_xvld(p_src, FENC_STRIDE);
+    src2 = __lasx_xvldx(p_src, i_src_stride_x2);
+    src3 = __lasx_xvldx(p_src, i_src_stride_x3);
+    p_src += i_src_stride_x4;
+    src4 = __lasx_xvld(p_src, 0);
+    src5 = __lasx_xvld(p_src, FENC_STRIDE);
+    src6 = __lasx_xvldx(p_src, i_src_stride_x2);
+    src7 = __lasx_xvldx(p_src, i_src_stride_x3);
+    p_src += i_src_stride_x4;
+
+    src0 = __lasx_xvilvl_d( src1, src0 );
+    src1 = __lasx_xvilvl_d( src3, src2 );
+    src2 = __lasx_xvilvl_d( src5, src4 );
+    src3 = __lasx_xvilvl_d( src7, src6 );
+    src0 = __lasx_xvpermi_q( src0, src1, 0x20 );
+    src1 = __lasx_xvpermi_q( src2, src3, 0x20 );
+
+    LOAD_REF_DATA_8W( p_ref0, sad0 );
+    LOAD_REF_DATA_8W( p_ref1, sad1 );
+    LOAD_REF_DATA_8W( p_ref2, sad2 );
+    LOAD_REF_DATA_8W( p_ref3, sad3 );
+
+    p_ref0 += i_ref_stride_x4;
+    p_ref1 += i_ref_stride_x4;
+    p_ref2 += i_ref_stride_x4;
+    p_ref3 += i_ref_stride_x4;
+
+    src0 = __lasx_xvld(p_src, 0);
+    src1 = __lasx_xvld(p_src, FENC_STRIDE);
+    src2 = __lasx_xvldx(p_src, i_src_stride_x2);
+    src3 = __lasx_xvldx(p_src, i_src_stride_x3);
+    p_src += i_src_stride_x4;
+    src4 = __lasx_xvld(p_src, 0);
+    src5 = __lasx_xvld(p_src, FENC_STRIDE);
+    src6 = __lasx_xvldx(p_src, i_src_stride_x2);
+    src7 = __lasx_xvldx(p_src, i_src_stride_x3);
+
+    src0 = __lasx_xvilvl_d( src1, src0 );
+    src1 = __lasx_xvilvl_d( src3, src2 );
+    src2 = __lasx_xvilvl_d( src5, src4 );
+    src3 = __lasx_xvilvl_d( src7, src6 );
+    src0 = __lasx_xvpermi_q( src0, src1, 0x20 );
+    src1 = __lasx_xvpermi_q( src2, src3, 0x20 );
+
+    LOAD_REF_DATA_8W( p_ref0, sad0 );
+    LOAD_REF_DATA_8W( p_ref1, sad1 );
+    LOAD_REF_DATA_8W( p_ref2, sad2 );
+    LOAD_REF_DATA_8W( p_ref3, sad3 );
+
+    ST_REF_DATA(sad0);
+    p_sad_array[0] = __lasx_xvpickve2gr_wu(sad0, 0) + __lasx_xvpickve2gr_wu(sad0, 4);
+    ST_REF_DATA(sad1);
+    p_sad_array[1] = __lasx_xvpickve2gr_wu(sad1, 0) + __lasx_xvpickve2gr_wu(sad1, 4);
+    ST_REF_DATA(sad2);
+    p_sad_array[2] = __lasx_xvpickve2gr_wu(sad2, 0) + __lasx_xvpickve2gr_wu(sad2, 4);
+    ST_REF_DATA(sad3);
+    p_sad_array[3] = __lasx_xvpickve2gr_wu(sad3, 0) + __lasx_xvpickve2gr_wu(sad3, 4);
+}
+
+#undef SAD_LOAD
+#undef LOAD_REF_DATA_8W
+#undef ST_REF_DATA
+
+void x264_pixel_sad_x4_8x4_lasx( uint8_t *p_src, uint8_t *p_ref0,
+                                 uint8_t *p_ref1, uint8_t *p_ref2,
+                                 uint8_t *p_ref3, intptr_t i_ref_stride,
+                                 int32_t p_sad_array[4] )
+{
+    __m256i src0, src1, src2, src3;
+    __m256i ref0, ref1, ref2, ref3;
+    __m256i diff;
+    __m256i sad0, sad1, sad2, sad3;
+    intptr_t i_src_stride_x2 = FENC_STRIDE << 1;
+    intptr_t i_ref_stride_x2 = i_ref_stride << 1;
+    intptr_t i_src_stride_x3 = FENC_STRIDE + i_src_stride_x2;
+    intptr_t i_ref_stride_x3 = i_ref_stride + i_ref_stride_x2;
+
+    src0 = __lasx_xvld(p_src, 0);
+    src1 = __lasx_xvld(p_src, FENC_STRIDE);
+    src2 = __lasx_xvldx(p_src, i_src_stride_x2);
+    src3 = __lasx_xvldx(p_src, i_src_stride_x3);
+    src0 = __lasx_xvilvl_d( src1, src0 );
+    src1 = __lasx_xvilvl_d( src3, src2 );
+    src0 = __lasx_xvpermi_q( src0, src1, 0x20 );
+
+#define LOAD_REF_DATA_8W_4H( p_ref, sad)                                \
+    LASX_LOAD_4( p_ref, i_ref_stride, i_ref_stride_x2, i_ref_stride_x3, \
+                 ref0, ref1, ref2, ref3 );                              \
+    ref0 = __lasx_xvilvl_d( ref1, ref0 );                               \
+    ref1 = __lasx_xvilvl_d( ref3, ref2 );                               \
+    ref0 = __lasx_xvpermi_q( ref0, ref1, 0x20 );                        \
+    diff = __lasx_xvabsd_bu( src0, ref0 );                              \
+    sad = __lasx_xvhaddw_hu_bu( diff, diff );                           \
+    sad = __lasx_xvhaddw_wu_hu( sad, sad );                             \
+    sad = __lasx_xvhaddw_du_wu( sad, sad );                             \
+    sad = __lasx_xvhaddw_qu_du( sad, sad );                             \
+
+    LOAD_REF_DATA_8W_4H( p_ref0, sad0 );
+    LOAD_REF_DATA_8W_4H( p_ref1, sad1 );
+    LOAD_REF_DATA_8W_4H( p_ref2, sad2 );
+    LOAD_REF_DATA_8W_4H( p_ref3, sad3 );
+
+#undef LOAD_REF_DATA_8W_4H
+
+    p_sad_array[0] = __lasx_xvpickve2gr_wu(sad0, 0) + __lasx_xvpickve2gr_wu(sad0, 4);
+    p_sad_array[1] = __lasx_xvpickve2gr_wu(sad1, 0) + __lasx_xvpickve2gr_wu(sad1, 4);
+    p_sad_array[2] = __lasx_xvpickve2gr_wu(sad2, 0) + __lasx_xvpickve2gr_wu(sad2, 4);
+    p_sad_array[3] = __lasx_xvpickve2gr_wu(sad3, 0) + __lasx_xvpickve2gr_wu(sad3, 4);
+}
+
+void x264_pixel_sad_x4_4x8_lasx( uint8_t *p_src, uint8_t *p_ref0,
+                                 uint8_t *p_ref1, uint8_t *p_ref2,
+                                 uint8_t *p_ref3, intptr_t i_ref_stride,
+                                 int32_t p_sad_array[4] )
+{
+    __m256i src0, src1, src2, src3, src4, src5, src6, src7;
+    __m256i ref0, ref1, ref2, ref3, ref4, ref5, ref6, ref7;
+    __m256i diff;
+    __m256i sad0, sad1, sad2, sad3;
+    intptr_t i_src_stride_x2 = FENC_STRIDE << 1;
+    intptr_t i_ref_stride_x2 = i_ref_stride << 1;
+    intptr_t i_src_stride_x3 = FENC_STRIDE + i_src_stride_x2;
+    intptr_t i_ref_stride_x3 = i_ref_stride + i_ref_stride_x2;
+    intptr_t i_src_stride_x4 = i_src_stride_x2 << 1;
+    intptr_t i_ref_stride_x4 = i_ref_stride_x2 << 1;
+
+    src0 = __lasx_xvld( p_src, 0);
+    src1 = __lasx_xvld( p_src, FENC_STRIDE);
+    src2 = __lasx_xvldx( p_src, i_src_stride_x2);
+    src3 = __lasx_xvldx( p_src, i_src_stride_x3);
+    p_src += i_src_stride_x4;
+    src4 = __lasx_xvld( p_src, 0);
+    src5 = __lasx_xvld( p_src, FENC_STRIDE);
+    src6 = __lasx_xvldx( p_src, i_src_stride_x2);
+    src7 = __lasx_xvldx( p_src, i_src_stride_x3);
+    src0 = __lasx_xvilvl_w( src1, src0 );
+    src1 = __lasx_xvilvl_w( src3, src2 );
+    src2 = __lasx_xvilvl_w( src5, src4 );
+    src3 = __lasx_xvilvl_w( src7, src6 );
+    src0 = __lasx_xvilvl_d( src1, src0 );
+    src1 = __lasx_xvilvl_d( src3, src2 );
+    src0 = __lasx_xvpermi_q( src0, src1, 0x20 );
+
+#define LOAD_REF_DATA_4W_8H( p_ref, sad) \
+    LASX_LOAD_4( p_ref, i_ref_stride, i_ref_stride_x2, i_ref_stride_x3,       \
+                 ref0, ref1, ref2, ref3 );                                    \
+    p_ref += i_ref_stride_x4;                                                 \
+    LASX_LOAD_4( p_ref, i_ref_stride, i_ref_stride_x2, i_ref_stride_x3,       \
+                 ref4, ref5, ref6, ref7 );                                    \
+    ref0 = __lasx_xvilvl_w( ref1, ref0 );                                     \
+    ref1 = __lasx_xvilvl_w( ref3, ref2 );                                     \
+    ref2 = __lasx_xvilvl_w( ref5, ref4 );                                     \
+    ref3 = __lasx_xvilvl_w( ref7, ref6 );                                     \
+    ref0 = __lasx_xvilvl_d( ref1, ref0 );                                     \
+    ref1 = __lasx_xvilvl_d( ref3, ref2 );                                     \
+    ref0 = __lasx_xvpermi_q( ref0, ref1, 0x20 );                              \
+    diff = __lasx_xvabsd_bu( src0, ref0 );                                    \
+    sad = __lasx_xvhaddw_hu_bu( diff, diff );                                 \
+    sad = __lasx_xvhaddw_wu_hu( sad, sad );                                   \
+    sad = __lasx_xvhaddw_du_wu( sad, sad );                                   \
+    sad = __lasx_xvhaddw_qu_du( sad, sad );                                   \
+
+    LOAD_REF_DATA_4W_8H( p_ref0, sad0 );
+    LOAD_REF_DATA_4W_8H( p_ref1, sad1 );
+    LOAD_REF_DATA_4W_8H( p_ref2, sad2 );
+    LOAD_REF_DATA_4W_8H( p_ref3, sad3 );
+
+#undef LOAD_REF_DATA_4W_8H
+
+    p_sad_array[0] = __lasx_xvpickve2gr_wu(sad0, 0) + __lasx_xvpickve2gr_wu(sad0, 4);
+    p_sad_array[1] = __lasx_xvpickve2gr_wu(sad1, 0) + __lasx_xvpickve2gr_wu(sad1, 4);
+    p_sad_array[2] = __lasx_xvpickve2gr_wu(sad2, 0) + __lasx_xvpickve2gr_wu(sad2, 4);
+    p_sad_array[3] = __lasx_xvpickve2gr_wu(sad3, 0) + __lasx_xvpickve2gr_wu(sad3, 4);
+}
+
+void x264_pixel_sad_x4_4x4_lasx( uint8_t *p_src, uint8_t *p_ref0,
+                                 uint8_t *p_ref1, uint8_t *p_ref2,
+                                 uint8_t *p_ref3, intptr_t i_ref_stride,
+                                 int32_t p_sad_array[4] )
+{
+    __m256i src0, src1, src2, src3;
+    __m256i ref0, ref1, ref2, ref3, ref4, ref5, ref6, ref7;
+    __m256i diff;
+    intptr_t i_src_stride_x2 = FENC_STRIDE << 1;
+    intptr_t i_ref_stride_x2 = i_ref_stride << 1;
+    intptr_t i_src_stride_x3 = FENC_STRIDE + i_src_stride_x2;
+    intptr_t i_ref_stride_x3 = i_ref_stride + i_ref_stride_x2;
+
+    src0 = __lasx_xvld( p_src, 0 );
+    src1 = __lasx_xvld( p_src, FENC_STRIDE );
+    src2 = __lasx_xvldx( p_src, i_src_stride_x2 );
+    src3 = __lasx_xvldx( p_src, i_src_stride_x3 );
+    src0 = __lasx_xvilvl_w( src1, src0 );
+    src1 = __lasx_xvilvl_w( src3, src2 );
+    src0 = __lasx_xvilvl_d( src1, src0 );
+    src0 = __lasx_xvpermi_q(src0, src0, 0x00);
+
+#define LOAD_REF_DATA_4W_4H( p0, p1 )                                    \
+    LASX_LOAD_4( p0, i_ref_stride, i_ref_stride_x2, i_ref_stride_x3,     \
+                 ref0, ref1, ref2, ref3 );                               \
+    LASX_LOAD_4( p1, i_ref_stride, i_ref_stride_x2, i_ref_stride_x3,     \
+                 ref4, ref5, ref6, ref7 );                               \
+    ref0 = __lasx_xvilvl_w( ref1, ref0 );                                \
+    ref1 = __lasx_xvilvl_w( ref3, ref2 );                                \
+    ref0 = __lasx_xvilvl_d( ref1, ref0 );                                \
+    ref2 = __lasx_xvilvl_w( ref5, ref4 );                                \
+    ref3 = __lasx_xvilvl_w( ref7, ref6 );                                \
+    ref1 = __lasx_xvilvl_d( ref3, ref2 );                                \
+    ref0 = __lasx_xvpermi_q(ref0, ref1, 0x02);                           \
+    diff = __lasx_xvabsd_bu( src0, ref0 );                               \
+    diff = __lasx_xvhaddw_hu_bu( diff, diff );                           \
+    diff = __lasx_xvhaddw_wu_hu( diff, diff );                           \
+    diff = __lasx_xvhaddw_du_wu( diff, diff );                           \
+    diff = __lasx_xvhaddw_qu_du( diff, diff );                           \
+
+    LOAD_REF_DATA_4W_4H( p_ref0, p_ref1 );
+    p_sad_array[0] = __lasx_xvpickve2gr_wu(diff, 0);
+    p_sad_array[1] = __lasx_xvpickve2gr_wu(diff, 4);
+    LOAD_REF_DATA_4W_4H( p_ref2, p_ref3 );
+    p_sad_array[2] = __lasx_xvpickve2gr_wu(diff, 0);
+    p_sad_array[3] = __lasx_xvpickve2gr_wu(diff, 4);
+
+#undef LOAD_REF_DATA_4W_4H
+
+}
+
+#define SAD_LOAD                                                              \
+    __m256i src0, src1, src2, src3, src4, src5, src6, src7;                   \
+    __m256i ref0, ref1, ref2, ref3, ref4, ref5, ref6, ref7;                   \
+    __m256i diff;                                                             \
+    __m256i sad0 = __lasx_xvldi(0);                                           \
+    __m256i sad1 = __lasx_xvldi(0);                                           \
+    __m256i sad2 = __lasx_xvldi(0);                                           \
+    int32_t i_src_stride_x2 = FENC_STRIDE << 1;                               \
+    int32_t i_ref_stride_x2 = i_ref_stride << 1;                              \
+    int32_t i_src_stride_x3 = FENC_STRIDE + i_src_stride_x2;                  \
+    int32_t i_ref_stride_x3 = i_ref_stride + i_ref_stride_x2;                 \
+    int32_t i_src_stride_x4 = i_src_stride_x2 << 1;                           \
+    int32_t i_ref_stride_x4 = i_ref_stride_x2 << 1;
+
+
+#define LOAD_REF_DATA_16W( p_ref, sad)                                        \
+    LASX_LOAD_4( p_ref, i_ref_stride, i_ref_stride_x2, i_ref_stride_x3,       \
+                 ref0, ref1, ref2, ref3 );                                    \
+    p_ref += i_ref_stride_x4;                                                 \
+    LASX_LOAD_4( p_ref, i_ref_stride, i_ref_stride_x2, i_ref_stride_x3,       \
+                 ref4, ref5, ref6, ref7 );                                    \
+    ref0 = __lasx_xvpermi_q( ref0, ref1, 0x20 );                              \
+    ref1 = __lasx_xvpermi_q( ref2, ref3, 0x20 );                              \
+    ref2 = __lasx_xvpermi_q( ref4, ref5, 0x20 );                              \
+    ref3 = __lasx_xvpermi_q( ref6, ref7, 0x20 );                              \
+    diff = __lasx_xvabsd_bu( src0, ref0 );                                    \
+    diff = __lasx_xvhaddw_hu_bu( diff, diff );                                \
+    sad  = __lasx_xvadd_h(sad, diff);                                         \
+    diff = __lasx_xvabsd_bu( src1, ref1 );                                    \
+    diff = __lasx_xvhaddw_hu_bu( diff, diff );                                \
+    sad  = __lasx_xvadd_h(sad, diff);                                         \
+    diff = __lasx_xvabsd_bu( src2, ref2 );                                    \
+    diff = __lasx_xvhaddw_hu_bu( diff, diff );                                \
+    sad  = __lasx_xvadd_h(sad, diff);                                         \
+    diff = __lasx_xvabsd_bu( src3, ref3 );                                    \
+    diff = __lasx_xvhaddw_hu_bu( diff, diff );                                \
+    sad  = __lasx_xvadd_h(sad, diff);                                         \
+
+
+#define ST_REF_DATA(sad)                                  \
+    sad = __lasx_xvhaddw_wu_hu(sad, sad);                 \
+    sad = __lasx_xvhaddw_du_wu(sad, sad);                 \
+    sad = __lasx_xvhaddw_qu_du(sad, sad);                 \
+
+void x264_pixel_sad_x3_16x8_lasx( uint8_t *p_src, uint8_t *p_ref0,
+                                  uint8_t *p_ref1, uint8_t *p_ref2,
+                                  intptr_t i_ref_stride,
+                                  int32_t p_sad_array[3] )
+{
+    SAD_LOAD
+
+    src0 = __lasx_xvld(p_src, 0);
+    src1 = __lasx_xvld(p_src, FENC_STRIDE);
+    src2 = __lasx_xvldx(p_src, i_src_stride_x2);
+    src3 = __lasx_xvldx(p_src, i_src_stride_x3);
+    p_src += i_src_stride_x4;
+    src4 = __lasx_xvld(p_src, 0);
+    src5 = __lasx_xvld(p_src, FENC_STRIDE);
+    src6 = __lasx_xvldx(p_src, i_src_stride_x2);
+    src7 = __lasx_xvldx(p_src, i_src_stride_x3);
+    src0 = __lasx_xvpermi_q( src0, src1, 0x20 );
+    src1 = __lasx_xvpermi_q( src2, src3, 0x20 );
+    src2 = __lasx_xvpermi_q( src4, src5, 0x20 );
+    src3 = __lasx_xvpermi_q( src6, src7, 0x20 );
+
+    LOAD_REF_DATA_16W( p_ref0, sad0 );
+    LOAD_REF_DATA_16W( p_ref1, sad1 );
+    LOAD_REF_DATA_16W( p_ref2, sad2 );
+
+    ST_REF_DATA(sad0);
+    p_sad_array[0] = __lasx_xvpickve2gr_wu(sad0, 0) + __lasx_xvpickve2gr_wu(sad0, 4);
+    ST_REF_DATA(sad1);
+    p_sad_array[1] = __lasx_xvpickve2gr_wu(sad1, 0) + __lasx_xvpickve2gr_wu(sad1, 4);
+    ST_REF_DATA(sad2);
+    p_sad_array[2] = __lasx_xvpickve2gr_wu(sad2, 0) + __lasx_xvpickve2gr_wu(sad2, 4);
+}
+
+#undef LOAD_REF_DATA_16W
+
+#define LOAD_REF_DATA_8W( p_ref, sad)                                          \
+    LASX_LOAD_4( p_ref, i_ref_stride, i_ref_stride_x2, i_ref_stride_x3,        \
+                 ref0, ref1, ref2, ref3 );                                     \
+    p_ref += i_ref_stride_x4;                                                  \
+    LASX_LOAD_4( p_ref, i_ref_stride, i_ref_stride_x2, i_ref_stride_x3,        \
+                 ref4, ref5, ref6, ref7 );                                     \
+    ref0 = __lasx_xvilvl_d( ref1, ref0 );                                      \
+    ref1 = __lasx_xvilvl_d( ref3, ref2 );                                      \
+    ref2 = __lasx_xvilvl_d( ref5, ref4 );                                      \
+    ref3 = __lasx_xvilvl_d( ref7, ref6 );                                      \
+    ref0 = __lasx_xvpermi_q( ref0, ref1, 0x20 );                               \
+    ref1 = __lasx_xvpermi_q( ref2, ref3, 0x20 );                               \
+    diff = __lasx_xvabsd_bu( src0, ref0 );                                     \
+    diff = __lasx_xvhaddw_hu_bu( diff, diff );                                 \
+    sad  = __lasx_xvadd_h(sad, diff);                                          \
+    diff = __lasx_xvabsd_bu( src1, ref1 );                                     \
+    diff = __lasx_xvhaddw_hu_bu( diff, diff );                                 \
+    sad  = __lasx_xvadd_h(sad, diff);                                          \
+
+void x264_pixel_sad_x3_8x16_lasx( uint8_t *p_src, uint8_t *p_ref0,
+                                  uint8_t *p_ref1, uint8_t *p_ref2,
+                                  intptr_t i_ref_stride,
+                                  int32_t p_sad_array[3] )
+{
+    SAD_LOAD
+
+    src0 = __lasx_xvld(p_src, 0);
+    src1 = __lasx_xvld(p_src, FENC_STRIDE);
+    src2 = __lasx_xvldx(p_src, i_src_stride_x2);
+    src3 = __lasx_xvldx(p_src, i_src_stride_x3);
+    p_src += i_src_stride_x4;
+    src4 = __lasx_xvld(p_src, 0);
+    src5 = __lasx_xvld(p_src, FENC_STRIDE);
+    src6 = __lasx_xvldx(p_src, i_src_stride_x2);
+    src7 = __lasx_xvldx(p_src, i_src_stride_x3);
+    p_src += i_src_stride_x4;
+    src0 = __lasx_xvilvl_d( src1, src0 );
+    src1 = __lasx_xvilvl_d( src3, src2 );
+    src2 = __lasx_xvilvl_d( src5, src4 );
+    src3 = __lasx_xvilvl_d( src7, src6 );
+    src0 = __lasx_xvpermi_q( src0, src1, 0x20 );
+    src1 = __lasx_xvpermi_q( src2, src3, 0x20 );
+
+    LOAD_REF_DATA_8W( p_ref0, sad0 );
+    LOAD_REF_DATA_8W( p_ref1, sad1 );
+    LOAD_REF_DATA_8W( p_ref2, sad2 );
+
+    src0 = __lasx_xvld(p_src, 0);
+    src1 = __lasx_xvld(p_src, FENC_STRIDE);
+    src2 = __lasx_xvldx(p_src, i_src_stride_x2);
+    src3 = __lasx_xvldx(p_src, i_src_stride_x3);
+    p_src += i_src_stride_x4;
+    src4 = __lasx_xvld(p_src, 0);
+    src5 = __lasx_xvld(p_src, FENC_STRIDE);
+    src6 = __lasx_xvldx(p_src, i_src_stride_x2);
+    src7 = __lasx_xvldx(p_src, i_src_stride_x3);
+    src0 = __lasx_xvilvl_d( src1, src0 );
+    src1 = __lasx_xvilvl_d( src3, src2 );
+    src2 = __lasx_xvilvl_d( src5, src4 );
+    src3 = __lasx_xvilvl_d( src7, src6 );
+    src0 = __lasx_xvpermi_q( src0, src1, 0x20 );
+    src1 = __lasx_xvpermi_q( src2, src3, 0x20 );
+
+    p_ref0 += i_ref_stride_x4;
+    p_ref1 += i_ref_stride_x4;
+    p_ref2 += i_ref_stride_x4;
+
+    LOAD_REF_DATA_8W( p_ref0, sad0 );
+    LOAD_REF_DATA_8W( p_ref1, sad1 );
+    LOAD_REF_DATA_8W( p_ref2, sad2 );
+
+    ST_REF_DATA(sad0);
+    p_sad_array[0] = __lasx_xvpickve2gr_wu(sad0, 0) + __lasx_xvpickve2gr_wu(sad0, 4);
+    ST_REF_DATA(sad1);
+    p_sad_array[1] = __lasx_xvpickve2gr_wu(sad1, 0) + __lasx_xvpickve2gr_wu(sad1, 4);
+    ST_REF_DATA(sad2);
+    p_sad_array[2] = __lasx_xvpickve2gr_wu(sad2, 0) + __lasx_xvpickve2gr_wu(sad2, 4);
+}
+
+void x264_pixel_sad_x3_8x8_lasx( uint8_t *p_src, uint8_t *p_ref0,
+                                 uint8_t *p_ref1, uint8_t *p_ref2,
+                                 intptr_t i_ref_stride,
+                                 int32_t p_sad_array[3] )
+{
+    SAD_LOAD
+
+    src0 = __lasx_xvld(p_src, 0);
+    src1 = __lasx_xvld(p_src, FENC_STRIDE);
+    src2 = __lasx_xvldx(p_src, i_src_stride_x2);
+    src3 = __lasx_xvldx(p_src, i_src_stride_x3);
+    p_src += i_src_stride_x4;
+    src4 = __lasx_xvld(p_src, 0);
+    src5 = __lasx_xvld(p_src, FENC_STRIDE);
+    src6 = __lasx_xvldx(p_src, i_src_stride_x2);
+    src7 = __lasx_xvldx(p_src, i_src_stride_x3);
+    src0 = __lasx_xvilvl_d( src1, src0 );
+    src1 = __lasx_xvilvl_d( src3, src2 );
+    src2 = __lasx_xvilvl_d( src5, src4 );
+    src3 = __lasx_xvilvl_d( src7, src6 );
+    src0 = __lasx_xvpermi_q( src0, src1, 0x20 );
+    src1 = __lasx_xvpermi_q( src2, src3, 0x20 );
+
+    LOAD_REF_DATA_8W( p_ref0, sad0 );
+    LOAD_REF_DATA_8W( p_ref1, sad1 );
+    LOAD_REF_DATA_8W( p_ref2, sad2 );
+
+    ST_REF_DATA(sad0);
+    p_sad_array[0] = __lasx_xvpickve2gr_wu(sad0, 0) + __lasx_xvpickve2gr_wu(sad0, 4);
+    ST_REF_DATA(sad1);
+    p_sad_array[1] = __lasx_xvpickve2gr_wu(sad1, 0) + __lasx_xvpickve2gr_wu(sad1, 4);
+    ST_REF_DATA(sad2);
+    p_sad_array[2] = __lasx_xvpickve2gr_wu(sad2, 0) + __lasx_xvpickve2gr_wu(sad2, 4);
+}
+
+#undef SAD_LOAD
+#undef LOAD_REF_DATA_8W
+
+void x264_pixel_sad_x3_8x4_lasx( uint8_t *p_src, uint8_t *p_ref0,
+                                 uint8_t *p_ref1, uint8_t *p_ref2,
+                                 intptr_t i_ref_stride,
+                                 int32_t p_sad_array[3] )
+{
+    __m256i src0, src1, src2, src3;
+    __m256i ref0, ref1, ref2, ref3;
+    __m256i diff;
+    __m256i sad0, sad1, sad2;
+    intptr_t i_src_stride_x2 = FENC_STRIDE << 1;
+    intptr_t i_ref_stride_x2 = i_ref_stride << 1;
+    intptr_t i_src_stride_x3 = i_src_stride_x2 + FENC_STRIDE;
+    intptr_t i_ref_stride_x3 = i_ref_stride_x2 + i_ref_stride;
+
+    src0 = __lasx_xvld( p_src, 0 );
+    src1 = __lasx_xvld( p_src, FENC_STRIDE );
+    src2 = __lasx_xvldx( p_src, i_src_stride_x2 );
+    src3 = __lasx_xvldx( p_src, i_src_stride_x3 );
+    src0 = __lasx_xvilvl_d( src1, src0 );
+    src1 = __lasx_xvilvl_d( src3, src2 );
+    src0 = __lasx_xvpermi_q( src0, src1, 0x20 );
+
+#define LOAD_REF_DATA_8W_4H( p_ref, sad)                                \
+    LASX_LOAD_4( p_ref, i_ref_stride, i_ref_stride_x2, i_ref_stride_x3, \
+                 ref0, ref1, ref2, ref3 );                              \
+    ref0 = __lasx_xvilvl_d( ref1, ref0 );                               \
+    ref1 = __lasx_xvilvl_d( ref3, ref2 );                               \
+    ref0 = __lasx_xvpermi_q( ref0, ref1, 0x20 );                        \
+    diff = __lasx_xvabsd_bu( src0, ref0 );                              \
+    sad = __lasx_xvhaddw_hu_bu( diff, diff );
+
+    LOAD_REF_DATA_8W_4H( p_ref0, sad0 );
+    LOAD_REF_DATA_8W_4H( p_ref1, sad1 );
+    LOAD_REF_DATA_8W_4H( p_ref2, sad2 );
+
+#undef LOAD_REF_DATA_8W_4H
+
+    ST_REF_DATA(sad0);
+    p_sad_array[0] = __lasx_xvpickve2gr_wu(sad0, 0) + __lasx_xvpickve2gr_wu(sad0, 4);
+    ST_REF_DATA(sad1);
+    p_sad_array[1] = __lasx_xvpickve2gr_wu(sad1, 0) + __lasx_xvpickve2gr_wu(sad1, 4);
+    ST_REF_DATA(sad2);
+    p_sad_array[2] = __lasx_xvpickve2gr_wu(sad2, 0) + __lasx_xvpickve2gr_wu(sad2, 4);
+
+#undef ST_REF_DATA
+
+}
+
+void x264_pixel_sad_x3_4x8_lasx( uint8_t *p_src, uint8_t *p_ref0,
+                                 uint8_t *p_ref1, uint8_t *p_ref2,
+                                 intptr_t i_ref_stride,
+                                 int32_t p_sad_array[3] )
+{
+    __m256i src0, src1, src2, src3, src4, src5, src6, src7;
+    __m256i ref0, ref1, ref2, ref3, ref4, ref5, ref6, ref7;
+    __m256i diff;
+    __m256i sad0 = __lasx_xvldi( 0 );
+    __m256i sad1 = __lasx_xvldi( 0 );
+    __m256i sad2 = __lasx_xvldi( 0 );
+    intptr_t i_src_stride_x2 = FENC_STRIDE << 1;
+    intptr_t i_ref_stride_x2 = i_ref_stride << 1;
+    intptr_t i_src_stride_x3 = i_src_stride_x2 + FENC_STRIDE;
+    intptr_t i_ref_stride_x3 = i_ref_stride_x2 + i_ref_stride;
+    intptr_t i_src_stride_x4 = i_src_stride_x2 << 1;
+    intptr_t i_ref_stride_x4 = i_ref_stride_x2 << 1;
+
+    src0 = __lasx_xvld( p_src, 0 );
+    src1 = __lasx_xvld( p_src, FENC_STRIDE );
+    src2 = __lasx_xvldx( p_src, i_src_stride_x2 );
+    src3 = __lasx_xvldx( p_src, i_src_stride_x3 );
+    p_src += i_src_stride_x4;
+    src4 = __lasx_xvld( p_src, 0 );
+    src5 = __lasx_xvld( p_src, FENC_STRIDE );
+    src6 = __lasx_xvldx( p_src, i_src_stride_x2 );
+    src7 = __lasx_xvldx( p_src, i_src_stride_x3 );
+    src0 = __lasx_xvilvl_w( src1, src0 );
+    src1 = __lasx_xvilvl_w( src3, src2 );
+    src2 = __lasx_xvilvl_w( src5, src4 );
+    src3 = __lasx_xvilvl_w( src7, src6 );
+    src0 = __lasx_xvilvl_d( src1, src0 );
+    src1 = __lasx_xvilvl_d( src3, src2 );
+    src0 = __lasx_xvpermi_q( src0, src1, 0x20 );
+
+#define LOAD_REF_DATA_4W_8H( p_ref, sad)                                       \
+    LASX_LOAD_4( p_ref, i_ref_stride, i_ref_stride_x2, i_ref_stride_x3,        \
+                 ref0, ref1, ref2, ref3 );                                     \
+    p_ref += i_ref_stride_x4;                                                  \
+    LASX_LOAD_4( p_ref, i_ref_stride, i_ref_stride_x2, i_ref_stride_x3,        \
+                 ref4, ref5, ref6, ref7 );                                     \
+    ref0 = __lasx_xvilvl_w( ref1, ref0 );                                      \
+    ref1 = __lasx_xvilvl_w( ref3, ref2 );                                      \
+    ref2 = __lasx_xvilvl_w( ref5, ref4 );                                      \
+    ref3 = __lasx_xvilvl_w( ref7, ref6 );                                      \
+    ref0 = __lasx_xvilvl_d( ref1, ref0 );                                      \
+    ref1 = __lasx_xvilvl_d( ref3, ref2 );                                      \
+    ref0 = __lasx_xvpermi_q( ref0, ref1, 0x20 );                               \
+    diff = __lasx_xvabsd_bu( src0, ref0 );                                     \
+    sad = __lasx_xvhaddw_hu_bu( diff, diff );
+
+    LOAD_REF_DATA_4W_8H( p_ref0, sad0 );
+    LOAD_REF_DATA_4W_8H( p_ref1, sad1 );
+    LOAD_REF_DATA_4W_8H( p_ref2, sad2 );
+
+#undef LOAD_REF_DATA_4W_8H
+
+#define ST_REF_DATA(sad)                                  \
+    sad = __lasx_xvhaddw_wu_hu(sad, sad);                 \
+    sad = __lasx_xvhaddw_du_wu(sad, sad);                 \
+    sad = __lasx_xvhaddw_qu_du(sad, sad);                 \
+
+    ST_REF_DATA(sad0);
+    p_sad_array[0] = __lasx_xvpickve2gr_wu(sad0, 0) + __lasx_xvpickve2gr_wu(sad0, 4);
+    ST_REF_DATA(sad1);
+    p_sad_array[1] = __lasx_xvpickve2gr_wu(sad1, 0) + __lasx_xvpickve2gr_wu(sad1, 4);
+    ST_REF_DATA(sad2);
+    p_sad_array[2] = __lasx_xvpickve2gr_wu(sad2, 0) + __lasx_xvpickve2gr_wu(sad2, 4);
+
+#undef ST_REF_DATA
+
+}
+
+void x264_pixel_sad_x3_4x4_lasx( uint8_t *p_src, uint8_t *p_ref0,
+                                 uint8_t *p_ref1, uint8_t *p_ref2,
+                                 intptr_t i_ref_stride,
+                                 int32_t p_sad_array[3] )
+{
+    __m256i src0, src1, src2, src3;
+    __m256i ref0, ref1, ref2, ref3, ref4, ref5, ref6, ref7;
+    __m256i diff;
+    intptr_t i_src_stride_x2 = FENC_STRIDE << 1;
+    intptr_t i_ref_stride_x2 = i_ref_stride << 1;
+    intptr_t i_src_stride_x3 = i_src_stride_x2 + FENC_STRIDE;
+    intptr_t i_ref_stride_x3 = i_ref_stride_x2 + i_ref_stride;
+
+    src0 = __lasx_xvld( p_src, 0 );
+    src1 = __lasx_xvld( p_src, FENC_STRIDE );
+    src2 = __lasx_xvldx( p_src, i_src_stride_x2 );
+    src3 = __lasx_xvldx( p_src, i_src_stride_x3 );
+    src0 = __lasx_xvilvl_w( src1, src0 );
+    src1 = __lasx_xvilvl_w( src3, src2 );
+    src0 = __lasx_xvilvl_d( src1, src0 );
+    src0 = __lasx_xvpermi_q(src0, src0, 0x00);
+
+    LASX_LOAD_4( p_ref0, i_ref_stride, i_ref_stride_x2, i_ref_stride_x3,
+                 ref0, ref1, ref2, ref3 );
+    LASX_LOAD_4( p_ref1, i_ref_stride, i_ref_stride_x2, i_ref_stride_x3,
+                 ref4, ref5, ref6, ref7 );
+    ref0 = __lasx_xvilvl_w( ref1, ref0 );
+    ref1 = __lasx_xvilvl_w( ref3, ref2 );
+    ref0 = __lasx_xvilvl_d( ref1, ref0 );
+    ref2 = __lasx_xvilvl_w( ref5, ref4 );
+    ref3 = __lasx_xvilvl_w( ref7, ref6 );
+    ref1 = __lasx_xvilvl_d( ref3, ref2 );
+    ref0 = __lasx_xvpermi_q(ref0, ref1, 0x02);
+    diff = __lasx_xvabsd_bu( src0, ref0 );
+    diff = __lasx_xvhaddw_hu_bu( diff, diff );
+    diff = __lasx_xvhaddw_wu_hu( diff, diff );
+    diff = __lasx_xvhaddw_du_wu( diff, diff );
+    diff = __lasx_xvhaddw_qu_du( diff, diff );
+
+    p_sad_array[0] = __lasx_xvpickve2gr_wu(diff, 0);
+    p_sad_array[1] = __lasx_xvpickve2gr_wu(diff, 4);
+    LASX_LOAD_4( p_ref2, i_ref_stride, i_ref_stride_x2, i_ref_stride_x3,
+                 ref0, ref1, ref2, ref3 );
+    ref0 = __lasx_xvilvl_w( ref1, ref0 );
+    ref1 = __lasx_xvilvl_w( ref3, ref2 );
+    ref0 = __lasx_xvilvl_d( ref1, ref0 );
+    diff = __lasx_xvabsd_bu( src0, ref0 );
+    diff = __lasx_xvhaddw_hu_bu( diff, diff );
+    diff = __lasx_xvhaddw_wu_hu( diff, diff );
+    diff = __lasx_xvhaddw_du_wu( diff, diff );
+    diff = __lasx_xvhaddw_qu_du( diff, diff );
+    p_sad_array[2] = __lasx_xvpickve2gr_wu(diff, 0);
+
+}
+
+static inline uint32_t sad_4width_lasx( uint8_t *p_src, int32_t i_src_stride,
+                                        uint8_t *p_ref, int32_t i_ref_stride,
+                                        int32_t i_height )
+{
+    int32_t i_ht_cnt;
+    uint32_t result;
+    uint8_t * p_src2;
+    __m256i src0, src1, src2, src3, src4, src5, src6, src7;
+    __m256i ref0, ref1, ref2, ref3, ref4, ref5, ref6, ref7;
+    __m256i diff;
+    __m256i sad = __lasx_xvldi( 0 );
+    int32_t i_src_stride_x2 = i_src_stride << 1;
+    int32_t i_ref_stride_x2 = i_ref_stride << 1;
+    int32_t i_src_stride_x3 = i_src_stride + i_src_stride_x2;
+    int32_t i_ref_stride_x3 = i_ref_stride + i_ref_stride_x2;
+    int32_t i_src_stride_x4 = i_src_stride_x2 << 1;
+    int32_t i_ref_stride_x4 = i_ref_stride_x2 << 1;
+    int32_t i_src_stride_x8 = i_src_stride << 3;
+
+    for( i_ht_cnt = ( i_height >> 3 ); i_ht_cnt--; )
+    {
+        src0 = __lasx_xvld( p_src, 0 );
+        src1 = __lasx_xvldx( p_src, i_src_stride );
+        src2 = __lasx_xvldx( p_src, i_src_stride_x2 );
+        src3 = __lasx_xvldx( p_src, i_src_stride_x3 );
+        p_src2 = p_src + i_src_stride_x4;
+        src4 = __lasx_xvld( p_src2, 0 );
+        src5 = __lasx_xvldx( p_src2, i_src_stride );
+        src6 = __lasx_xvldx( p_src2, i_src_stride_x2 );
+        src7 = __lasx_xvldx( p_src2, i_src_stride_x3 );
+        p_src += i_src_stride_x8;
+        src0 = __lasx_xvilvl_w( src1, src0 );
+        src1 = __lasx_xvilvl_w( src3, src2 );
+        src2 = __lasx_xvilvl_w( src5, src4 );
+        src3 = __lasx_xvilvl_w( src7, src6 );
+        src0 = __lasx_xvilvl_d( src1, src0 );
+        src1 = __lasx_xvilvl_d( src3, src2 );
+        src0 = __lasx_xvpermi_q( src0, src1, 0x20 );
+
+        LASX_LOAD_4( p_ref, i_ref_stride, i_ref_stride_x2, i_ref_stride_x3,
+                     ref0, ref1, ref2, ref3 );
+        p_ref += i_ref_stride_x4;
+        LASX_LOAD_4( p_ref, i_ref_stride, i_ref_stride_x2, i_ref_stride_x3,
+                     ref4, ref5, ref6, ref7 );
+        p_ref += i_ref_stride_x4;
+        ref0 = __lasx_xvilvl_w( ref1, ref0 );
+        ref1 = __lasx_xvilvl_w( ref3, ref2 );
+        ref2 = __lasx_xvilvl_w( ref5, ref4 );
+        ref3 = __lasx_xvilvl_w( ref7, ref6 );
+        ref0 = __lasx_xvilvl_d( ref1, ref0 );
+        ref1 = __lasx_xvilvl_d( ref3, ref2 );
+        ref0 = __lasx_xvpermi_q( ref0, ref1, 0x20 );
+        diff = __lasx_xvabsd_bu( src0, ref0 );
+        diff = __lasx_xvhaddw_hu_bu( diff, diff );
+        sad  = __lasx_xvadd_h( sad, diff );
+    }
+    sad = __lasx_xvhaddw_wu_hu(sad, sad);
+    sad = __lasx_xvhaddw_du_wu(sad, sad);
+    sad = __lasx_xvhaddw_qu_du(sad, sad);
+    result = __lasx_xvpickve2gr_wu(sad, 0) + __lasx_xvpickve2gr_wu(sad, 4);
+
+    return ( result );
+}
+
+int32_t x264_pixel_sad_16x8_lasx( uint8_t *p_src, intptr_t i_src_stride,
+                                  uint8_t *p_ref, intptr_t i_ref_stride )
+{
+    uint32_t result;
+    __m256i src0, src1, src2, src3, src4, src5, src6, src7;
+    __m256i ref0, ref1, ref2, ref3, ref4, ref5, ref6, ref7;
+    __m256i diff, sad;
+    int32_t i_src_stride_x2 = i_src_stride << 1;
+    int32_t i_ref_stride_x2 = i_ref_stride << 1;
+    int32_t i_src_stride_x3 = i_src_stride + i_src_stride_x2;
+    int32_t i_ref_stride_x3 = i_ref_stride + i_ref_stride_x2;
+    int32_t i_src_stride_x4 = i_src_stride_x2 << 1;
+    int32_t i_ref_stride_x4 = i_ref_stride_x2 << 1;
+
+    LASX_LOAD_4( p_src, i_src_stride, i_src_stride_x2, i_src_stride_x3,
+                 src0, src1, src2, src3 );
+    p_src += i_src_stride_x4;
+    LASX_LOAD_4( p_src, i_src_stride, i_src_stride_x2, i_src_stride_x3,
+                 src4, src5, src6, src7 );
+    src0 = __lasx_xvpermi_q( src0, src1, 0x20 );
+    src1 = __lasx_xvpermi_q( src2, src3, 0x20 );
+    src2 = __lasx_xvpermi_q( src4, src5, 0x20 );
+    src3 = __lasx_xvpermi_q( src6, src7, 0x20 );
+
+    LASX_LOAD_4( p_ref, i_ref_stride, i_ref_stride_x2, i_ref_stride_x3,
+                 ref0, ref1, ref2, ref3 );
+    p_ref += i_ref_stride_x4;
+    LASX_LOAD_4( p_ref, i_ref_stride, i_ref_stride_x2, i_ref_stride_x3,
+                 ref4, ref5, ref6, ref7 );
+    ref0 = __lasx_xvpermi_q( ref0, ref1, 0x20 );
+    ref1 = __lasx_xvpermi_q( ref2, ref3, 0x20 );
+    ref2 = __lasx_xvpermi_q( ref4, ref5, 0x20 );
+    ref3 = __lasx_xvpermi_q( ref6, ref7, 0x20 );
+    diff = __lasx_xvabsd_bu( src0, ref0 );
+    sad  = __lasx_xvhaddw_hu_bu( diff, diff );
+    diff = __lasx_xvabsd_bu( src1, ref1 );
+    diff = __lasx_xvhaddw_hu_bu( diff, diff );
+    sad  = __lasx_xvadd_h(sad, diff);
+    diff = __lasx_xvabsd_bu( src2, ref2 );
+    diff = __lasx_xvhaddw_hu_bu( diff, diff );
+    sad  = __lasx_xvadd_h(sad, diff);
+    diff = __lasx_xvabsd_bu( src3, ref3 );
+    diff = __lasx_xvhaddw_hu_bu( diff, diff );
+    sad  = __lasx_xvadd_h(sad, diff);
+    sad = __lasx_xvhaddw_wu_hu(sad, sad);
+    sad = __lasx_xvhaddw_du_wu(sad, sad);
+    sad = __lasx_xvhaddw_qu_du(sad, sad);
+    result = __lasx_xvpickve2gr_wu(sad, 0) + __lasx_xvpickve2gr_wu(sad, 4);
+
+    return ( result );
+}
+
+int32_t x264_pixel_sad_8x16_lasx( uint8_t *p_src, intptr_t i_src_stride,
+                                  uint8_t *p_ref, intptr_t i_ref_stride )
+{
+    uint32_t result;
+    __m256i src0, src1, src2, src3, src4, src5, src6, src7;
+    __m256i ref0, ref1, ref2, ref3, ref4, ref5, ref6, ref7;
+    __m256i diff, sad;
+    int32_t i_src_stride_x2 = i_src_stride << 1;
+    int32_t i_ref_stride_x2 = i_ref_stride << 1;
+    int32_t i_src_stride_x3 = i_src_stride + i_src_stride_x2;
+    int32_t i_ref_stride_x3 = i_ref_stride + i_ref_stride_x2;
+    int32_t i_src_stride_x4 = i_src_stride_x2 << 1;
+    int32_t i_ref_stride_x4 = i_ref_stride_x2 << 1;
+
+    LASX_LOAD_4( p_src, i_src_stride, i_src_stride_x2, i_src_stride_x3,
+                 src0, src1, src2, src3 );
+    p_src += i_src_stride_x4;
+    LASX_LOAD_4( p_src, i_src_stride, i_src_stride_x2, i_src_stride_x3,
+                 src4, src5, src6, src7 );
+    p_src += i_src_stride_x4;
+    src0 = __lasx_xvilvl_d( src1, src0 );
+    src1 = __lasx_xvilvl_d( src3, src2 );
+    src2 = __lasx_xvilvl_d( src5, src4 );
+    src3 = __lasx_xvilvl_d( src7, src6 );
+    src0 = __lasx_xvpermi_q( src0, src1, 0x20 );
+    src1 = __lasx_xvpermi_q( src2, src3, 0x20 );
+
+    LASX_LOAD_4( p_ref, i_ref_stride, i_ref_stride_x2, i_ref_stride_x3,
+                 ref0, ref1, ref2, ref3 );
+    p_ref += i_ref_stride_x4;
+    LASX_LOAD_4( p_ref, i_ref_stride, i_ref_stride_x2, i_ref_stride_x3,
+                 ref4, ref5, ref6, ref7 );
+    p_ref += i_ref_stride_x4;
+    ref0 = __lasx_xvilvl_d( ref1, ref0 );
+    ref1 = __lasx_xvilvl_d( ref3, ref2 );
+    ref2 = __lasx_xvilvl_d( ref5, ref4 );
+    ref3 = __lasx_xvilvl_d( ref7, ref6 );
+    ref0 = __lasx_xvpermi_q( ref0, ref1, 0x20 );
+    ref1 = __lasx_xvpermi_q( ref2, ref3, 0x20 );
+    diff = __lasx_xvabsd_bu( src0, ref0 );
+    sad  = __lasx_xvhaddw_hu_bu( diff, diff );
+    diff = __lasx_xvabsd_bu( src1, ref1 );
+    diff = __lasx_xvhaddw_hu_bu( diff, diff );
+    sad  = __lasx_xvadd_h(sad, diff);
+
+    LASX_LOAD_4( p_src, i_src_stride, i_src_stride_x2, i_src_stride_x3,
+                 src0, src1, src2, src3 );
+    p_src += i_src_stride_x4;
+    LASX_LOAD_4( p_src, i_src_stride, i_src_stride_x2, i_src_stride_x3,
+                 src4, src5, src6, src7 );
+    src0 = __lasx_xvilvl_d( src1, src0 );
+    src1 = __lasx_xvilvl_d( src3, src2 );
+    src2 = __lasx_xvilvl_d( src5, src4 );
+    src3 = __lasx_xvilvl_d( src7, src6 );
+    src0 = __lasx_xvpermi_q( src0, src1, 0x20 );
+    src1 = __lasx_xvpermi_q( src2, src3, 0x20 );
+
+    LASX_LOAD_4( p_ref, i_ref_stride, i_ref_stride_x2, i_ref_stride_x3,
+                 ref0, ref1, ref2, ref3 );
+    p_ref += i_ref_stride_x4;
+    LASX_LOAD_4( p_ref, i_ref_stride, i_ref_stride_x2, i_ref_stride_x3,
+                 ref4, ref5, ref6, ref7 );
+    ref0 = __lasx_xvilvl_d( ref1, ref0 );
+    ref1 = __lasx_xvilvl_d( ref3, ref2 );
+    ref2 = __lasx_xvilvl_d( ref5, ref4 );
+    ref3 = __lasx_xvilvl_d( ref7, ref6 );
+    ref0 = __lasx_xvpermi_q( ref0, ref1, 0x20 );
+    ref1 = __lasx_xvpermi_q( ref2, ref3, 0x20 );
+    diff = __lasx_xvabsd_bu( src0, ref0 );
+    diff = __lasx_xvhaddw_hu_bu( diff, diff );
+    sad  = __lasx_xvadd_h(sad, diff);
+    diff = __lasx_xvabsd_bu( src1, ref1 );
+    diff = __lasx_xvhaddw_hu_bu( diff, diff );
+    sad  = __lasx_xvadd_h(sad, diff);
+
+    sad = __lasx_xvhaddw_wu_hu(sad, sad);
+    sad = __lasx_xvhaddw_du_wu(sad, sad);
+    sad = __lasx_xvhaddw_qu_du(sad, sad);
+    result = __lasx_xvpickve2gr_wu(sad, 0) + __lasx_xvpickve2gr_wu(sad, 4);
+
+    return ( result );
+}
+
+int32_t x264_pixel_sad_8x8_lasx( uint8_t *p_src, intptr_t i_src_stride,
+                                 uint8_t *p_ref, intptr_t i_ref_stride )
+{
+    uint32_t result;
+    __m256i src0, src1, src2, src3, src4, src5, src6, src7;
+    __m256i ref0, ref1, ref2, ref3, ref4, ref5, ref6, ref7;
+    __m256i diff, sad;
+    int32_t i_src_stride_x2 = i_src_stride << 1;
+    int32_t i_ref_stride_x2 = i_ref_stride << 1;
+    int32_t i_src_stride_x3 = i_src_stride + i_src_stride_x2;
+    int32_t i_ref_stride_x3 = i_ref_stride + i_ref_stride_x2;
+    int32_t i_src_stride_x4 = i_src_stride_x2 << 1;
+    int32_t i_ref_stride_x4 = i_ref_stride_x2 << 1;
+
+    LASX_LOAD_4( p_src, i_src_stride, i_src_stride_x2, i_src_stride_x3,
+                 src0, src1, src2, src3 );
+    p_src += i_src_stride_x4;
+    LASX_LOAD_4( p_src, i_src_stride, i_src_stride_x2, i_src_stride_x3,
+                 src4, src5, src6, src7 );
+    src0 = __lasx_xvilvl_d( src1, src0 );
+    src1 = __lasx_xvilvl_d( src3, src2 );
+    src2 = __lasx_xvilvl_d( src5, src4 );
+    src3 = __lasx_xvilvl_d( src7, src6 );
+    src0 = __lasx_xvpermi_q( src0, src1, 0x20 );
+    src1 = __lasx_xvpermi_q( src2, src3, 0x20 );
+
+    LASX_LOAD_4( p_ref, i_ref_stride, i_ref_stride_x2, i_ref_stride_x3,
+                 ref0, ref1, ref2, ref3 );
+    p_ref += i_ref_stride_x4;
+    LASX_LOAD_4( p_ref, i_ref_stride, i_ref_stride_x2, i_ref_stride_x3,
+                 ref4, ref5, ref6, ref7 );
+    ref0 = __lasx_xvilvl_d( ref1, ref0 );
+    ref1 = __lasx_xvilvl_d( ref3, ref2 );
+    ref2 = __lasx_xvilvl_d( ref5, ref4 );
+    ref3 = __lasx_xvilvl_d( ref7, ref6 );
+    ref0 = __lasx_xvpermi_q( ref0, ref1, 0x20 );
+    ref1 = __lasx_xvpermi_q( ref2, ref3, 0x20 );
+    diff = __lasx_xvabsd_bu( src0, ref0 );
+    sad  = __lasx_xvhaddw_hu_bu( diff, diff );
+    diff = __lasx_xvabsd_bu( src1, ref1 );
+    diff = __lasx_xvhaddw_hu_bu( diff, diff );
+    sad  = __lasx_xvadd_h(sad, diff);
+    sad = __lasx_xvhaddw_wu_hu(sad, sad);
+    sad = __lasx_xvhaddw_du_wu(sad, sad);
+    sad = __lasx_xvhaddw_qu_du(sad, sad);
+    result = __lasx_xvpickve2gr_wu(sad, 0) + __lasx_xvpickve2gr_wu(sad, 4);
+
+    return ( result );
+}
+
+int32_t x264_pixel_sad_8x4_lasx( uint8_t *p_src, intptr_t i_src_stride,
+                                 uint8_t *p_ref, intptr_t i_ref_stride )
+{
+    __m256i src0, src1, src2, src3;
+    __m256i ref0, ref1, ref2, ref3;
+    __m256i diff;
+    int32_t result;
+    intptr_t i_src_stride_x2 = i_src_stride << 1;
+    intptr_t i_src_stride_x3 = i_src_stride_x2 + i_src_stride;
+    intptr_t i_ref_stride_x2 = i_ref_stride << 1;
+    intptr_t i_ref_stride_x3 = i_ref_stride + i_ref_stride_x2;
+
+    src0 = __lasx_xvld( p_src, 0 );
+    src1 = __lasx_xvldx( p_src, i_src_stride );
+    src2 = __lasx_xvldx( p_src, i_src_stride_x2 );
+    src3 = __lasx_xvldx( p_src, i_src_stride_x3 );
+    src0 = __lasx_xvilvl_d( src1, src0 );
+    src1 = __lasx_xvilvl_d( src3, src2 );
+    src0 = __lasx_xvpermi_q( src0, src1, 0x20 );
+
+    ref0 = __lasx_xvld( p_ref, 0 );
+    ref1 = __lasx_xvldx( p_ref, i_ref_stride );
+    ref2 = __lasx_xvldx( p_ref, i_ref_stride_x2 );
+    ref3 = __lasx_xvldx( p_ref, i_ref_stride_x3 );
+    ref0 = __lasx_xvilvl_d( ref1, ref0 );
+    ref1 = __lasx_xvilvl_d( ref3, ref2 );
+    ref0 = __lasx_xvpermi_q( ref0, ref1, 0x20 );
+    diff = __lasx_xvabsd_bu( src0, ref0 );
+    diff = __lasx_xvhaddw_hu_bu( diff, diff );
+    diff = __lasx_xvhaddw_wu_hu( diff, diff );
+    diff = __lasx_xvhaddw_du_wu( diff, diff );
+    diff = __lasx_xvhaddw_qu_du( diff, diff );
+    result = __lasx_xvpickve2gr_wu(diff, 0) + __lasx_xvpickve2gr_wu(diff, 4);
+    return ( result );
+}
+
+int32_t x264_pixel_sad_4x16_lasx( uint8_t *p_src, intptr_t i_src_stride,
+                                  uint8_t *p_ref, intptr_t i_ref_stride )
+{
+    return sad_4width_lasx( p_src, i_src_stride, p_ref, i_ref_stride, 16 );
+}
+
+int32_t x264_pixel_sad_4x8_lasx( uint8_t *p_src, intptr_t i_src_stride,
+                                 uint8_t *p_ref, intptr_t i_ref_stride )
+{
+    return sad_4width_lasx( p_src, i_src_stride, p_ref, i_ref_stride, 8 );
+}
+
+int32_t __attribute__ ((noinline)) x264_pixel_sad_4x4_lasx(
+                                 uint8_t *p_src, intptr_t i_src_stride,
+                                 uint8_t *p_ref, intptr_t i_ref_stride )
+{
+    __m256i src0, src1, src2, src3;
+    __m256i ref0, ref1, ref2, ref3;
+    __m256i diff;
+    int32_t result;
+    intptr_t i_src_stride_x2 = i_src_stride << 1;
+    intptr_t i_src_stride_x3 = i_src_stride_x2 + i_src_stride;
+    intptr_t i_ref_stride_x2 = i_ref_stride << 1;
+    intptr_t i_ref_stride_x3 = i_ref_stride + i_ref_stride_x2;
+
+    src0 = __lasx_xvld( p_src, 0);
+    src1 = __lasx_xvldx( p_src, i_src_stride );
+    src2 = __lasx_xvldx( p_src, i_src_stride_x2 );
+    src3 = __lasx_xvldx( p_src, i_src_stride_x3 );
+    src0 = __lasx_xvilvl_w( src1, src0 );
+    src1 = __lasx_xvilvl_w( src3, src2 );
+    src0 = __lasx_xvilvl_d( src1, src0 );
+
+    ref0 = __lasx_xvld( p_ref, 0 );
+    ref1 = __lasx_xvldx( p_ref, i_ref_stride );
+    ref2 = __lasx_xvldx( p_ref, i_ref_stride_x2 );
+    ref3 = __lasx_xvldx( p_ref, i_ref_stride_x3 );
+    ref0 = __lasx_xvilvl_w( ref1, ref0 );
+    ref1 = __lasx_xvilvl_w( ref3, ref2 );
+    ref0 = __lasx_xvilvl_d( ref1, ref0 );
+    diff = __lasx_xvabsd_bu( src0, ref0 );
+    diff = __lasx_xvhaddw_hu_bu( diff, diff );
+    diff = __lasx_xvhaddw_wu_hu( diff, diff );
+    diff = __lasx_xvhaddw_du_wu( diff, diff );
+    diff = __lasx_xvhaddw_qu_du( diff, diff );
+    result = __lasx_xvpickve2gr_w(diff, 0);
+
+    return ( result );
+}
+
+static inline uint64_t pixel_hadamard_ac_8x8_lasx( uint8_t *p_pix,
+                                                   int32_t i_stride )
+{
+    uint32_t u_sum4 = 0, u_sum8 = 0, u_dc;
+    __m256i src0, src1, src2, src3, src4, src5, src6, src7;
+    __m256i diff0, diff1, diff2, diff3;
+    __m256i sub0, sub1, sub2, sub3;
+    __m256i temp0, temp1, temp2, temp3;
+    int32_t i_stride2 = i_stride << 1;
+    int32_t i_stride3 = i_stride2 + i_stride;
+    int32_t i_stride4 = i_stride2 << 1;
+    v16i16  dc;
+
+    LASX_LOAD_4(p_pix, i_stride, i_stride2, i_stride3, src0, src1, src2, src3);
+    p_pix += i_stride4;
+    LASX_LOAD_4(p_pix, i_stride, i_stride2, i_stride3, src4, src5, src6, src7);
+
+    diff0 = __lasx_xvilvl_d(src1, src0);
+    diff1 = __lasx_xvilvl_d(src3, src2);
+    diff2 = __lasx_xvilvl_d(src5, src4);
+    diff3 = __lasx_xvilvl_d(src7, src6);
+    diff0 = __lasx_xvpermi_q(diff0, diff2, 0x02);
+    diff1 = __lasx_xvpermi_q(diff1, diff3, 0x02);
+    diff2 = __lasx_xvpickev_b(diff1, diff0);
+    diff3 = __lasx_xvpickod_b(diff1, diff0);
+    temp0 = __lasx_xvaddwev_h_bu(diff2, diff3);
+    temp1 = __lasx_xvaddwod_h_bu(diff2, diff3);
+    temp2 = __lasx_xvsubwev_h_bu(diff2, diff3);
+    temp3 = __lasx_xvsubwod_h_bu(diff2, diff3);
+
+    diff0 = __lasx_xvadd_h(temp0, temp1);
+    diff1 = __lasx_xvadd_h(temp2, temp3);
+    diff2 = __lasx_xvsub_h(temp0, temp1);
+    diff3 = __lasx_xvsub_h(temp2, temp3);
+
+    temp0 = __lasx_xvilvl_h(diff1, diff0);
+    temp1 = __lasx_xvilvh_h(diff1, diff0);
+    temp2 = __lasx_xvilvl_h(diff3, diff2);
+    temp3 = __lasx_xvilvh_h(diff3, diff2);
+
+    diff0 = __lasx_xvilvl_w(temp2, temp0);
+    diff1 = __lasx_xvilvh_w(temp2, temp0);
+    diff2 = __lasx_xvilvl_w(temp3, temp1);
+    diff3 = __lasx_xvilvh_w(temp3, temp1);
+
+    temp0 = __lasx_xvadd_h(diff0, diff1);
+    temp2 = __lasx_xvadd_h(diff2, diff3);
+    temp1 = __lasx_xvsub_h(diff0, diff1);
+    temp3 = __lasx_xvsub_h(diff2, diff3);
+
+    diff0 = __lasx_xvadd_h(temp0, temp2);
+    diff1 = __lasx_xvadd_h(temp1, temp3);
+    diff2 = __lasx_xvsub_h(temp0, temp2);
+    diff3 = __lasx_xvsub_h(temp1, temp3);
+
+    dc = (v16i16)diff0;
+    u_dc = (uint16_t)(dc[0] + dc[4] + dc[8] + dc[12]);
+
+    sub0 = __lasx_xvadda_h(diff0, diff1);
+    sub1 = __lasx_xvadda_h(diff2, diff3);
+
+    sub0 = __lasx_xvadd_h(sub0, sub1);
+    sub1 = __lasx_xvpermi_d(sub0, 0x4E);
+    sub0 = __lasx_xvadd_h(sub0, sub1);
+    sub0 = __lasx_xvhaddw_wu_hu(sub0, sub0);
+    sub0 = __lasx_xvhaddw_du_wu(sub0, sub0);
+    sub0 = __lasx_xvhaddw_qu_du(sub0, sub0);
+    u_sum4 = __lasx_xvpickve2gr_wu(sub0, 0);
+
+    temp0 = __lasx_xvpackev_h(diff1, diff0);
+    temp1 = __lasx_xvpackev_h(diff3, diff2);
+    temp2 = __lasx_xvpackod_h(diff1, diff0);
+    temp3 = __lasx_xvpackod_h(diff3, diff2);
+
+    sub0 = __lasx_xvilvl_d(temp1, temp0);
+    sub1 = __lasx_xvilvh_d(temp1, temp0);
+    sub2 = __lasx_xvilvl_d(temp3, temp2);
+    sub3 = __lasx_xvilvh_d(temp3, temp2);
+
+    diff0 = __lasx_xvpermi_q(sub0, sub2, 0x02);
+    diff1 = __lasx_xvpermi_q(sub1, sub2, 0x12);
+    diff2 = __lasx_xvpermi_q(sub0, sub3, 0x03);
+    diff3 = __lasx_xvpermi_q(sub1, sub3, 0x13);
+
+    temp0 = __lasx_xvadd_h(diff0, diff1);
+    temp1 = __lasx_xvsub_h(diff0, diff1);
+    temp2 = __lasx_xvadd_h(diff2, diff3);
+    temp3 = __lasx_xvsub_h(diff2, diff3);
+
+    diff0 = __lasx_xvadd_h(temp0, temp2);
+    diff1 = __lasx_xvadd_h(temp1, temp3);
+    diff2 = __lasx_xvsub_h(temp0, temp2);
+    diff3 = __lasx_xvsub_h(temp1, temp3);
+
+    sub0 = __lasx_xvadda_h(diff0, diff1);
+    sub1 = __lasx_xvadda_h(diff2, diff3);
+    sub0 = __lasx_xvadd_h(sub0, sub1);
+    sub1 = __lasx_xvpermi_d(sub0, 0x4E);
+    sub0 = __lasx_xvadd_h(sub0, sub1);
+    sub0 = __lasx_xvhaddw_wu_hu(sub0, sub0);
+    sub0 = __lasx_xvhaddw_du_wu(sub0, sub0);
+    sub0 = __lasx_xvhaddw_qu_du(sub0, sub0);
+    u_sum8 = __lasx_xvpickve2gr_wu(sub0, 0);
+
+    u_sum4 = u_sum4 - u_dc;
+    u_sum8 = u_sum8 - u_dc;
+
+    return ((uint64_t) u_sum8 << 32) + u_sum4;
+}
+
+static inline uint64_t pixel_hadamard_ac_16x8_lasx( uint8_t *p_pix,
+                                                    int32_t i_stride )
+{
+    uint32_t u_sum4 = 0, u_sum8 = 0, u_dc;
+    __m256i src0, src1, src2, src3, src4, src5, src6, src7;
+    __m256i diff0, diff1, diff2, diff3, diff4, diff5, diff6, diff7;
+    __m256i sub0, sub1, sub2, sub3, sub4, sub5, sub6, sub7;
+    int32_t i_stride2 = i_stride << 1;
+    int32_t i_stride3 = i_stride2 + i_stride;
+    int32_t i_stride4 = i_stride2 << 1;
+    v16i16  dc;
+
+    LASX_LOAD_4(p_pix, i_stride, i_stride2, i_stride3, src0, src1, src2, src3);
+    p_pix += i_stride4;
+    LASX_LOAD_4(p_pix, i_stride, i_stride2, i_stride3, src4, src5, src6, src7);
+
+    diff0 = __lasx_xvpermi_q(src0, src4, 0x02);
+    diff1 = __lasx_xvpermi_q(src1, src5, 0x02);
+    diff2 = __lasx_xvpermi_q(src2, src6, 0x02);
+    diff3 = __lasx_xvpermi_q(src3, src7, 0x02);
+
+    diff4 = __lasx_xvpickev_b(diff1, diff0);
+    diff5 = __lasx_xvpickod_b(diff1, diff0);
+    diff6 = __lasx_xvpickev_b(diff3, diff2);
+    diff7 = __lasx_xvpickod_b(diff3, diff2);
+
+    src0 = __lasx_xvaddwev_h_bu(diff4, diff5);
+    src1 = __lasx_xvaddwod_h_bu(diff4, diff5);
+    src2 = __lasx_xvsubwev_h_bu(diff4, diff5);
+    src3 = __lasx_xvsubwod_h_bu(diff4, diff5);
+    src4 = __lasx_xvaddwev_h_bu(diff6, diff7);
+    src5 = __lasx_xvaddwod_h_bu(diff6, diff7);
+    src6 = __lasx_xvsubwev_h_bu(diff6, diff7);
+    src7 = __lasx_xvsubwod_h_bu(diff6, diff7);
+
+    diff0 = __lasx_xvadd_h(src0, src1);
+    diff1 = __lasx_xvadd_h(src2, src3);
+    diff2 = __lasx_xvsub_h(src0, src1);
+    diff3 = __lasx_xvsub_h(src2, src3);
+    diff4 = __lasx_xvadd_h(src4, src5);
+    diff5 = __lasx_xvadd_h(src6, src7);
+    diff6 = __lasx_xvsub_h(src4, src5);
+    diff7 = __lasx_xvsub_h(src6, src7);
+
+    src0 = __lasx_xvilvl_h(diff1, diff0);
+    src1 = __lasx_xvilvh_h(diff1, diff0);
+    src2 = __lasx_xvilvl_h(diff3, diff2);
+    src3 = __lasx_xvilvh_h(diff3, diff2);
+
+    src4 = __lasx_xvilvl_h(diff5, diff4);
+    src5 = __lasx_xvilvh_h(diff5, diff4);
+    src6 = __lasx_xvilvl_h(diff7, diff6);
+    src7 = __lasx_xvilvh_h(diff7, diff6);
+
+    diff0 = __lasx_xvilvl_w(src2, src0);
+    diff1 = __lasx_xvilvh_w(src2, src0);
+    diff2 = __lasx_xvilvl_w(src3, src1);
+    diff3 = __lasx_xvilvh_w(src3, src1);
+
+    diff4 = __lasx_xvilvl_w(src6, src4);
+    diff5 = __lasx_xvilvh_w(src6, src4);
+    diff6 = __lasx_xvilvl_w(src7, src5);
+    diff7 = __lasx_xvilvh_w(src7, src5);
+
+    src0 = __lasx_xvadd_h(diff0, diff2);
+    src4 = __lasx_xvadd_h(diff1, diff3);
+    src2 = __lasx_xvadd_h(diff4, diff6);
+    src6 = __lasx_xvadd_h(diff5, diff7);
+    src1 = __lasx_xvsub_h(diff0, diff2);
+    src5 = __lasx_xvsub_h(diff1, diff3);
+    src3 = __lasx_xvsub_h(diff4, diff6);
+    src7 = __lasx_xvsub_h(diff5, diff7);
+
+    diff0 = __lasx_xvadd_h(src0, src2);
+    diff1 = __lasx_xvadd_h(src1, src3);
+    diff2 = __lasx_xvsub_h(src0, src2);
+    diff3 = __lasx_xvsub_h(src1, src3);
+    diff4 = __lasx_xvadd_h(src4, src6);
+    diff5 = __lasx_xvadd_h(src5, src7);
+    diff6 = __lasx_xvsub_h(src4, src6);
+    diff7 = __lasx_xvsub_h(src5, src7);
+
+    dc = (v16i16)diff0;
+    u_dc = (uint16_t)(dc[0] + dc[4] + dc[8] + dc[12]);
+    dc = (v16i16)diff4;
+    u_dc += (uint16_t)(dc[0] + dc[4] + dc[8] + dc[12]);
+
+    sub0 = __lasx_xvadda_h(diff0, diff1);
+    sub1 = __lasx_xvadda_h(diff2, diff3);
+    sub2 = __lasx_xvadda_h(diff4, diff5);
+    sub3 = __lasx_xvadda_h(diff6, diff7);
+    sub0 = __lasx_xvadd_h(sub0, sub1);
+    sub0 = __lasx_xvadd_h(sub0, sub2);
+    sub0 = __lasx_xvadd_h(sub0, sub3);
+    sub0 = __lasx_xvhaddw_wu_hu(sub0, sub0);
+    sub0 = __lasx_xvhaddw_du_wu(sub0, sub0);
+    sub0 = __lasx_xvhaddw_qu_du(sub0, sub0);
+    u_sum4 = __lasx_xvpickve2gr_wu(sub0, 0) + __lasx_xvpickve2gr_wu(sub0, 4);
+
+    sub0 = __lasx_xvpackev_h(diff1, diff0);
+    sub1 = __lasx_xvpackod_h(diff1, diff0);
+    sub2 = __lasx_xvpackev_h(diff3, diff2);
+    sub3 = __lasx_xvpackod_h(diff3, diff2);
+    sub4 = __lasx_xvpackev_h(diff5, diff4);
+    sub5 = __lasx_xvpackod_h(diff5, diff4);
+    sub6 = __lasx_xvpackev_h(diff7, diff6);
+    sub7 = __lasx_xvpackod_h(diff7, diff6);
+
+    src0 = __lasx_xvilvl_d(sub2, sub0);
+    src1 = __lasx_xvilvh_d(sub2, sub0);
+    src2 = __lasx_xvilvl_d(sub3, sub1);
+    src3 = __lasx_xvilvh_d(sub3, sub1);
+    src4 = __lasx_xvilvl_d(sub6, sub4);
+    src5 = __lasx_xvilvh_d(sub6, sub4);
+    src6 = __lasx_xvilvl_d(sub7, sub5);
+    src7 = __lasx_xvilvh_d(sub7, sub5);
+
+    diff0 = __lasx_xvpermi_q(src0, src4, 0x02);
+    diff1 = __lasx_xvpermi_q(src1, src5, 0x02);
+    diff2 = __lasx_xvpermi_q(src0, src4, 0x13);
+    diff3 = __lasx_xvpermi_q(src1, src5, 0x13);
+    diff4 = __lasx_xvpermi_q(src2, src6, 0x02);
+    diff5 = __lasx_xvpermi_q(src2, src6, 0x13);
+    diff6 = __lasx_xvpermi_q(src3, src7, 0x02);
+    diff7 = __lasx_xvpermi_q(src3, src7, 0x13);
+
+    src0 = __lasx_xvadd_h(diff0, diff1);
+    src1 = __lasx_xvsub_h(diff0, diff1);
+    src2 = __lasx_xvadd_h(diff2, diff3);
+    src3 = __lasx_xvsub_h(diff2, diff3);
+    src4 = __lasx_xvadd_h(diff4, diff5);
+    src5 = __lasx_xvsub_h(diff4, diff5);
+    src6 = __lasx_xvadd_h(diff6, diff7);
+    src7 = __lasx_xvsub_h(diff6, diff7);
+
+    diff0 = __lasx_xvadd_h(src0, src2);
+    diff1 = __lasx_xvadd_h(src1, src3);
+    diff2 = __lasx_xvsub_h(src0, src2);
+    diff3 = __lasx_xvsub_h(src1, src3);
+    diff4 = __lasx_xvadd_h(src4, src6);
+    diff5 = __lasx_xvadd_h(src5, src7);
+    diff6 = __lasx_xvsub_h(src4, src6);
+    diff7 = __lasx_xvsub_h(src5, src7);
+
+    sub0 = __lasx_xvadda_h(diff0, diff1);
+    sub1 = __lasx_xvadda_h(diff2, diff3);
+    sub2 = __lasx_xvadda_h(diff4, diff5);
+    sub3 = __lasx_xvadda_h(diff6, diff7);
+
+    sub0 = __lasx_xvadd_h(sub0, sub1);
+    sub0 = __lasx_xvadd_h(sub0, sub2);
+    sub0 = __lasx_xvadd_h(sub0, sub3);
+
+    sub0 = __lasx_xvhaddw_wu_hu(sub0, sub0);
+    sub0 = __lasx_xvhaddw_du_wu(sub0, sub0);
+    sub0 = __lasx_xvhaddw_qu_du(sub0, sub0);
+    u_sum8 = __lasx_xvpickve2gr_wu(sub0, 0) + __lasx_xvpickve2gr_wu(sub0, 4);
+    u_sum4 = u_sum4 - u_dc;
+    u_sum8 = u_sum8 - u_dc;
+    return ((uint64_t) u_sum8 << 32) + u_sum4;
+}
+
+uint64_t x264_pixel_hadamard_ac_8x8_lasx( uint8_t *p_pix, intptr_t i_stride )
+{
+    uint64_t u_sum;
+
+    u_sum = pixel_hadamard_ac_8x8_lasx( p_pix, i_stride );
+
+    return ( ( u_sum >> 34 ) << 32 ) + ( ( uint32_t ) u_sum >> 1 );
+}
+
+uint64_t x264_pixel_hadamard_ac_8x16_lasx( uint8_t *p_pix, intptr_t i_stride )
+{
+    uint64_t u_sum;
+
+    u_sum = pixel_hadamard_ac_8x8_lasx( p_pix, i_stride );
+    u_sum += pixel_hadamard_ac_8x8_lasx( p_pix + ( i_stride << 3 ), i_stride );
+
+    return ( ( u_sum >> 34 ) << 32 ) + ( ( uint32_t ) u_sum >> 1 );
+}
+
+uint64_t x264_pixel_hadamard_ac_16x8_lasx( uint8_t *p_pix, intptr_t i_stride )
+{
+    uint64_t u_sum;
+
+    u_sum = pixel_hadamard_ac_16x8_lasx( p_pix, i_stride );
+
+    return ( ( u_sum >> 34 ) << 32 ) + ( ( uint32_t ) u_sum >> 1 );
+}
+
+uint64_t x264_pixel_hadamard_ac_16x16_lasx( uint8_t *p_pix, intptr_t i_stride )
+{
+    uint64_t u_sum;
+
+    u_sum = pixel_hadamard_ac_16x8_lasx( p_pix, i_stride );
+    u_sum += pixel_hadamard_ac_16x8_lasx( p_pix + ( i_stride << 3 ), i_stride );
+
+    return ( ( u_sum >> 34 ) << 32 ) + ( ( uint32_t ) u_sum >> 1 );
+}
+
+static int32_t sa8d_8x8_lasx( uint8_t *p_src, int32_t i_src_stride,
+                              uint8_t *p_ref, int32_t i_ref_stride )
+{
+    uint32_t u_sum = 0;
+    int32_t i_src_stride_x2 = i_src_stride << 1;
+    int32_t i_src_stride_x3 = i_src_stride_x2 + i_src_stride;
+    int32_t i_src_stride_x4 = i_src_stride << 2;
+    int32_t i_ref_stride_x2 = i_ref_stride << 1;
+    int32_t i_ref_stride_x3 = i_ref_stride_x2 + i_ref_stride;
+    int32_t i_ref_stride_x4 = i_ref_stride << 2;
+    __m256i src0, src1, src2, src3, src4, src5, src6, src7;
+    __m256i ref0, ref1, ref2, ref3, ref4, ref5, ref6, ref7;
+    __m256i diff0, diff1, diff2, diff3, diff4, diff5, diff6, diff7;
+    __m256i temp0, temp1, temp2, temp3;
+    v4u64 out;
+
+    DUP4_ARG2( __lasx_xvldx, p_src, 0, p_src, i_src_stride, p_src, i_src_stride_x2, p_src,
+               i_src_stride_x3, src0, src1, src2, src3 );
+    p_src += i_src_stride_x4;
+    DUP4_ARG2( __lasx_xvldx, p_src, 0, p_src, i_src_stride, p_src, i_src_stride_x2, p_src,
+               i_src_stride_x3, src4, src5, src6, src7 );
+    DUP4_ARG2( __lasx_xvldx, p_ref, 0, p_ref, i_ref_stride, p_ref, i_ref_stride_x2,
+               p_ref, i_ref_stride_x3, ref0, ref1, ref2, ref3 );
+    p_ref += i_ref_stride_x4;
+    DUP4_ARG2( __lasx_xvldx, p_ref, 0, p_ref, i_ref_stride, p_ref, i_ref_stride_x2,
+               p_ref, i_ref_stride_x3, ref4, ref5, ref6, ref7 );
+
+    DUP4_ARG2( __lasx_xvilvl_b, src0, ref0, src1, ref1, src2, ref2, src3, ref3,
+               src0, src1, src2, src3 );
+    DUP4_ARG2( __lasx_xvilvl_b, src4, ref4, src5, ref5, src6, ref6, src7, ref7,
+               src4, src5, src6, src7 );
+    DUP4_ARG2( __lasx_xvhsubw_hu_bu, src0, src0, src1, src1, src2, src2, src3, src3,
+               src0, src1, src2, src3 );
+    DUP4_ARG2( __lasx_xvhsubw_hu_bu, src4, src4, src5, src5, src6, src6, src7, src7,
+               src4, src5, src6, src7 );
+    LASX_TRANSPOSE8x8_H( src0, src1, src2, src3,
+                         src4, src5, src6, src7,
+                         src0, src1, src2, src3,
+                         src4, src5, src6, src7 );
+    LASX_BUTTERFLY_4_H( src0, src2, src3, src1, diff0, diff1, diff4, diff5 );
+    LASX_BUTTERFLY_4_H( src4, src6, src7, src5, diff2, diff3, diff7, diff6 );
+    LASX_BUTTERFLY_4_H( diff0, diff2, diff3, diff1, temp0, temp2, temp3, temp1 );
+    LASX_BUTTERFLY_4_H( temp0, temp1, temp3, temp2, diff0, diff1, diff3, diff2 );
+    LASX_BUTTERFLY_4_H( diff4, diff6, diff7, diff5, temp0, temp2, temp3, temp1 );
+    LASX_BUTTERFLY_4_H( temp0, temp1, temp3, temp2, diff4, diff5, diff7, diff6 );
+    LASX_TRANSPOSE8x8_H( diff0, diff1, diff2, diff3,
+                         diff4, diff5, diff6, diff7,
+                         diff0, diff1, diff2, diff3,
+                         diff4, diff5, diff6, diff7 );
+    LASX_BUTTERFLY_4_H( diff0, diff2, diff3, diff1, temp0, temp2, temp3, temp1 );
+    LASX_BUTTERFLY_4_H( temp0, temp1, temp3, temp2, diff0, diff1, diff3, diff2 );
+    LASX_BUTTERFLY_4_H( diff4, diff6, diff7, diff5, temp0, temp2, temp3, temp1 );
+    LASX_BUTTERFLY_4_H( temp0, temp1, temp3, temp2, diff4, diff5, diff7, diff6 );
+
+    temp0 = __lasx_xvadd_h( diff0, diff4 );
+    temp1 = __lasx_xvadd_h( diff1, diff5 );
+    temp2 = __lasx_xvadd_h( diff2, diff6 );
+    temp3 = __lasx_xvadd_h( diff3, diff7 );
+
+    diff0 = __lasx_xvabsd_h( diff0, diff4 );
+    diff1 = __lasx_xvabsd_h( diff1, diff5 );
+    diff2 = __lasx_xvabsd_h( diff2, diff6 );
+    diff3 = __lasx_xvabsd_h( diff3, diff7 );
+    diff0 = __lasx_xvadda_h( diff0, temp0 );
+    diff1 = __lasx_xvadda_h( diff1, temp1 );
+    diff2 = __lasx_xvadda_h( diff2, temp2 );
+    diff3 = __lasx_xvadda_h( diff3, temp3 );
+
+    diff0 = __lasx_xvadd_h( diff0, diff1 );
+    diff0 = __lasx_xvadd_h( diff0, diff2 );
+    diff0 = __lasx_xvadd_h( diff0, diff3 );
+
+    diff0 = __lasx_xvhaddw_wu_hu( diff0, diff0 );
+    out = ( v4u64 ) __lasx_xvhaddw_du_wu( diff0, diff0 );
+    u_sum = out[0] + out[1];
+
+    return u_sum;
+}
+
+static int32_t sa8d_8x16_lasx( uint8_t *p_src, int32_t i_src_stride,
+                               uint8_t *p_ref, int32_t i_ref_stride )
+{
+    uint32_t u_sum = 0;
+    int32_t i_src_stride_x2 = i_src_stride << 1;
+    int32_t i_src_stride_x3 = i_src_stride_x2 + i_src_stride;
+    int32_t i_src_stride_x4 = i_src_stride << 2;
+    int32_t i_ref_stride_x2 = i_ref_stride << 1;
+    int32_t i_ref_stride_x3 = i_ref_stride_x2 + i_ref_stride;
+    int32_t i_ref_stride_x4 = i_ref_stride << 2;
+    __m256i src0, src1, src2, src3, src4, src5, src6, src7;
+    __m256i ref0, ref1, ref2, ref3, ref4, ref5, ref6, ref7;
+    __m256i diff0, diff1, diff2, diff3, diff4, diff5, diff6, diff7;
+    __m256i temp0, temp1, temp2, temp3;
+
+    DUP4_ARG2( __lasx_xvldx, p_src, 0, p_src, i_src_stride, p_src, i_src_stride_x2, p_src,
+               i_src_stride_x3, src0, src1, src2, src3 );
+    p_src += i_src_stride_x4;
+    DUP4_ARG2( __lasx_xvldx, p_src, 0, p_src, i_src_stride, p_src, i_src_stride_x2, p_src,
+               i_src_stride_x3, src4, src5, src6, src7 );
+    src0 = __lasx_xvpermi_d(src0, 0x50);
+    src1 = __lasx_xvpermi_d(src1, 0x50);
+    src2 = __lasx_xvpermi_d(src2, 0x50);
+    src3 = __lasx_xvpermi_d(src3, 0x50);
+    src4 = __lasx_xvpermi_d(src4, 0x50);
+    src5 = __lasx_xvpermi_d(src5, 0x50);
+    src6 = __lasx_xvpermi_d(src6, 0x50);
+    src7 = __lasx_xvpermi_d(src7, 0x50);
+
+    DUP4_ARG2( __lasx_xvldx, p_ref, 0, p_ref, i_ref_stride, p_ref, i_ref_stride_x2,
+               p_ref, i_ref_stride_x3, ref0, ref1, ref2, ref3 );
+    p_ref += i_ref_stride_x4;
+    DUP4_ARG2( __lasx_xvldx, p_ref, 0, p_ref, i_ref_stride, p_ref, i_ref_stride_x2,
+               p_ref, i_ref_stride_x3, ref4, ref5, ref6, ref7 );
+    ref0 = __lasx_xvpermi_d(ref0, 0x50);
+    ref1 = __lasx_xvpermi_d(ref1, 0x50);
+    ref2 = __lasx_xvpermi_d(ref2, 0x50);
+    ref3 = __lasx_xvpermi_d(ref3, 0x50);
+    ref4 = __lasx_xvpermi_d(ref4, 0x50);
+    ref5 = __lasx_xvpermi_d(ref5, 0x50);
+    ref6 = __lasx_xvpermi_d(ref6, 0x50);
+    ref7 = __lasx_xvpermi_d(ref7, 0x50);
+
+    DUP4_ARG2( __lasx_xvilvl_b, src0, ref0, src1, ref1, src2, ref2, src3, ref3,
+               src0, src1, src2, src3 );
+    DUP4_ARG2( __lasx_xvilvl_b, src4, ref4, src5, ref5, src6, ref6, src7, ref7,
+               src4, src5, src6, src7 );
+    DUP4_ARG2( __lasx_xvhsubw_hu_bu, src0, src0, src1, src1, src2, src2, src3, src3,
+               src0, src1, src2, src3 );
+    DUP4_ARG2( __lasx_xvhsubw_hu_bu, src4, src4, src5, src5, src6, src6, src7, src7,
+               src4, src5, src6, src7 );
+    LASX_TRANSPOSE8x8_H( src0, src1, src2, src3,
+                         src4, src5, src6, src7,
+                         src0, src1, src2, src3,
+                         src4, src5, src6, src7 );
+    LASX_BUTTERFLY_4_H( src0, src2, src3, src1, diff0, diff1, diff4, diff5 );
+    LASX_BUTTERFLY_4_H( src4, src6, src7, src5, diff2, diff3, diff7, diff6 );
+    LASX_BUTTERFLY_4_H( diff0, diff2, diff3, diff1, temp0, temp2, temp3, temp1 );
+    LASX_BUTTERFLY_4_H( temp0, temp1, temp3, temp2, diff0, diff1, diff3, diff2 );
+    LASX_BUTTERFLY_4_H( diff4, diff6, diff7, diff5, temp0, temp2, temp3, temp1 );
+    LASX_BUTTERFLY_4_H( temp0, temp1, temp3, temp2, diff4, diff5, diff7, diff6 );
+    LASX_TRANSPOSE8x8_H( diff0, diff1, diff2, diff3,
+                         diff4, diff5, diff6, diff7,
+                         diff0, diff1, diff2, diff3,
+                         diff4, diff5, diff6, diff7 );
+    LASX_BUTTERFLY_4_H( diff0, diff2, diff3, diff1, temp0, temp2, temp3, temp1 );
+    LASX_BUTTERFLY_4_H( temp0, temp1, temp3, temp2, diff0, diff1, diff3, diff2 );
+    LASX_BUTTERFLY_4_H( diff4, diff6, diff7, diff5, temp0, temp2, temp3, temp1 );
+    LASX_BUTTERFLY_4_H( temp0, temp1, temp3, temp2, diff4, diff5, diff7, diff6 );
+
+    temp0 = __lasx_xvadd_h( diff0, diff4 );
+    temp1 = __lasx_xvadd_h( diff1, diff5 );
+    temp2 = __lasx_xvadd_h( diff2, diff6 );
+    temp3 = __lasx_xvadd_h( diff3, diff7 );
+
+    diff0 = __lasx_xvabsd_h( diff0, diff4 );
+    diff1 = __lasx_xvabsd_h( diff1, diff5 );
+    diff2 = __lasx_xvabsd_h( diff2, diff6 );
+    diff3 = __lasx_xvabsd_h( diff3, diff7 );
+    diff0 = __lasx_xvadda_h( diff0, temp0 );
+    diff1 = __lasx_xvadda_h( diff1, temp1 );
+    diff2 = __lasx_xvadda_h( diff2, temp2 );
+    diff3 = __lasx_xvadda_h( diff3, temp3 );
+
+    diff0 = __lasx_xvadd_h( diff0, diff1 );
+    diff0 = __lasx_xvadd_h( diff0, diff2 );
+    diff0 = __lasx_xvadd_h( diff0, diff3 );
+
+    u_sum = LASX_HADD_UH_U32( diff0 );
+
+    return u_sum;
+}
+
+int32_t x264_pixel_sa8d_8x8_lasx( uint8_t *p_pix1, intptr_t i_stride,
+                                  uint8_t *p_pix2, intptr_t i_stride2 )
+{
+    int32_t i32Sum = sa8d_8x8_lasx( p_pix1, i_stride, p_pix2, i_stride2 );
+
+    return ( i32Sum + 2 ) >> 2;
+}
+
+int32_t x264_pixel_sa8d_16x16_lasx( uint8_t *p_pix1, intptr_t i_stride,
+                                    uint8_t *p_pix2, intptr_t i_stride2 )
+{
+    int32_t i32Sum = sa8d_8x16_lasx( p_pix1, i_stride, p_pix2, i_stride2 ) +
+                     sa8d_8x16_lasx( p_pix1 + 8 * i_stride, i_stride,
+                                     p_pix2 + 8 * i_stride2, i_stride2 );
+
+    return ( i32Sum + 2 ) >> 2;
+}
+
+void x264_intra_sa8d_x3_8x8_lasx( uint8_t *p_enc, uint8_t p_edge[36],
+                                  int32_t p_sad_array[3] )
+{
+    ALIGNED_ARRAY_16( uint8_t, pix, [8 * FDEC_STRIDE] );
+
+    x264_intra_predict_v_8x8_lasx( pix, p_edge );
+    p_sad_array[0] = x264_pixel_sa8d_8x8_lasx( pix, FDEC_STRIDE,
+                                               p_enc, FENC_STRIDE );
+
+    x264_intra_predict_h_8x8_lasx( pix, p_edge );
+    p_sad_array[1] = x264_pixel_sa8d_8x8_lasx( pix, FDEC_STRIDE,
+                                               p_enc, FENC_STRIDE );
+
+    x264_intra_predict_dc_8x8_lasx( pix, p_edge );
+    p_sad_array[2] = x264_pixel_sa8d_8x8_lasx( pix, FDEC_STRIDE,
+                                               p_enc, FENC_STRIDE );
+}
+
+void x264_intra_satd_x3_4x4_lasx( uint8_t *p_enc, uint8_t *p_dec,
+                                  int32_t p_sad_array[3] )
+{
+    x264_intra_predict_vert_4x4_lasx( p_dec );
+    p_sad_array[0] = x264_pixel_satd_4x4_lasx( p_dec, FDEC_STRIDE,
+                                               p_enc, FENC_STRIDE );
+
+    x264_intra_predict_hor_4x4_lasx( p_dec );
+    p_sad_array[1] = x264_pixel_satd_4x4_lasx( p_dec, FDEC_STRIDE,
+                                               p_enc, FENC_STRIDE );
+
+    x264_intra_predict_dc_4x4_lasx( p_dec );
+    p_sad_array[2] = x264_pixel_satd_4x4_lasx( p_dec, FDEC_STRIDE,
+                                               p_enc, FENC_STRIDE );
+}
+
+void x264_intra_satd_x3_16x16_lasx( uint8_t *p_enc, uint8_t *p_dec,
+                                    int32_t p_sad_array[3] )
+{
+    x264_intra_predict_vert_16x16_lasx( p_dec );
+    p_sad_array[0] = x264_pixel_satd_16x16_lasx( p_dec, FDEC_STRIDE,
+                                                 p_enc, FENC_STRIDE );
+
+    x264_intra_predict_hor_16x16_lasx( p_dec );
+    p_sad_array[1] = x264_pixel_satd_16x16_lasx( p_dec, FDEC_STRIDE,
+                                                 p_enc, FENC_STRIDE );
+
+    x264_intra_predict_dc_16x16_lasx( p_dec );
+    p_sad_array[2] = x264_pixel_satd_16x16_lasx( p_dec, FDEC_STRIDE,
+                                                 p_enc, FENC_STRIDE );
+}
+
+void x264_intra_satd_x3_8x8c_lasx( uint8_t *p_enc, uint8_t *p_dec,
+                                   int32_t p_sad_array[3] )
+{
+    x264_intra_predict_dc_4blk_8x8_lasx( p_dec );
+    p_sad_array[0] = x264_pixel_satd_8x8_lasx( p_dec, FDEC_STRIDE,
+                                               p_enc, FENC_STRIDE );
+
+    x264_intra_predict_hor_8x8_lasx( p_dec );
+    p_sad_array[1] = x264_pixel_satd_8x8_lasx( p_dec, FDEC_STRIDE,
+                                               p_enc, FENC_STRIDE );
+
+    x264_intra_predict_vert_8x8_lasx( p_dec );
+    p_sad_array[2] = x264_pixel_satd_8x8_lasx( p_dec, FDEC_STRIDE,
+                                               p_enc, FENC_STRIDE );
+}
+
+void x264_intra_sad_x3_4x4_lasx( uint8_t *p_enc, uint8_t *p_dec,
+                                 int32_t p_sad_array[3] )
+{
+    x264_intra_predict_vert_4x4_lasx( p_dec );
+    p_sad_array[0] = x264_pixel_sad_4x4_lasx( p_dec, FDEC_STRIDE,
+                                              p_enc, FENC_STRIDE );
+
+    x264_intra_predict_hor_4x4_lasx( p_dec );
+    p_sad_array[1] = x264_pixel_sad_4x4_lasx( p_dec, FDEC_STRIDE,
+                                              p_enc, FENC_STRIDE );
+
+    x264_intra_predict_dc_4x4_lasx( p_dec );
+    p_sad_array[2] = x264_pixel_sad_4x4_lasx( p_dec, FDEC_STRIDE,
+                                              p_enc, FENC_STRIDE );
+}
+
+void x264_intra_sad_x3_16x16_lasx( uint8_t *p_enc, uint8_t *p_dec,
+                                   int32_t p_sad_array[3] )
+{
+    x264_intra_predict_vert_16x16_lasx( p_dec );
+    p_sad_array[0] = x264_pixel_sad_16x16_lasx( p_dec, FDEC_STRIDE,
+                                                p_enc, FENC_STRIDE );
+
+    x264_intra_predict_hor_16x16_lasx( p_dec );
+    p_sad_array[1] = x264_pixel_sad_16x16_lasx( p_dec, FDEC_STRIDE,
+                                                p_enc, FENC_STRIDE );
+
+    x264_intra_predict_dc_16x16_lasx( p_dec );
+    p_sad_array[2] = x264_pixel_sad_16x16_lasx( p_dec, FDEC_STRIDE,
+                                                p_enc, FENC_STRIDE );
+}
+
+void x264_intra_sad_x3_8x8_lasx( uint8_t *p_enc, uint8_t p_edge[36],
+                                 int32_t p_sad_array[3] )
+{
+    ALIGNED_ARRAY_16( uint8_t, pix, [8 * FDEC_STRIDE] );
+
+    x264_intra_predict_v_8x8_lasx( pix, p_edge );
+    p_sad_array[0] = x264_pixel_sad_8x8_lasx( pix, FDEC_STRIDE,
+                                              p_enc, FENC_STRIDE );
+
+    x264_intra_predict_h_8x8_lasx( pix, p_edge );
+    p_sad_array[1] = x264_pixel_sad_8x8_lasx( pix, FDEC_STRIDE,
+                                              p_enc, FENC_STRIDE );
+
+    x264_intra_predict_dc_8x8_lasx( pix, p_edge );
+    p_sad_array[2] = x264_pixel_sad_8x8_lasx( pix, FDEC_STRIDE,
+                                              p_enc, FENC_STRIDE );
+}
+
+void x264_intra_sad_x3_8x8c_lasx( uint8_t *p_enc, uint8_t *p_dec,
+                                  int32_t p_sad_array[3] )
+{
+    x264_intra_predict_dc_4blk_8x8_lasx( p_dec );
+    p_sad_array[0] = x264_pixel_sad_8x8_lasx( p_dec, FDEC_STRIDE,
+                                              p_enc, FENC_STRIDE );
+
+    x264_intra_predict_hor_8x8_lasx( p_dec );
+    p_sad_array[1] = x264_pixel_sad_8x8_lasx( p_dec, FDEC_STRIDE,
+                                              p_enc, FENC_STRIDE );
+
+    x264_intra_predict_vert_8x8_lasx( p_dec );
+    p_sad_array[2] = x264_pixel_sad_8x8_lasx( p_dec, FDEC_STRIDE,
+                                              p_enc, FENC_STRIDE );
+}
+
+#define SSD_LOAD_8(_p_src, _stride, _stride2, _stride3, _stride4,                  \
+                   _src0, _src1, _src2, _src3, _src4, _src5, _src6, _src7)         \
+{                                                                                  \
+    _src0 = __lasx_xvld(_p_src, 0);                                                \
+    _src1 = __lasx_xvldx(_p_src, _stride);                                         \
+    _src2 = __lasx_xvldx(_p_src, _stride2);                                        \
+    _src3 = __lasx_xvldx(_p_src, _stride3);                                        \
+    _p_src += _stride4;                                                            \
+    _src4 = __lasx_xvld(_p_src, 0);                                                \
+    _src5 = __lasx_xvldx(_p_src, _stride);                                         \
+    _src6 = __lasx_xvldx(_p_src, _stride2);                                        \
+    _src7 = __lasx_xvldx(_p_src, _stride3);                                        \
+}
+
+#define SSD_INSERT_8(_src0, _src1, _src2, _src3, _src4, _src5, _src6, _src7,       \
+                     _ref0, _ref1, _ref2, _ref3, _ref4, _ref5, _ref6, _ref7)       \
+{                                                                                  \
+    _src0 = __lasx_xvpermi_q(_src0, _src1, 0x02);                                  \
+    _src2 = __lasx_xvpermi_q(_src2, _src3, 0x02);                                  \
+    _src4 = __lasx_xvpermi_q(_src4, _src5, 0x02);                                  \
+    _src6 = __lasx_xvpermi_q(_src6, _src7, 0x02);                                  \
+                                                                                   \
+    _ref0 = __lasx_xvpermi_q(_ref0, _ref1, 0x02);                                  \
+    _ref2 = __lasx_xvpermi_q(_ref2, _ref3, 0x02);                                  \
+    _ref4 = __lasx_xvpermi_q(_ref4, _ref5, 0x02);                                  \
+    _ref6 = __lasx_xvpermi_q(_ref6, _ref7, 0x02);                                  \
+}
+
+#define SSD_SUB_8(_src0, _src1, _src2, _src3, _src4, _src5, _src6, _src7,          \
+                  _ref0, _ref1, _ref2, _ref3, _ref4, _ref5, _ref6, _ref7)          \
+{                                                                                  \
+    _src1 = __lasx_xvsubwev_h_bu(_src0, _ref0);                                    \
+    _ref1 = __lasx_xvsubwod_h_bu(_src0, _ref0);                                    \
+    _src3 = __lasx_xvsubwev_h_bu(_src2, _ref2);                                    \
+    _ref3 = __lasx_xvsubwod_h_bu(_src2, _ref2);                                    \
+    _src5 = __lasx_xvsubwev_h_bu(_src4, _ref4);                                    \
+    _ref5 = __lasx_xvsubwod_h_bu(_src4, _ref4);                                    \
+    _src7 = __lasx_xvsubwev_h_bu(_src6, _ref6);                                    \
+    _ref7 = __lasx_xvsubwod_h_bu(_src6, _ref6);                                    \
+}
+
+
+int32_t x264_pixel_ssd_16x16_lasx( uint8_t *p_src, intptr_t i_src_stride,
+                                   uint8_t *p_ref, intptr_t i_ref_stride )
+{
+    uint32_t u_ssd;
+    intptr_t src_stride2 = i_src_stride << 1;
+    intptr_t ref_stride2 = i_ref_stride << 1;
+    intptr_t src_stride3 = i_src_stride + src_stride2;
+    intptr_t ref_stride3 = i_ref_stride + ref_stride2;
+    intptr_t src_stride4 = src_stride2 << 1;
+    intptr_t ref_stride4 = ref_stride2 << 1;
+    __m256i src0, src1, src2, src3, src4, src5, src6, src7;
+    __m256i src8, src9, src10, src11, src12, src13, src14, src15;
+    __m256i ref0, ref1, ref2, ref3, ref4, ref5, ref6, ref7;
+    __m256i ref8, ref9, ref10, ref11, ref12, ref13, ref14, ref15;
+
+    SSD_LOAD_8(p_src, i_src_stride, src_stride2, src_stride3, src_stride4,
+               src0, src1, src2, src3, src4, src5, src6, src7);
+    p_src += src_stride4;
+    SSD_LOAD_8(p_src, i_src_stride, src_stride2, src_stride3, src_stride4,
+               src8, src9, src10, src11, src12, src13, src14, src15);
+
+    SSD_LOAD_8(p_ref, i_ref_stride, ref_stride2, ref_stride3, ref_stride4,
+               ref0, ref1, ref2, ref3, ref4, ref5, ref6, ref7);
+    p_ref += ref_stride4;
+    SSD_LOAD_8(p_ref, i_ref_stride, ref_stride2, ref_stride3, ref_stride4,
+               ref8, ref9, ref10, ref11, ref12, ref13, ref14, ref15);
+
+    SSD_INSERT_8(src0, src1, src2, src3, src4, src5, src6, src7,
+                 ref0, ref1, ref2, ref3, ref4, ref5, ref6, ref7);
+    SSD_INSERT_8(src8, src9, src10, src11, src12, src13, src14, src15,
+                 ref8, ref9, ref10, ref11, ref12, ref13, ref14, ref15);
+
+    SSD_SUB_8(src0, src1, src2, src3, src4, src5, src6, src7,
+              ref0, ref1, ref2, ref3, ref4, ref5, ref6, ref7);
+    SSD_SUB_8(src8, src9, src10, src11, src12, src13, src14, src15,
+              ref8, ref9, ref10, ref11, ref12, ref13, ref14, ref15);
+
+    src0 = __lasx_xvmulwev_w_h(src1, src1);
+    src0 = __lasx_xvmaddwod_w_h(src0, src1, src1);
+    src0 = __lasx_xvmaddwev_w_h(src0, ref1, ref1);
+    src0 = __lasx_xvmaddwod_w_h(src0, ref1, ref1);
+    src0 = __lasx_xvmaddwev_w_h(src0, src3, src3);
+    src0 = __lasx_xvmaddwod_w_h(src0, src3, src3);
+    src0 = __lasx_xvmaddwev_w_h(src0, ref3, ref3);
+    src0 = __lasx_xvmaddwod_w_h(src0, ref3, ref3);
+    src0 = __lasx_xvmaddwev_w_h(src0, src5, src5);
+    src0 = __lasx_xvmaddwod_w_h(src0, src5, src5);
+    src0 = __lasx_xvmaddwev_w_h(src0, ref5, ref5);
+    src0 = __lasx_xvmaddwod_w_h(src0, ref5, ref5);
+    src0 = __lasx_xvmaddwev_w_h(src0, src7, src7);
+    src0 = __lasx_xvmaddwod_w_h(src0, src7, src7);
+    src0 = __lasx_xvmaddwev_w_h(src0, ref7, ref7);
+    src0 = __lasx_xvmaddwod_w_h(src0, ref7, ref7);
+
+    src0 = __lasx_xvmaddwev_w_h(src0, src9, src9);
+    src0 = __lasx_xvmaddwod_w_h(src0, src9, src9);
+    src0 = __lasx_xvmaddwev_w_h(src0, ref9, ref9);
+    src0 = __lasx_xvmaddwod_w_h(src0, ref9, ref9);
+    src0 = __lasx_xvmaddwev_w_h(src0, src11, src11);
+    src0 = __lasx_xvmaddwod_w_h(src0, src11, src11);
+    src0 = __lasx_xvmaddwev_w_h(src0, ref11, ref11);
+    src0 = __lasx_xvmaddwod_w_h(src0, ref11, ref11);
+    src0 = __lasx_xvmaddwev_w_h(src0, src13, src13);
+    src0 = __lasx_xvmaddwod_w_h(src0, src13, src13);
+    src0 = __lasx_xvmaddwev_w_h(src0, ref13, ref13);
+    src0 = __lasx_xvmaddwod_w_h(src0, ref13, ref13);
+    src0 = __lasx_xvmaddwev_w_h(src0, src15, src15);
+    src0 = __lasx_xvmaddwod_w_h(src0, src15, src15);
+    src0 = __lasx_xvmaddwev_w_h(src0, ref15, ref15);
+    src0 = __lasx_xvmaddwod_w_h(src0, ref15, ref15);
+
+    ref0 = __lasx_xvhaddw_d_w(src0, src0);
+    ref0 = __lasx_xvhaddw_q_d(ref0, ref0);
+    u_ssd = __lasx_xvpickve2gr_w(ref0, 0) + __lasx_xvpickve2gr_w(ref0, 4);
+
+    return u_ssd;
+}
+
+int32_t x264_pixel_ssd_16x8_lasx( uint8_t *p_src, intptr_t i_src_stride,
+                                  uint8_t *p_ref, intptr_t i_ref_stride )
+{
+    uint32_t u_ssd;
+    intptr_t src_stride2 = i_src_stride << 1;
+    intptr_t ref_stride2 = i_ref_stride << 1;
+    intptr_t src_stride3 = i_src_stride + src_stride2;
+    intptr_t ref_stride3 = i_ref_stride + ref_stride2;
+    intptr_t src_stride4 = src_stride2 << 1;
+    intptr_t ref_stride4 = ref_stride2 << 1;
+    __m256i src0, src1, src2, src3, src4, src5, src6, src7;
+    __m256i ref0, ref1, ref2, ref3, ref4, ref5, ref6, ref7;
+
+    SSD_LOAD_8(p_src, i_src_stride, src_stride2, src_stride3, src_stride4,
+               src0, src1, src2, src3, src4, src5, src6, src7);
+
+    SSD_LOAD_8(p_ref, i_ref_stride, ref_stride2, ref_stride3, ref_stride4,
+               ref0, ref1, ref2, ref3, ref4, ref5, ref6, ref7);
+
+    SSD_INSERT_8(src0, src1, src2, src3, src4, src5, src6, src7,
+                 ref0, ref1, ref2, ref3, ref4, ref5, ref6, ref7);
+
+    SSD_SUB_8(src0, src1, src2, src3, src4, src5, src6, src7,
+              ref0, ref1, ref2, ref3, ref4, ref5, ref6, ref7);
+
+    src0 = __lasx_xvmulwev_w_h(src1, src1);
+    src0 = __lasx_xvmaddwod_w_h(src0, src1, src1);
+    src0 = __lasx_xvmaddwev_w_h(src0, ref1, ref1);
+    src0 = __lasx_xvmaddwod_w_h(src0, ref1, ref1);
+    src0 = __lasx_xvmaddwev_w_h(src0, src3, src3);
+    src0 = __lasx_xvmaddwod_w_h(src0, src3, src3);
+    src0 = __lasx_xvmaddwev_w_h(src0, ref3, ref3);
+    src0 = __lasx_xvmaddwod_w_h(src0, ref3, ref3);
+    src0 = __lasx_xvmaddwev_w_h(src0, src5, src5);
+    src0 = __lasx_xvmaddwod_w_h(src0, src5, src5);
+    src0 = __lasx_xvmaddwev_w_h(src0, ref5, ref5);
+    src0 = __lasx_xvmaddwod_w_h(src0, ref5, ref5);
+    src0 = __lasx_xvmaddwev_w_h(src0, src7, src7);
+    src0 = __lasx_xvmaddwod_w_h(src0, src7, src7);
+    src0 = __lasx_xvmaddwev_w_h(src0, ref7, ref7);
+    src0 = __lasx_xvmaddwod_w_h(src0, ref7, ref7);
+
+    ref0 = __lasx_xvhaddw_d_w(src0, src0);
+    ref0 = __lasx_xvhaddw_q_d(ref0, ref0);
+    u_ssd = __lasx_xvpickve2gr_w(ref0, 0) + __lasx_xvpickve2gr_w(ref0, 4);
+
+    return u_ssd;
+}
+
+#undef SSD_LOAD_8
+#undef SSD_INSERT_8
+#undef SSD_SUB_8
+
+#define SSD_LOAD_8(_p_src, _src_stride, _src0, _src1, _src2,                       \
+                   _src3, _src4, _src5, _src6, _src7)                              \
+{                                                                                  \
+    _src0 = __lasx_xvldrepl_d(_p_src, 0 );                                         \
+    _p_src += _src_stride;                                                         \
+    _src1 = __lasx_xvldrepl_d(_p_src, 0 );                                         \
+    _p_src += _src_stride;                                                         \
+    _src2 = __lasx_xvldrepl_d(_p_src, 0 );                                         \
+    _p_src += _src_stride;                                                         \
+    _src3 = __lasx_xvldrepl_d(_p_src, 0 );                                         \
+    _p_src += _src_stride;                                                         \
+    _src4 = __lasx_xvldrepl_d(_p_src, 0 );                                         \
+    _p_src += _src_stride;                                                         \
+    _src5 = __lasx_xvldrepl_d(_p_src, 0 );                                         \
+    _p_src += _src_stride;                                                         \
+    _src6 = __lasx_xvldrepl_d(_p_src, 0 );                                         \
+    _p_src += _src_stride;                                                         \
+    _src7 = __lasx_xvldrepl_d(_p_src, 0 );                                         \
+}
+
+#define SSD_INSERT_8(_src0, _src1, _src2, _src3, _src4, _src5, _src6, _src7,       \
+                     _ref0, _ref1, _ref2, _ref3, _ref4, _ref5, _ref6, _ref7)       \
+{                                                                                  \
+    _src0 = __lasx_xvilvl_b(_src0, _ref0);                                         \
+    _src1 = __lasx_xvilvl_b(_src1, _ref1);                                         \
+    _src2 = __lasx_xvilvl_b(_src2, _ref2);                                         \
+    _src3 = __lasx_xvilvl_b(_src3, _ref3);                                         \
+    _src4 = __lasx_xvilvl_b(_src4, _ref4);                                         \
+    _src5 = __lasx_xvilvl_b(_src5, _ref5);                                         \
+    _src6 = __lasx_xvilvl_b(_src6, _ref6);                                         \
+    _src7 = __lasx_xvilvl_b(_src7, _ref7);                                         \
+}
+
+int32_t x264_pixel_ssd_8x16_lasx( uint8_t *p_src, intptr_t i_src_stride,
+                                  uint8_t *p_ref, intptr_t i_ref_stride )
+{
+    uint32_t u_ssd;
+    __m256i src0, src1, src2, src3, src4, src5, src6, src7;
+    __m256i src8, src9, src10, src11, src12, src13, src14, src15;
+    __m256i ref0, ref1, ref2, ref3, ref4, ref5, ref6, ref7;
+    __m256i ref8, ref9, ref10, ref11, ref12, ref13, ref14, ref15;
+
+    SSD_LOAD_8(p_src, i_src_stride, src0, src1, src2, src3,
+               src4, src5, src6, src7);
+    p_src += i_src_stride;
+    SSD_LOAD_8(p_src, i_src_stride, src8, src9, src10, src11,
+               src12, src13, src14, src15);
+    SSD_LOAD_8(p_ref, i_ref_stride, ref0, ref1, ref2, ref3,
+               ref4, ref5, ref6, ref7);
+    p_ref += i_ref_stride;
+    SSD_LOAD_8(p_ref, i_ref_stride, ref8, ref9, ref10, ref11,
+               ref12, ref13, ref14, ref15);
+
+    SSD_INSERT_8(src0, src1, src2, src3, src4, src5, src6, src7,
+                 ref0, ref1, ref2, ref3, ref4, ref5, ref6, ref7);
+    SSD_INSERT_8(src8, src9, src10, src11, src12, src13, src14, src15,
+                 ref8, ref9, ref10, ref11, ref12, ref13, ref14, ref15);
+
+    src0 = __lasx_xvpermi_q(src0, src1, 0x02);
+    src2 = __lasx_xvpermi_q(src2, src3, 0x02);
+    src4 = __lasx_xvpermi_q(src4, src5, 0x02);
+    src6 = __lasx_xvpermi_q(src6, src7, 0x02);
+    src8 = __lasx_xvpermi_q(src8, src9, 0x02);
+    src10 = __lasx_xvpermi_q(src10, src11, 0x02);
+    src12 = __lasx_xvpermi_q(src12, src13, 0x02);
+    src14 = __lasx_xvpermi_q(src14, src15, 0x02);
+    ref0 = __lasx_xvhsubw_hu_bu(src0, src0);
+    ref2 = __lasx_xvhsubw_hu_bu(src2, src2);
+    ref4 = __lasx_xvhsubw_hu_bu(src4, src4);
+    ref6 = __lasx_xvhsubw_hu_bu(src6, src6);
+    ref8 = __lasx_xvhsubw_hu_bu(src8, src8);
+    ref10 = __lasx_xvhsubw_hu_bu(src10, src10);
+    ref12 = __lasx_xvhsubw_hu_bu(src12, src12);
+    ref14 = __lasx_xvhsubw_hu_bu(src14, src14);
+    src0 = __lasx_xvmulwev_w_h(ref0, ref0);
+    src0 = __lasx_xvmaddwod_w_h(src0, ref0, ref0);
+    src0 = __lasx_xvmaddwev_w_h(src0, ref2, ref2);
+    src0 = __lasx_xvmaddwod_w_h(src0, ref2, ref2);
+    src0 = __lasx_xvmaddwev_w_h(src0, ref4, ref4);
+    src0 = __lasx_xvmaddwod_w_h(src0, ref4, ref4);
+    src0 = __lasx_xvmaddwev_w_h(src0, ref6, ref6);
+    src0 = __lasx_xvmaddwod_w_h(src0, ref6, ref6);
+    src0 = __lasx_xvmaddwev_w_h(src0, ref8, ref8);
+    src0 = __lasx_xvmaddwod_w_h(src0, ref8, ref8);
+    src0 = __lasx_xvmaddwev_w_h(src0, ref10, ref10);
+    src0 = __lasx_xvmaddwod_w_h(src0, ref10, ref10);
+    src0 = __lasx_xvmaddwev_w_h(src0, ref12, ref12);
+    src0 = __lasx_xvmaddwod_w_h(src0, ref12, ref12);
+    src0 = __lasx_xvmaddwev_w_h(src0, ref14, ref14);
+    src0 = __lasx_xvmaddwod_w_h(src0, ref14, ref14);
+    ref0 = __lasx_xvhaddw_d_w(src0, src0);
+    ref0 = __lasx_xvhaddw_q_d(ref0, ref0);
+    u_ssd = __lasx_xvpickve2gr_w(ref0, 0) + __lasx_xvpickve2gr_w(ref0, 4);
+
+    return u_ssd;
+}
+
+int32_t x264_pixel_ssd_8x8_lasx( uint8_t *p_src, intptr_t i_src_stride,
+                                 uint8_t *p_ref, intptr_t i_ref_stride )
+{
+    uint32_t u_ssd;
+    __m256i src0, src1, src2, src3, src4, src5, src6, src7;
+    __m256i ref0, ref1, ref2, ref3, ref4, ref5, ref6, ref7;
+
+    SSD_LOAD_8(p_src, i_src_stride, src0, src1, src2, src3,
+               src4, src5, src6, src7);
+    SSD_LOAD_8(p_ref, i_ref_stride, ref0, ref1, ref2, ref3,
+               ref4, ref5, ref6, ref7);
+
+    SSD_INSERT_8(src0, src1, src2, src3, src4, src5, src6, src7,
+                 ref0, ref1, ref2, ref3, ref4, ref5, ref6, ref7);
+
+    src0 = __lasx_xvpermi_q(src0, src1, 0x02);
+    src2 = __lasx_xvpermi_q(src2, src3, 0x02);
+    src4 = __lasx_xvpermi_q(src4, src5, 0x02);
+    src6 = __lasx_xvpermi_q(src6, src7, 0x02);
+
+    ref0 = __lasx_xvhsubw_hu_bu(src0, src0);
+    ref2 = __lasx_xvhsubw_hu_bu(src2, src2);
+    ref4 = __lasx_xvhsubw_hu_bu(src4, src4);
+    ref6 = __lasx_xvhsubw_hu_bu(src6, src6);
+    src0 = __lasx_xvmulwev_w_h(ref0, ref0);
+    src0 = __lasx_xvmaddwod_w_h(src0, ref0, ref0);
+    src0 = __lasx_xvmaddwev_w_h(src0, ref2, ref2);
+    src0 = __lasx_xvmaddwod_w_h(src0, ref2, ref2);
+    src0 = __lasx_xvmaddwev_w_h(src0, ref4, ref4);
+    src0 = __lasx_xvmaddwod_w_h(src0, ref4, ref4);
+    src0 = __lasx_xvmaddwev_w_h(src0, ref6, ref6);
+    src0 = __lasx_xvmaddwod_w_h(src0, ref6, ref6);
+    ref0 = __lasx_xvhaddw_d_w(src0, src0);
+    ref0 = __lasx_xvhaddw_q_d(ref0, ref0);
+    u_ssd = __lasx_xvpickve2gr_w(ref0, 0) + __lasx_xvpickve2gr_w(ref0, 4);
+
+    return u_ssd;
+}
+
+int32_t x264_pixel_ssd_8x4_lasx( uint8_t *p_src, intptr_t i_src_stride,
+                                 uint8_t *p_ref, intptr_t i_ref_stride )
+{
+    uint32_t u_ssd;
+    __m256i src0, src1, src2, src3;
+    __m256i ref0, ref1, ref2, ref3;
+
+    src0 = __lasx_xvldrepl_d( p_src, 0 );
+    p_src += i_src_stride;
+    src1 = __lasx_xvldrepl_d( p_src, 0 );
+    p_src += i_src_stride;
+    src2 = __lasx_xvldrepl_d( p_src, 0 );
+    p_src += i_src_stride;
+    src3 = __lasx_xvldrepl_d( p_src, 0 );
+
+    ref0 = __lasx_xvldrepl_d( p_ref, 0 );
+    p_ref += i_ref_stride;
+    ref1 = __lasx_xvldrepl_d( p_ref, 0 );
+    p_ref += i_ref_stride;
+    ref2 = __lasx_xvldrepl_d( p_ref, 0 );
+    p_ref += i_ref_stride;
+    ref3 = __lasx_xvldrepl_d( p_ref, 0 );
+
+    src0 = __lasx_xvilvl_b(src0, ref0);
+    src1 = __lasx_xvilvl_b(src1, ref1);
+    src2 = __lasx_xvilvl_b(src2, ref2);
+    src3 = __lasx_xvilvl_b(src3, ref3);
+    src0 = __lasx_xvpermi_q(src0, src1, 0x02);
+    src2 = __lasx_xvpermi_q(src2, src3, 0x02);
+    ref0 = __lasx_xvhsubw_hu_bu(src0, src0);
+    ref2 = __lasx_xvhsubw_hu_bu(src2, src2);
+    src0 = __lasx_xvmulwev_w_h(ref0, ref0);
+    src0 = __lasx_xvmaddwod_w_h(src0, ref0, ref0);
+    src0 = __lasx_xvmaddwev_w_h(src0, ref2, ref2);
+    src0 = __lasx_xvmaddwod_w_h(src0, ref2, ref2);
+    ref0 = __lasx_xvhaddw_d_w(src0, src0);
+    ref0 = __lasx_xvhaddw_q_d(ref0, ref0);
+    u_ssd = __lasx_xvpickve2gr_w(ref0, 0) + __lasx_xvpickve2gr_w(ref0, 4);
+
+    return u_ssd;
+}
+
+#undef SSD_LOAD_8
+
+#define SSD_LOAD_8(_p_src, _src_stride, _src0, _src1, _src2,                       \
+                   _src3, _src4, _src5, _src6, _src7)                              \
+{                                                                                  \
+    _src0 = __lasx_xvldrepl_w(_p_src, 0 );                                         \
+    _p_src += _src_stride;                                                         \
+    _src1 = __lasx_xvldrepl_w(_p_src, 0 );                                         \
+    _p_src += _src_stride;                                                         \
+    _src2 = __lasx_xvldrepl_w(_p_src, 0 );                                         \
+    _p_src += _src_stride;                                                         \
+    _src3 = __lasx_xvldrepl_w(_p_src, 0 );                                         \
+    _p_src += _src_stride;                                                         \
+    _src4 = __lasx_xvldrepl_w(_p_src, 0 );                                         \
+    _p_src += _src_stride;                                                         \
+    _src5 = __lasx_xvldrepl_w(_p_src, 0 );                                         \
+    _p_src += _src_stride;                                                         \
+    _src6 = __lasx_xvldrepl_w(_p_src, 0 );                                         \
+    _p_src += _src_stride;                                                         \
+    _src7 = __lasx_xvldrepl_w(_p_src, 0 );                                         \
+}
+
+int32_t x264_pixel_ssd_4x16_lasx( uint8_t *p_src, intptr_t i_src_stride,
+                                  uint8_t *p_ref, intptr_t i_ref_stride )
+{
+    uint32_t u_ssd;
+    __m256i src0, src1, src2, src3, src4, src5, src6, src7;
+    __m256i src8, src9, src10, src11, src12, src13, src14, src15;
+    __m256i ref0, ref1, ref2, ref3, ref4, ref5, ref6, ref7;
+    __m256i ref8, ref9, ref10, ref11, ref12, ref13, ref14, ref15;
+
+    SSD_LOAD_8(p_src, i_src_stride, src0, src1, src2, src3,
+               src4, src5, src6, src7);
+    p_src += i_src_stride;
+    SSD_LOAD_8(p_src, i_src_stride, src8, src9, src10, src11,
+               src12, src13, src14, src15);
+    SSD_LOAD_8(p_ref, i_ref_stride, ref0, ref1, ref2, ref3,
+               ref4, ref5, ref6, ref7);
+    p_ref += i_ref_stride;
+    SSD_LOAD_8(p_ref, i_ref_stride, ref8, ref9, ref10, ref11,
+               ref12, ref13, ref14, ref15);
+
+    SSD_INSERT_8(src0, src1, src2, src3, src4, src5, src6, src7,
+                 ref0, ref1, ref2, ref3, ref4, ref5, ref6, ref7);
+    SSD_INSERT_8(src8, src9, src10, src11, src12, src13, src14, src15,
+                 ref8, ref9, ref10, ref11, ref12, ref13, ref14, ref15);
+
+    src0 = __lasx_xvilvl_d(src1, src0);
+    src2 = __lasx_xvilvl_d(src3, src2);
+    src4 = __lasx_xvilvl_d(src5, src4);
+    src6 = __lasx_xvilvl_d(src7, src6);
+    src0 = __lasx_xvpermi_q(src0, src2, 0x02);
+    src4 = __lasx_xvpermi_q(src4, src6, 0x02);
+
+    src1 = __lasx_xvilvl_d(src9, src8);
+    src3 = __lasx_xvilvl_d(src11, src10);
+    src5 = __lasx_xvilvl_d(src13, src12);
+    src7 = __lasx_xvilvl_d(src15, src14);
+    src1 = __lasx_xvpermi_q(src1, src3, 0x02);
+    src5 = __lasx_xvpermi_q(src5, src7, 0x02);
+
+    ref0 = __lasx_xvhsubw_hu_bu(src0, src0);
+    ref4 = __lasx_xvhsubw_hu_bu(src4, src4);
+    src0 = __lasx_xvmulwev_w_h(ref0, ref0);
+    ref0 = __lasx_xvmaddwod_w_h(src0, ref0, ref0);
+    src4 = __lasx_xvmaddwev_w_h(ref0, ref4, ref4);
+    ref4 = __lasx_xvmaddwod_w_h(src4, ref4, ref4);
+
+    ref1 = __lasx_xvhsubw_hu_bu(src1, src1);
+    ref5 = __lasx_xvhsubw_hu_bu(src5, src5);
+    src1 = __lasx_xvmaddwev_w_h(ref4, ref1, ref1);
+    src1 = __lasx_xvmaddwod_w_h(src1, ref1, ref1);
+    src1 = __lasx_xvmaddwev_w_h(src1, ref5, ref5);
+    src1 = __lasx_xvmaddwod_w_h(src1, ref5, ref5);
+    ref4 = __lasx_xvhaddw_d_w(src1, src1);
+    ref4 = __lasx_xvhaddw_q_d(ref4, ref4);
+    u_ssd = __lasx_xvpickve2gr_w(ref4, 0) + __lasx_xvpickve2gr_w(ref4, 4);
+
+    return u_ssd;
+}
+
+int32_t x264_pixel_ssd_4x8_lasx( uint8_t *p_src, intptr_t i_src_stride,
+                                 uint8_t *p_ref, intptr_t i_ref_stride )
+{
+    uint32_t u_ssd;
+    __m256i src0, src1, src2, src3, src4, src5, src6, src7;
+    __m256i ref0, ref1, ref2, ref3, ref4, ref5, ref6, ref7;
+
+    SSD_LOAD_8(p_src, i_src_stride, src0, src1, src2, src3,
+               src4, src5, src6, src7);
+    SSD_LOAD_8(p_ref, i_ref_stride, ref0, ref1, ref2, ref3,
+               ref4, ref5, ref6, ref7);
+
+    SSD_INSERT_8(src0, src1, src2, src3, src4, src5, src6, src7,
+                 ref0, ref1, ref2, ref3, ref4, ref5, ref6, ref7);
+
+    src0 = __lasx_xvilvl_d(src1, src0);
+    src2 = __lasx_xvilvl_d(src3, src2);
+    src4 = __lasx_xvilvl_d(src5, src4);
+    src6 = __lasx_xvilvl_d(src7, src6);
+    src0 = __lasx_xvpermi_q(src0, src2, 0x02);
+    src4 = __lasx_xvpermi_q(src4, src6, 0x02);
+
+    ref0 = __lasx_xvhsubw_hu_bu(src0, src0);
+    ref4 = __lasx_xvhsubw_hu_bu(src4, src4);
+    src0 = __lasx_xvmulwev_w_h(ref0, ref0);
+    src0 = __lasx_xvmaddwod_w_h(src0, ref0, ref0);
+    src0 = __lasx_xvmaddwev_w_h(src0, ref4, ref4);
+    src0 = __lasx_xvmaddwod_w_h(src0, ref4, ref4);
+    ref4 = __lasx_xvhaddw_d_w(src0, src0);
+    ref4 = __lasx_xvhaddw_q_d(ref4, ref4);
+    u_ssd = __lasx_xvpickve2gr_w(ref4, 0) + __lasx_xvpickve2gr_w(ref4, 4);
+
+    return u_ssd;
+}
+
+int32_t x264_pixel_ssd_4x4_lasx( uint8_t *p_src, intptr_t i_src_stride,
+                                 uint8_t *p_ref, intptr_t i_ref_stride )
+{
+    uint32_t u_ssd;
+    __m256i src0, src1, src2, src3;
+    __m256i ref0, ref1, ref2, ref3;
+
+    src0 = __lasx_xvldrepl_w( p_src, 0 );
+    p_src += i_src_stride;
+    src1 = __lasx_xvldrepl_w( p_src, 0 );
+    p_src += i_src_stride;
+    src2 = __lasx_xvldrepl_w( p_src, 0 );
+    p_src += i_src_stride;
+    src3 = __lasx_xvldrepl_w( p_src, 0 );
+
+    ref0 = __lasx_xvldrepl_w( p_ref, 0 );
+    p_ref += i_ref_stride;
+    ref1 = __lasx_xvldrepl_w( p_ref, 0 );
+    p_ref += i_ref_stride;
+    ref2 = __lasx_xvldrepl_w( p_ref, 0 );
+    p_ref += i_ref_stride;
+    ref3 = __lasx_xvldrepl_w( p_ref, 0 );
+
+    src0 = __lasx_xvilvl_b(src0, ref0);
+    src1 = __lasx_xvilvl_b(src1, ref1);
+    src2 = __lasx_xvilvl_b(src2, ref2);
+    src3 = __lasx_xvilvl_b(src3, ref3);
+    src0 = __lasx_xvilvl_d(src1, src0);
+    src2 = __lasx_xvilvl_d(src3, src2);
+    src0 = __lasx_xvpermi_q(src0, src2, 0x02);
+    ref0 = __lasx_xvhsubw_hu_bu(src0, src0);
+    src0 = __lasx_xvmulwev_w_h(ref0, ref0);
+    src0 = __lasx_xvmaddwod_w_h(src0, ref0, ref0);
+    ref0 = __lasx_xvhaddw_d_w(src0, src0);
+    ref0 = __lasx_xvhaddw_q_d(ref0, ref0);
+    u_ssd = __lasx_xvpickve2gr_w(ref0, 0) + __lasx_xvpickve2gr_w(ref0, 4);
+
+    return u_ssd;
+}
+#undef SSD_LOAD_8
+#undef SSD_INSERT_8
+
+#define LASX_CALC_MSE_AVG_B( src, ref, var, sub )                          \
+{                                                                          \
+    __m256i src_l0_m, src_l1_m;                                            \
+    __m256i res_l0_m, res_l1_m;                                            \
+                                                                           \
+    src_l1_m = __lasx_xvilvl_b( src, ref );                                \
+    src_l0_m = __lasx_xvilvh_b( src, ref );                                \
+    DUP2_ARG2( __lasx_xvhsubw_hu_bu, src_l0_m, src_l0_m, src_l1_m,         \
+               src_l1_m, res_l0_m, res_l1_m );                             \
+    DUP2_ARG3( __lasx_xvdp2add_w_h, var, res_l0_m, res_l0_m,               \
+               var, res_l1_m, res_l1_m, var, var );                        \
+                                                                           \
+    res_l0_m = __lasx_xvadd_h( res_l0_m, res_l1_m );                       \
+    sub = __lasx_xvadd_h( sub, res_l0_m );                                 \
+}
+
+#define VARIANCE_WxH( sse, diff, shift )                                \
+    ( ( sse ) - ( ( ( uint32_t )( diff ) * ( diff ) ) >> ( shift ) ) )
+
+static inline uint32_t sse_diff_8width_lasx( uint8_t *p_src,
+                                             int32_t i_src_stride,
+                                             uint8_t *p_ref,
+                                             int32_t i_ref_stride,
+                                             int32_t i_height,
+                                             int32_t *p_diff )
+{
+    int32_t i_ht_cnt;
+    uint32_t u_sse;
+    __m256i src0, src1, src2, src3;
+    __m256i ref0, ref1, ref2, ref3;
+    __m256i avg = __lasx_xvldi( 0 );
+    __m256i var = __lasx_xvldi( 0 );
+
+    for( i_ht_cnt = ( i_height >> 2 ); i_ht_cnt--; )
+    {
+        src0 = __lasx_xvldrepl_d( p_src, 0 );
+        p_src += i_src_stride;
+        src1 = __lasx_xvldrepl_d( p_src, 0 );
+        p_src += i_src_stride;
+        src2 = __lasx_xvldrepl_d( p_src, 0 );
+        p_src += i_src_stride;
+        src3 = __lasx_xvldrepl_d( p_src, 0 );
+        p_src += i_src_stride;
+
+        ref0 = __lasx_xvldrepl_d( p_ref, 0 );
+        p_ref += i_ref_stride;
+        ref1 = __lasx_xvldrepl_d( p_ref, 0 );
+        p_ref += i_ref_stride;
+        ref2 = __lasx_xvldrepl_d( p_ref, 0 );
+        p_ref += i_ref_stride;
+        ref3 = __lasx_xvldrepl_d( p_ref, 0 );
+        p_ref += i_ref_stride;
+
+        DUP4_ARG2( __lasx_xvpickev_d, src1, src0, src3, src2, ref1, ref0, ref3, ref2,
+                   src0, src1, ref0, ref1 );
+        src0 = __lasx_xvpermi_q( src1, src0, 0x20 );
+        ref0 = __lasx_xvpermi_q( ref1, ref0, 0x20 );
+        LASX_CALC_MSE_AVG_B( src0, ref0, var, avg );
+    }
+
+    avg = __lasx_xvhaddw_w_h( avg, avg );
+    *p_diff = LASX_HADD_SW_S32( avg );
+    u_sse = LASX_HADD_SW_S32( var );
+
+    return u_sse;
+}
+
+static uint64_t avc_pixel_var16width_lasx( uint8_t *p_pix, int32_t i_stride,
+                                           uint8_t i_height )
+{
+    uint32_t u_sum = 0, u_sqr_out = 0, u_cnt;
+    int32_t i_stride_x2 = i_stride << 1;
+    int32_t i_stride_x3 = i_stride_x2 + i_stride;
+    int32_t i_stride_x4 = i_stride << 2;
+    int32_t i_stride_x5 = i_stride_x4 + i_stride;
+    int32_t i_stride_x6 = i_stride_x4 + i_stride_x2;
+    int32_t i_stride_x7 = i_stride_x4 + i_stride_x3;
+    __m256i pix0, pix1, pix2, pix3, pix4, pix5, pix6, pix7;
+    __m256i zero = __lasx_xvldi( 0 );
+    __m256i add, pix_h, pix_l;
+    __m256i sqr = __lasx_xvldi( 0 );
+
+#define LASX_PIXEL_VAR_16W( src0, src1 )                       \
+    src0 = __lasx_xvpermi_q( src1, src0, 0x20 );               \
+    add = __lasx_xvhaddw_hu_bu( src0, src0 );                  \
+    u_sum += LASX_HADD_UH_U32( add );                          \
+    pix_h =__lasx_xvilvl_b( zero, src0 );                      \
+    pix_l =__lasx_xvilvh_b( zero, src0 );                      \
+    DUP2_ARG3( __lasx_xvdp2add_w_h, sqr, pix_h, pix_h,         \
+               sqr, pix_l, pix_l, sqr, sqr );
+
+    for( u_cnt = ( i_height >> 3 ); u_cnt--; )
+    {
+        DUP4_ARG2( __lasx_xvldx, p_pix, 0, p_pix, i_stride, p_pix, i_stride_x2, p_pix,
+                   i_stride_x3, pix0, pix1, pix2, pix3 );
+        DUP4_ARG2( __lasx_xvldx, p_pix, i_stride_x4, p_pix, i_stride_x5, p_pix,
+                   i_stride_x6, p_pix, i_stride_x7, pix4, pix5, pix6, pix7 );
+        p_pix += ( i_stride << 3 );
+
+        LASX_PIXEL_VAR_16W( pix0, pix1 );
+        LASX_PIXEL_VAR_16W( pix2, pix3 );
+        LASX_PIXEL_VAR_16W( pix4, pix5 );
+        LASX_PIXEL_VAR_16W( pix6, pix7 );
+    }
+
+    u_sqr_out = LASX_HADD_SW_S32( sqr );
+
+#undef LASX_PIXEL_VAR_16W
+
+    return ( u_sum + ( ( uint64_t ) u_sqr_out << 32 ) );
+}
+
+static uint64_t avc_pixel_var8width_lasx( uint8_t *p_pix, int32_t i_stride,
+                                          uint8_t i_height )
+{
+    uint32_t u_sum = 0, u_sqr_out = 0, u_cnt;
+    __m256i pix0, pix1, pix2, pix3, pix4, pix5, pix6, pix7;
+    __m256i zero = __lasx_xvldi( 0 );
+    __m256i add, pix_h, pix_l;
+    __m256i sqr = __lasx_xvldi( 0 );
+
+#define LASX_PIXEL_VAR_8W( src0, src1, src2, src3 )            \
+    src0 = __lasx_xvpickev_d( src1, src0 );                    \
+    src1 = __lasx_xvpickev_d( src3, src2 );                    \
+    src0 = __lasx_xvpermi_q( src1, src0, 0x20 );               \
+    add = __lasx_xvhaddw_hu_bu( src0, src0 );                  \
+    u_sum += LASX_HADD_UH_U32( add );                          \
+    pix_h = __lasx_xvilvl_b( zero, src0 );                     \
+    pix_l = __lasx_xvilvh_b( zero, src0 );                     \
+    DUP2_ARG3( __lasx_xvdp2add_w_h, sqr, pix_h, pix_h,         \
+               sqr, pix_l, pix_l, sqr, sqr );
+
+    for( u_cnt = ( i_height >> 3 ); u_cnt--; )
+    {
+        pix0 = __lasx_xvldrepl_d( p_pix, 0 );
+        p_pix += i_stride;
+        pix1 = __lasx_xvldrepl_d( p_pix, 0 );
+        p_pix += i_stride;
+        pix2 = __lasx_xvldrepl_d( p_pix, 0 );
+        p_pix += i_stride;
+        pix3 = __lasx_xvldrepl_d( p_pix, 0 );
+        p_pix += i_stride;
+        pix4 = __lasx_xvldrepl_d( p_pix, 0 );
+        p_pix += i_stride;
+        pix5 = __lasx_xvldrepl_d( p_pix, 0 );
+        p_pix += i_stride;
+        pix6 = __lasx_xvldrepl_d( p_pix, 0 );
+        p_pix += i_stride;
+        pix7 = __lasx_xvldrepl_d( p_pix, 0 );
+        p_pix += i_stride;
+
+        LASX_PIXEL_VAR_8W( pix0, pix1, pix2, pix3 );
+        LASX_PIXEL_VAR_8W( pix4, pix5, pix6, pix7 );
+    }
+
+    u_sqr_out = LASX_HADD_SW_S32( sqr );
+
+#undef LASX_PIXEL_VAR_8W
+
+    return ( u_sum + ( ( uint64_t ) u_sqr_out << 32 ) );
+}
+
+uint64_t x264_pixel_var_16x16_lasx( uint8_t *p_pix, intptr_t i_stride )
+{
+    return avc_pixel_var16width_lasx( p_pix, i_stride, 16 );
+}
+
+uint64_t x264_pixel_var_8x16_lasx( uint8_t *p_pix, intptr_t i_stride )
+{
+    return avc_pixel_var8width_lasx( p_pix, i_stride, 16 );
+}
+
+uint64_t x264_pixel_var_8x8_lasx( uint8_t *p_pix, intptr_t i_stride )
+{
+    return avc_pixel_var8width_lasx( p_pix, i_stride, 8 );
+}
+
+int32_t x264_pixel_var2_8x16_lasx( uint8_t *p_pix1, uint8_t *p_pix2,
+                                   int32_t ssd[2] )
+{
+    int32_t i_var = 0, i_diff_u = 0, i_sqr_u = 0;
+    int32_t i_diff_v = 0, i_sqr_v = 0;
+
+    i_sqr_u = sse_diff_8width_lasx( p_pix1, FENC_STRIDE,
+                                    p_pix2, FDEC_STRIDE, 16, &i_diff_u );
+    i_sqr_v = sse_diff_8width_lasx( p_pix1 + (FENC_STRIDE >> 1),
+                                    FENC_STRIDE,
+                                    p_pix2 + (FDEC_STRIDE >> 1),
+                                    FDEC_STRIDE, 16, &i_diff_v );
+    i_var = VARIANCE_WxH( i_sqr_u, i_diff_u, 7 ) +
+            VARIANCE_WxH( i_sqr_v, i_diff_v, 7 );
+    ssd[0] = i_sqr_u;
+    ssd[1] = i_sqr_v;
+
+    return i_var;
+}
+
+int32_t x264_pixel_var2_8x8_lasx( uint8_t *p_pix1, uint8_t *p_pix2,
+                                  int32_t ssd[2] )
+{
+    int32_t i_var = 0, i_diff_u = 0, i_sqr_u = 0;
+    int32_t i_diff_v = 0, i_sqr_v = 0;
+
+    i_sqr_u = sse_diff_8width_lasx( p_pix1, FENC_STRIDE,
+                                    p_pix2, FDEC_STRIDE, 8, &i_diff_u );
+    i_sqr_v = sse_diff_8width_lasx( p_pix1 + (FENC_STRIDE >> 1),
+                                    FENC_STRIDE,
+                                    p_pix2 + (FDEC_STRIDE >> 1),
+                                    FDEC_STRIDE, 8, &i_diff_v );
+    i_var = VARIANCE_WxH( i_sqr_u, i_diff_u, 6 ) +
+            VARIANCE_WxH( i_sqr_v, i_diff_v, 6 );
+    ssd[0] = i_sqr_u;
+    ssd[1] = i_sqr_v;
+
+    return i_var;
+}
+
+#endif
diff --git a/common/loongarch/pixel.h b/common/loongarch/pixel.h
new file mode 100644
index 0000000..ca56f2a
--- /dev/null
+++ b/common/loongarch/pixel.h
@@ -0,0 +1,232 @@
+/*****************************************************************************
+ * pixel.h: loongarch pixel metrics
+ *****************************************************************************
+ * Copyright (C) 2015-2018 x264 project
+ * Copyright (C) 2020 Loongson Technology Corporation Limited
+ *
+ * Authors: zhou peng <zhoupeng@loongson.cn>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02111, USA.
+ *
+ * This program is also available under a commercial proprietary license.
+ * For more information, contact us at licensing@x264.com.
+ *****************************************************************************/
+
+#ifndef X264_LOONGARCH_PIXEL_H
+#define X264_LOONGARCH_PIXEL_H
+
+#define x264_pixel_satd_4x4_lasx x264_template(pixel_satd_4x4_lasx)
+int32_t x264_pixel_satd_4x4_lasx( uint8_t *p_pix1, intptr_t i_stride,
+                                  uint8_t *p_pix2, intptr_t i_stride2 );
+#define x264_pixel_satd_4x8_lasx x264_template(pixel_satd_4x8_lasx)
+int32_t x264_pixel_satd_4x8_lasx( uint8_t *p_pix1, intptr_t i_stride,
+                                  uint8_t *p_pix2, intptr_t i_stride2 );
+#define x264_pixel_satd_4x16_lasx x264_template(pixel_satd_4x16_lasx)
+int32_t x264_pixel_satd_4x16_lasx( uint8_t *p_pix1, intptr_t i_stride,
+                                   uint8_t *p_pix2, intptr_t i_stride2 );
+#define x264_pixel_satd_8x4_lasx x264_template(pixel_satd_8x4_lasx)
+int32_t x264_pixel_satd_8x4_lasx( uint8_t *p_pix1, intptr_t i_stride,
+                                  uint8_t *p_pix2, intptr_t i_stride2 );
+#define x264_pixel_satd_8x8_lasx x264_template(pixel_satd_8x8_lasx)
+int32_t x264_pixel_satd_8x8_lasx( uint8_t *p_pix1, intptr_t i_stride,
+                                  uint8_t *p_pix2, intptr_t i_stride2 );
+#define x264_pixel_satd_8x16_lasx x264_template(pixel_satd_8x16_lasx)
+int32_t x264_pixel_satd_8x16_lasx( uint8_t *p_pix1, intptr_t i_stride,
+                                   uint8_t *p_pix2, intptr_t i_stride2 );
+#define x264_pixel_satd_16x8_lasx x264_template(pixel_satd_16x8_lasx)
+int32_t x264_pixel_satd_16x8_lasx( uint8_t *p_pix1, intptr_t i_stride,
+                                   uint8_t *p_pix2, intptr_t i_stride2 );
+#define x264_pixel_satd_16x16_lasx x264_template(pixel_satd_16x16_lasx)
+int32_t x264_pixel_satd_16x16_lasx( uint8_t *p_pix1, intptr_t i_stride,
+                                    uint8_t *p_pix2, intptr_t i_stride2 );
+
+#define x264_pixel_sad_x4_16x16_lasx x264_template(pixel_sad_x4_16x16_lasx)
+void x264_pixel_sad_x4_16x16_lasx( uint8_t *p_src, uint8_t *p_ref0,
+                                   uint8_t *p_ref1, uint8_t *p_ref2,
+                                   uint8_t *p_ref3, intptr_t i_ref_stride,
+                                   int32_t p_sad_array[4] );
+#define x264_pixel_sad_x4_16x8_lasx x264_template(pixel_sad_x4_16x8_lasx)
+void x264_pixel_sad_x4_16x8_lasx( uint8_t *p_src, uint8_t *p_ref0,
+                                  uint8_t *p_ref1, uint8_t *p_ref2,
+                                  uint8_t *p_ref3, intptr_t i_ref_stride,
+                                  int32_t p_sad_array[4] );
+#define x264_pixel_sad_x4_8x16_lasx x264_template(pixel_sad_x4_8x16_lasx)
+void x264_pixel_sad_x4_8x16_lasx( uint8_t *p_src, uint8_t *p_ref0,
+                                  uint8_t *p_ref1, uint8_t *p_ref2,
+                                  uint8_t *p_ref3, intptr_t i_ref_stride,
+                                  int32_t p_sad_array[4] );
+#define x264_pixel_sad_x4_8x8_lasx x264_template(pixel_sad_x4_8x8_lasx)
+void x264_pixel_sad_x4_8x8_lasx( uint8_t *p_src, uint8_t *p_ref0,
+                                 uint8_t *p_ref1, uint8_t *p_ref2,
+                                 uint8_t *p_ref3, intptr_t i_ref_stride,
+                                 int32_t p_sad_array[4] );
+#define x264_pixel_sad_x4_8x4_lasx x264_template(pixel_sad_x4_8x4_lasx)
+void x264_pixel_sad_x4_8x4_lasx( uint8_t *p_src, uint8_t *p_ref0,
+                                 uint8_t *p_ref1, uint8_t *p_ref2,
+                                 uint8_t *p_ref3, intptr_t i_ref_stride,
+                                 int32_t p_sad_array[4] );
+#define x264_pixel_sad_x4_4x8_lasx x264_template(pixel_sad_x4_4x8_lasx)
+void x264_pixel_sad_x4_4x8_lasx( uint8_t *p_src, uint8_t *p_ref0,
+                                 uint8_t *p_ref1, uint8_t *p_ref2,
+                                 uint8_t *p_ref3, intptr_t i_ref_stride,
+                                 int32_t p_sad_array[4] );
+#define x264_pixel_sad_x4_4x4_lasx x264_template(pixel_sad_x4_4x4_lasx)
+void x264_pixel_sad_x4_4x4_lasx( uint8_t *p_src, uint8_t *p_ref0,
+                                 uint8_t *p_ref1, uint8_t *p_ref2,
+                                 uint8_t *p_ref3, intptr_t i_ref_stride,
+                                 int32_t p_sad_array[4] );
+
+#define x264_pixel_sad_x3_16x16_lasx x264_template(pixel_sad_x3_16x16_lasx)
+void x264_pixel_sad_x3_16x16_lasx( uint8_t *p_src, uint8_t *p_ref0,
+                                  uint8_t *p_ref1, uint8_t *p_ref2,
+                                  intptr_t i_ref_stride,
+                                  int32_t p_sad_array[3] );
+#define x264_pixel_sad_x3_16x8_lasx x264_template(pixel_sad_x3_16x8_lasx)
+void x264_pixel_sad_x3_16x8_lasx( uint8_t *p_src, uint8_t *p_ref0,
+                                  uint8_t *p_ref1, uint8_t *p_ref2,
+                                  intptr_t i_ref_stride,
+                                  int32_t p_sad_array[3] );
+#define x264_pixel_sad_x3_8x16_lasx x264_template(pixel_sad_x3_8x16_lasx)
+void x264_pixel_sad_x3_8x16_lasx( uint8_t *p_src, uint8_t *p_ref0,
+                                  uint8_t *p_ref1, uint8_t *p_ref2,
+                                  intptr_t i_ref_stride,
+                                  int32_t p_sad_array[3] );
+#define x264_pixel_sad_x3_8x8_lasx x264_template(pixel_sad_x3_8x8_lasx)
+void x264_pixel_sad_x3_8x8_lasx( uint8_t *p_src, uint8_t *p_ref0,
+                                 uint8_t *p_ref1, uint8_t *p_ref2,
+                                 intptr_t i_ref_stride,
+                                 int32_t p_sad_array[3] );
+#define x264_pixel_sad_x3_8x4_lasx x264_template(pixel_sad_x3_8x4_lasx)
+void x264_pixel_sad_x3_8x4_lasx( uint8_t *p_src, uint8_t *p_ref0,
+                                 uint8_t *p_ref1, uint8_t *p_ref2,
+                                 intptr_t i_ref_stride,
+                                 int32_t p_sad_array[3] );
+#define x264_pixel_sad_x3_4x8_lasx x264_template(pixel_sad_x3_4x8_lasx)
+void x264_pixel_sad_x3_4x8_lasx( uint8_t *p_src, uint8_t *p_ref0,
+                                 uint8_t *p_ref1, uint8_t *p_ref2,
+                                 intptr_t i_ref_stride,
+                                 int32_t p_sad_array[3] );
+#define x264_pixel_sad_x3_4x4_lasx x264_template(pixel_sad_x3_4x4_lasx)
+void x264_pixel_sad_x3_4x4_lasx( uint8_t *p_src, uint8_t *p_ref0,
+                                 uint8_t *p_ref1, uint8_t *p_ref2,
+                                 intptr_t i_ref_stride,
+                                 int32_t p_sad_array[3] );
+
+#define x264_pixel_sad_16x16_lasx x264_template(pixel_sad_16x16_lasx)
+int32_t x264_pixel_sad_16x16_lasx( uint8_t *p_src, intptr_t i_src_stride,
+                                   uint8_t *p_ref, intptr_t i_ref_stride );
+#define x264_pixel_sad_16x8_lasx x264_template(pixel_sad_16x8_lasx)
+int32_t x264_pixel_sad_16x8_lasx( uint8_t *p_src, intptr_t i_src_stride,
+                                  uint8_t *p_ref, intptr_t i_ref_stride );
+#define x264_pixel_sad_8x16_lasx x264_template(pixel_sad_8x16_lasx)
+int32_t x264_pixel_sad_8x16_lasx( uint8_t *p_src, intptr_t i_src_stride,
+                                  uint8_t *p_ref, intptr_t i_ref_stride );
+#define x264_pixel_sad_8x8_lasx x264_template(pixel_sad_8x8_lasx)
+int32_t x264_pixel_sad_8x8_lasx( uint8_t *p_src, intptr_t i_src_stride,
+                                 uint8_t *p_ref, intptr_t i_ref_stride );
+#define x264_pixel_sad_8x4_lasx x264_template(pixel_sad_8x4_lasx)
+int32_t x264_pixel_sad_8x4_lasx( uint8_t *p_src, intptr_t i_src_stride,
+                                 uint8_t *p_ref, intptr_t i_ref_stride );
+#define x264_pixel_sad_4x16_lasx x264_template(pixel_sad_4x16_lasx)
+int32_t x264_pixel_sad_4x16_lasx( uint8_t *p_src, intptr_t i_src_stride,
+                                  uint8_t *p_ref, intptr_t i_ref_stride );
+#define x264_pixel_sad_4x8_lasx x264_template(pixel_sad_4x8_lasx)
+int32_t x264_pixel_sad_4x8_lasx( uint8_t *p_src, intptr_t i_src_stride,
+                                 uint8_t *p_ref, intptr_t i_ref_stride );
+#define x264_pixel_sad_4x4_lasx x264_template(pixel_sad_4x4_lasx)
+int32_t x264_pixel_sad_4x4_lasx( uint8_t *p_src, intptr_t i_src_stride,
+                                 uint8_t *p_ref, intptr_t i_ref_stride );
+
+#define x264_pixel_hadamard_ac_8x8_lasx x264_template(pixel_hadamard_ac_8x8_lasx)
+uint64_t x264_pixel_hadamard_ac_8x8_lasx( uint8_t *p_pix, intptr_t i_stride );
+#define x264_pixel_hadamard_ac_8x16_lasx x264_template(pixel_hadamard_ac_8x16_lasx)
+uint64_t x264_pixel_hadamard_ac_8x16_lasx( uint8_t *p_pix, intptr_t i_stride );
+#define x264_pixel_hadamard_ac_16x8_lasx x264_template(pixel_hadamard_ac_16x8_lasx)
+uint64_t x264_pixel_hadamard_ac_16x8_lasx( uint8_t *p_pix, intptr_t i_stride );
+#define x264_pixel_hadamard_ac_16x16_lasx x264_template(pixel_hadamard_ac_16x16_lasx)
+uint64_t x264_pixel_hadamard_ac_16x16_lasx( uint8_t *p_pix, intptr_t i_stride );
+
+#define x264_intra_satd_x3_4x4_lasx x264_template(intra_satd_x3_4x4_lasx)
+void x264_intra_satd_x3_4x4_lasx( uint8_t *p_enc, uint8_t *p_dec,
+                                  int32_t p_sad_array[3] );
+#define x264_intra_satd_x3_16x16_lasx x264_template(intra_satd_x3_16x16_lasx)
+void x264_intra_satd_x3_16x16_lasx( uint8_t *p_enc, uint8_t *p_dec,
+                                    int32_t p_sad_array[3] );
+#define x264_intra_satd_x3_8x8c_lasx x264_template(intra_satd_x3_8x8c_lasx)
+void x264_intra_satd_x3_8x8c_lasx( uint8_t *p_enc, uint8_t *p_dec,
+                                   int32_t p_sad_array[3] );
+
+#define x264_pixel_ssd_16x16_lasx x264_template(pixel_ssd_16x16_lasx)
+int32_t x264_pixel_ssd_16x16_lasx( uint8_t *p_src, intptr_t i_src_stride,
+                                   uint8_t *p_ref, intptr_t i_ref_stride );
+#define x264_pixel_ssd_16x8_lasx x264_template(pixel_ssd_16x8_lasx)
+int32_t x264_pixel_ssd_16x8_lasx( uint8_t *p_src, intptr_t i_src_stride,
+                                  uint8_t *p_ref, intptr_t i_ref_stride );
+#define x264_pixel_ssd_8x16_lasx x264_template(pixel_ssd_8x16_lasx)
+int32_t x264_pixel_ssd_8x16_lasx( uint8_t *p_src, intptr_t i_src_stride,
+                                  uint8_t *p_ref, intptr_t i_ref_stride );
+#define x264_pixel_ssd_8x8_lasx x264_template(pixel_ssd_8x8_lasx)
+int32_t x264_pixel_ssd_8x8_lasx( uint8_t *p_src, intptr_t i_src_stride,
+                                 uint8_t *p_ref, intptr_t i_ref_stride );
+#define x264_pixel_ssd_8x4_lasx x264_template(pixel_ssd_8x4_lasx)
+int32_t x264_pixel_ssd_8x4_lasx( uint8_t *p_src, intptr_t i_src_stride,
+                                 uint8_t *p_ref, intptr_t i_ref_stride );
+#define x264_pixel_ssd_4x16_lasx x264_template(pixel_ssd_4x16_lasx)
+int32_t x264_pixel_ssd_4x16_lasx( uint8_t *p_src, intptr_t i_src_stride,
+                                  uint8_t *p_ref, intptr_t i_ref_stride );
+#define x264_pixel_ssd_4x8_lasx x264_template(pixel_ssd_4x8_lasx)
+int32_t x264_pixel_ssd_4x8_lasx( uint8_t *p_src, intptr_t i_src_stride,
+                                 uint8_t *p_ref, intptr_t i_ref_stride );
+#define x264_pixel_ssd_4x4_lasx x264_template(pixel_ssd_4x4_lasx)
+int32_t x264_pixel_ssd_4x4_lasx( uint8_t *p_src, intptr_t i_src_stride,
+                                 uint8_t *p_ref, intptr_t i_ref_stride );
+
+#define x264_pixel_var_16x16_lasx x264_template(pixel_var_16x16_lasx)
+uint64_t x264_pixel_var_16x16_lasx( uint8_t *p_pix, intptr_t i_stride );
+#define x264_pixel_var_8x16_lasx x264_template(pixel_var_8x16_lasx)
+uint64_t x264_pixel_var_8x16_lasx( uint8_t *p_pix, intptr_t i_stride );
+#define x264_pixel_var_8x8_lasx x264_template(pixel_var_8x8_lasx)
+uint64_t x264_pixel_var_8x8_lasx( uint8_t *p_pix, intptr_t i_stride );
+#define x264_pixel_var2_8x16_lasx x264_template(pixel_var2_8x16_lasx)
+int32_t x264_pixel_var2_8x16_lasx( uint8_t *p_pix1, uint8_t *p_pix2,
+                                   int32_t ssd[2] );
+#define x264_pixel_var2_8x8_lasx x264_template(pixel_var2_8x8_lasx)
+int32_t x264_pixel_var2_8x8_lasx( uint8_t *p_pix1, uint8_t *p_pix2,
+                                  int32_t ssd[2] );
+
+#define x264_intra_sa8d_x3_8x8_lasx x264_template(intra_sa8d_x3_8x8_lasx)
+void x264_intra_sa8d_x3_8x8_lasx( uint8_t *p_enc, uint8_t p_edge[36],
+                                  int32_t p_sad_array[3] );
+#define x264_pixel_sa8d_8x8_lasx x264_template(pixel_sa8d_8x8_lasx)
+int32_t x264_pixel_sa8d_8x8_lasx( uint8_t *p_pix1, intptr_t i_stride,
+                                  uint8_t *p_pix2, intptr_t i_stride2 );
+#define x264_pixel_sa8d_16x16_lasx x264_template(pixel_sa8d_16x16_lasx)
+int32_t x264_pixel_sa8d_16x16_lasx( uint8_t *p_pix1, intptr_t i_stride,
+                                    uint8_t *p_pix2, intptr_t i_stride2 );
+
+#define x264_intra_sad_x3_4x4_lasx x264_template(intra_sad_x3_4x4_lasx)
+void x264_intra_sad_x3_4x4_lasx( uint8_t *p_enc, uint8_t *p_dec,
+                                 int32_t p_sad_array[3] );
+#define x264_intra_sad_x3_16x16_lasx x264_template(intra_sad_x3_16x16_lasx)
+void x264_intra_sad_x3_16x16_lasx( uint8_t *p_enc, uint8_t *p_dec,
+                                   int32_t p_sad_array[3] );
+#define x264_intra_sad_x3_8x8_lasx x264_template(intra_sad_x3_8x8_lasx)
+void x264_intra_sad_x3_8x8_lasx( uint8_t *p_enc, uint8_t p_edge[36],
+                                 int32_t p_sad_array[3] );
+#define x264_intra_sad_x3_8x8c_lasx x264_template(intra_sad_x3_8x8c_lasx)
+void x264_intra_sad_x3_8x8c_lasx( uint8_t *p_enc, uint8_t *p_dec,
+                                  int32_t p_sad_array[3] );
+
+#endif
diff --git a/common/loongarch/predict-c.c b/common/loongarch/predict-c.c
new file mode 100644
index 0000000..c90d811
--- /dev/null
+++ b/common/loongarch/predict-c.c
@@ -0,0 +1,586 @@
+/*****************************************************************************
+ * predict-c.c: loongarch intra prediction
+ *****************************************************************************
+ * Copyright (C) 2015-2018 x264 project
+ * Copyright (C) 2020 Loongson Technology Corporation Limited
+ *
+ * Authors: zhou peng <zhoupeng@loongson.cn>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02111, USA.
+ *
+ * This program is also available under a commercial proprietary license.
+ * For more information, contact us at licensing@x264.com.
+ *****************************************************************************/
+
+#include "common/common.h"
+#include "loongson_intrinsics.h"
+#include "predict.h"
+
+#if !HIGH_BIT_DEPTH
+
+static inline void intra_predict_dc_4blk_8x8_lasx( uint8_t *p_src,
+                                                   int32_t i_stride )
+{
+    uint32_t u_mask = 0x01010101;
+    int32_t i_stride_x4 = i_stride << 2;
+    uint8_t *p_src1, *p_src2;
+    __m256i sum, mask;
+    v8u32 out;
+
+    sum = __lasx_xvldx( p_src, -i_stride );
+    sum = __lasx_xvhaddw_hu_bu( sum, sum );
+    out = ( v8u32 ) __lasx_xvhaddw_wu_hu( sum, sum );
+    mask = __lasx_xvreplgr2vr_w( u_mask );
+
+    p_src1 = p_src - 1;
+    p_src2 = p_src1 + ( i_stride << 2 );
+    out[0] += p_src1[0];
+    out[2] = p_src2[0];
+
+    p_src1 += i_stride;
+    p_src2 += i_stride;
+    out[0] += p_src1[0];
+    out[2] += p_src2[0];
+
+    p_src1 += i_stride;
+    p_src2 += i_stride;
+    out[0] += p_src1[0];
+    out[2] += p_src2[0];
+
+    p_src1 += i_stride;
+    p_src2 += i_stride;
+    out[0] += p_src1[0];
+    out[2] += p_src2[0];
+
+    out[0] = ( out[0] + 4 ) >> 3;
+    out[3] = ( out[1] + out[2] + 4 ) >> 3;
+    out[1] = ( out[1] + 2 ) >> 2;
+    out[2] = ( out[2] + 2 ) >> 2;
+
+    out = ( v8u32 ) __lasx_xvmul_w( ( __m256i ) out, mask );
+
+    __lasx_xvstelm_d( out, p_src, 0, 0 );
+    __lasx_xvstelm_d( out, p_src + i_stride_x4, 0, 1 );
+    p_src += i_stride;
+
+    __lasx_xvstelm_d( out, p_src, 0, 0 );
+    __lasx_xvstelm_d( out, p_src + i_stride_x4, 0, 1 );
+    p_src += i_stride;
+
+    __lasx_xvstelm_d( out, p_src, 0, 0 );
+    __lasx_xvstelm_d( out, p_src + i_stride_x4, 0, 1 );
+    p_src += i_stride;
+
+    __lasx_xvstelm_d( out, p_src, 0, 0 );
+    __lasx_xvstelm_d( out, p_src + i_stride_x4, 0, 1 );
+    p_src += i_stride;
+}
+
+static inline void intra_predict_dc_4x4_lasx( uint8_t *p_src_top,
+                                              uint8_t *p_src_left,
+                                              int32_t i_src_stride_left,
+                                              uint8_t *p_dst,
+                                              int32_t i_dst_stride,
+                                              uint8_t is_above,
+                                              uint8_t is_left )
+{
+    uint32_t u_row;
+    uint32_t u_addition = 0;
+    int32_t i_dst_stride_x2 =  i_dst_stride << 1;
+    int32_t i_dst_stride_x3 =  i_dst_stride_x2 + i_dst_stride;
+    __m256i src, store;
+    v8u32 sum;
+
+    if( is_left && is_above )
+    {
+        src  = __lasx_xvld( p_src_top, 0 );
+        src = __lasx_xvhaddw_hu_bu( src, src );
+        sum = ( v8u32 ) __lasx_xvhaddw_wu_hu( src, src );
+        u_addition = sum[0];
+
+        for( u_row = 0; u_row < 4; u_row++ )
+        {
+            u_addition += p_src_left[u_row * i_src_stride_left];
+        }
+
+        u_addition = ( u_addition + 4 ) >> 3;
+        store = __lasx_xvreplgr2vr_b( u_addition );
+    }
+    else if( is_left )
+    {
+        for( u_row = 0; u_row < 4; u_row++ )
+        {
+            u_addition += p_src_left[u_row * i_src_stride_left];
+        }
+
+        u_addition = ( u_addition + 2 ) >> 2;
+        store = __lasx_xvreplgr2vr_b( u_addition );
+    }
+    else if( is_above )
+    {
+        src  = __lasx_xvld( p_src_top, 0 );
+        src = __lasx_xvhaddw_hu_bu( src, src );
+        src = __lasx_xvhaddw_wu_hu( src, src );
+        src = __lasx_xvsrari_w( src, 2 );
+
+        store = __lasx_xvrepl128vei_b( src, 0 );
+    }
+    else
+    {
+        u_addition = 128;
+
+        store = __lasx_xvreplgr2vr_b( u_addition );
+    }
+
+    __lasx_xvstelm_w( store, p_dst, 0, 0 );
+    __lasx_xvstelm_w( store, p_dst + i_dst_stride, 0, 0 );
+    __lasx_xvstelm_w( store, p_dst + i_dst_stride_x2, 0, 0 );
+    __lasx_xvstelm_w( store, p_dst + i_dst_stride_x3, 0, 0 );
+}
+
+static inline void intra_predict_dc_8x8_lasx( uint8_t *p_src_top,
+                                              uint8_t *p_src_left,
+                                              uint8_t *p_dst,
+                                              int32_t i_dst_stride )
+{
+    __m256i src0, src1, store;
+    int32_t i_dst_stride_x2 =  i_dst_stride << 1;
+    int32_t i_dst_stride_x3 =  i_dst_stride_x2 + i_dst_stride;
+
+    src0 = __lasx_xvldrepl_d( p_src_top, 0 );
+    src1 = __lasx_xvldrepl_d( p_src_left, 0 );
+    src0 = __lasx_xvpickev_d( src1, src0 );
+
+    src0 = __lasx_xvhaddw_hu_bu( src0, src0 );
+    src0 = __lasx_xvhaddw_wu_hu( src0, src0 );
+    src0 = __lasx_xvhaddw_du_wu( src0, src0 );
+    src0 = __lasx_xvpickev_w( src0, src0 );
+    src0 = __lasx_xvhaddw_du_wu( src0, src0 );
+    src0 = __lasx_xvsrari_w( src0, 4 );
+    store = __lasx_xvrepl128vei_b( src0, 0 );
+
+    __lasx_xvstelm_d( store, p_dst, 0, 0 );
+    __lasx_xvstelm_d( store, p_dst + i_dst_stride, 0, 0 );
+    __lasx_xvstelm_d( store, p_dst + i_dst_stride_x2, 0, 0 );
+    __lasx_xvstelm_d( store, p_dst + i_dst_stride_x3, 0, 0 );
+    p_dst += ( i_dst_stride  << 2);
+    __lasx_xvstelm_d( store, p_dst, 0, 0 );
+    __lasx_xvstelm_d( store, p_dst + i_dst_stride, 0, 0 );
+    __lasx_xvstelm_d( store, p_dst + i_dst_stride_x2, 0, 0 );
+    __lasx_xvstelm_d( store, p_dst + i_dst_stride_x3, 0, 0 );
+}
+
+static inline void intra_predict_dc_16x16_lasx( uint8_t *p_src_top,
+                                                uint8_t *p_src_left,
+                                                int32_t i_src_stride_left,
+                                                uint8_t *p_dst,
+                                                int32_t i_dst_stride,
+                                                uint8_t is_above,
+                                                uint8_t is_left )
+{
+    uint32_t u_row;
+    int32_t i_index = 0;
+    uint32_t u_addition = 0;
+    int32_t i_dst_stride_x2 = i_dst_stride << 1;
+    int32_t i_dst_stride_x3 = i_dst_stride_x2 + i_dst_stride;
+    __m256i src, store;
+    v4u64 sum;
+
+    if( is_left && is_above )
+    {
+        src  = __lasx_xvld( p_src_top, 0 );
+        src = __lasx_xvhaddw_hu_bu( src, src );
+        src = __lasx_xvhaddw_wu_hu( src, src );
+        src = __lasx_xvhaddw_du_wu( src, src );
+        src = __lasx_xvpickev_w( src, src );
+        sum = ( v4u64 ) __lasx_xvhaddw_du_wu( src, src );
+        u_addition = sum[0];
+
+        for( u_row = 0; u_row < 4; u_row++ )
+        {
+            u_addition += p_src_left[i_index];
+            i_index += i_src_stride_left;
+            u_addition += p_src_left[i_index];
+            i_index += i_src_stride_left;
+            u_addition += p_src_left[i_index];
+            i_index += i_src_stride_left;
+            u_addition += p_src_left[i_index];
+            i_index += i_src_stride_left;
+        }
+
+        u_addition = ( u_addition + 16 ) >> 5;
+        store = __lasx_xvreplgr2vr_b( u_addition );
+    }
+    else if( is_left )
+    {
+        for( u_row = 0; u_row < 4; u_row++ )
+        {
+            u_addition += p_src_left[i_index];
+            i_index += i_src_stride_left;
+            u_addition += p_src_left[i_index];
+            i_index += i_src_stride_left;
+            u_addition += p_src_left[i_index];
+            i_index += i_src_stride_left;
+            u_addition += p_src_left[i_index];
+            i_index += i_src_stride_left;
+        }
+
+        u_addition = ( u_addition + 8 ) >> 4;
+        store = __lasx_xvreplgr2vr_b( u_addition );
+    }
+    else if( is_above )
+    {
+        src  = __lasx_xvld( p_src_top, 0 );
+        src = __lasx_xvhaddw_hu_bu( src, src );
+        src = __lasx_xvhaddw_wu_hu( src, src );
+        src = __lasx_xvhaddw_du_wu( src, src );
+        src = __lasx_xvpickev_w( src, src );
+        src = __lasx_xvhaddw_du_wu( src, src );
+        src = __lasx_xvsrari_d( src, 4 );
+
+        store = __lasx_xvrepl128vei_b( src, 0 );
+    }
+    else
+    {
+        u_addition = 128;
+
+        store = __lasx_xvreplgr2vr_b( u_addition );
+    }
+
+    __lasx_xvstelm_d( store, p_dst, 0, 0 );
+    __lasx_xvstelm_d( store, p_dst, 8, 1 );
+    __lasx_xvstelm_d( store, p_dst + i_dst_stride, 0, 0 );
+    __lasx_xvstelm_d( store, p_dst + i_dst_stride, 8, 1 );
+    __lasx_xvstelm_d( store, p_dst + i_dst_stride_x2, 0, 0 );
+    __lasx_xvstelm_d( store, p_dst + i_dst_stride_x2, 8, 1 );
+    __lasx_xvstelm_d( store, p_dst + i_dst_stride_x3, 0, 0 );
+    __lasx_xvstelm_d( store, p_dst + i_dst_stride_x3, 8, 1 );
+    p_dst += ( i_dst_stride  << 2);
+    __lasx_xvstelm_d( store, p_dst, 0, 0 );
+    __lasx_xvstelm_d( store, p_dst, 8, 1 );
+    __lasx_xvstelm_d( store, p_dst + i_dst_stride, 0, 0 );
+    __lasx_xvstelm_d( store, p_dst + i_dst_stride, 8, 1 );
+    __lasx_xvstelm_d( store, p_dst + i_dst_stride_x2, 0, 0 );
+    __lasx_xvstelm_d( store, p_dst + i_dst_stride_x2, 8, 1 );
+    __lasx_xvstelm_d( store, p_dst + i_dst_stride_x3, 0, 0 );
+    __lasx_xvstelm_d( store, p_dst + i_dst_stride_x3, 8, 1 );
+    p_dst += ( i_dst_stride  << 2);
+    __lasx_xvstelm_d( store, p_dst, 0, 0 );
+    __lasx_xvstelm_d( store, p_dst, 8, 1 );
+    __lasx_xvstelm_d( store, p_dst + i_dst_stride, 0, 0 );
+    __lasx_xvstelm_d( store, p_dst + i_dst_stride, 8, 1 );
+    __lasx_xvstelm_d( store, p_dst + i_dst_stride_x2, 0, 0 );
+    __lasx_xvstelm_d( store, p_dst + i_dst_stride_x2, 8, 1 );
+    __lasx_xvstelm_d( store, p_dst + i_dst_stride_x3, 0, 0 );
+    __lasx_xvstelm_d( store, p_dst + i_dst_stride_x3, 8, 1 );
+    p_dst += ( i_dst_stride  << 2);
+    __lasx_xvstelm_d( store, p_dst, 0, 0 );
+    __lasx_xvstelm_d( store, p_dst, 8, 1 );
+    __lasx_xvstelm_d( store, p_dst + i_dst_stride, 0, 0 );
+    __lasx_xvstelm_d( store, p_dst + i_dst_stride, 8, 1 );
+    __lasx_xvstelm_d( store, p_dst + i_dst_stride_x2, 0, 0 );
+    __lasx_xvstelm_d( store, p_dst + i_dst_stride_x2, 8, 1 );
+    __lasx_xvstelm_d( store, p_dst + i_dst_stride_x3, 0, 0 );
+    __lasx_xvstelm_d( store, p_dst + i_dst_stride_x3, 8, 1 );
+}
+
+static inline void intra_predict_horiz_16x16_lasx( uint8_t *p_src,
+                                                   int32_t i_src_stride,
+                                                   uint8_t *p_dst,
+                                                   int32_t i_dst_stride )
+{
+    uint32_t u_row;
+    uint8_t u_inp0, u_inp1, u_inp2, u_inp3;
+    __m256i src0, src1, src2, src3;
+
+    for( u_row = 4; u_row--; )
+    {
+        u_inp0 = p_src[0];
+        p_src += i_src_stride;
+        u_inp1 = p_src[0];
+        p_src += i_src_stride;
+        u_inp2 = p_src[0];
+        p_src += i_src_stride;
+        u_inp3 = p_src[0];
+        p_src += i_src_stride;
+
+        src0 = __lasx_xvreplgr2vr_b( u_inp0 );
+        src1 = __lasx_xvreplgr2vr_b( u_inp1 );
+        src2 = __lasx_xvreplgr2vr_b( u_inp2 );
+        src3 = __lasx_xvreplgr2vr_b( u_inp3 );
+
+        __lasx_xvstelm_d( src0, p_dst, 0, 0 );
+        __lasx_xvstelm_d( src0, p_dst, 8, 1 );
+        p_dst += i_dst_stride;
+        __lasx_xvstelm_d( src1, p_dst, 0, 0 );
+        __lasx_xvstelm_d( src1, p_dst, 8, 1 );
+        p_dst += i_dst_stride;
+        __lasx_xvstelm_d( src2, p_dst, 0, 0 );
+        __lasx_xvstelm_d( src2, p_dst, 8, 1 );
+        p_dst += i_dst_stride;
+        __lasx_xvstelm_d( src3, p_dst, 0, 0 );
+        __lasx_xvstelm_d( src3, p_dst, 8, 1 );
+        p_dst += i_dst_stride;
+    }
+}
+
+static inline void intra_predict_horiz_8x8_lasx( uint8_t *p_src,
+                                                 int32_t i_src_stride,
+                                                 uint8_t *p_dst,
+                                                 int32_t i_dst_stride )
+{
+    uint8_t u_inp0, u_inp1, u_inp2, u_inp3;
+    __m256i src0, src1, src2, src3;
+
+    u_inp0 = p_src[0];
+    p_src += i_src_stride;
+    u_inp1 = p_src[0];
+    p_src += i_src_stride;
+    u_inp2 = p_src[0];
+    p_src += i_src_stride;
+    u_inp3 = p_src[0];
+    p_src += i_src_stride;
+
+    src0 = __lasx_xvreplgr2vr_b( u_inp0 );
+    src1 = __lasx_xvreplgr2vr_b( u_inp1 );
+    src2 = __lasx_xvreplgr2vr_b( u_inp2 );
+    src3 = __lasx_xvreplgr2vr_b( u_inp3 );
+
+    __lasx_xvstelm_d( src0, p_dst, 0, 0 );
+    p_dst += i_dst_stride;
+    __lasx_xvstelm_d( src1, p_dst, 0, 0 );
+    p_dst += i_dst_stride;
+    __lasx_xvstelm_d( src2, p_dst, 0, 0 );
+    p_dst += i_dst_stride;
+    __lasx_xvstelm_d( src3, p_dst, 0, 0 );
+    p_dst += i_dst_stride;
+
+    u_inp0 = p_src[0];
+    p_src += i_src_stride;
+    u_inp1 = p_src[0];
+    p_src += i_src_stride;
+    u_inp2 = p_src[0];
+    p_src += i_src_stride;
+    u_inp3 = p_src[0];
+    p_src += i_src_stride;
+
+    src0 = __lasx_xvreplgr2vr_b( u_inp0 );
+    src1 = __lasx_xvreplgr2vr_b( u_inp1 );
+    src2 = __lasx_xvreplgr2vr_b( u_inp2 );
+    src3 = __lasx_xvreplgr2vr_b( u_inp3 );
+
+    __lasx_xvstelm_d( src0, p_dst, 0, 0 );
+    p_dst += i_dst_stride;
+    __lasx_xvstelm_d( src1, p_dst, 0, 0 );
+    p_dst += i_dst_stride;
+    __lasx_xvstelm_d( src2, p_dst, 0, 0 );
+    p_dst += i_dst_stride;
+    __lasx_xvstelm_d( src3, p_dst, 0, 0 );
+    p_dst += i_dst_stride;
+}
+
+static inline void intra_predict_horiz_4x4_lasx( uint8_t *p_src,
+                                                 int32_t i_src_stride,
+                                                 uint8_t *p_dst,
+                                                 int32_t i_dst_stride )
+{
+    uint8_t u_inp0, u_inp1, u_inp2, u_inp3;
+    __m256i src0, src1, src2, src3;
+
+    u_inp0 = p_src[0];
+    p_src += i_src_stride;
+    u_inp1 = p_src[0];
+    p_src += i_src_stride;
+    u_inp2 = p_src[0];
+    p_src += i_src_stride;
+    u_inp3 = p_src[0];
+    p_src += i_src_stride;
+
+    src0 = __lasx_xvreplgr2vr_b( u_inp0 );
+    src1 = __lasx_xvreplgr2vr_b( u_inp1 );
+    src2 = __lasx_xvreplgr2vr_b( u_inp2 );
+    src3 = __lasx_xvreplgr2vr_b( u_inp3 );
+
+    __lasx_xvstelm_w( src0, p_dst, 0, 0 );
+    p_dst += i_dst_stride;
+    __lasx_xvstelm_w( src1, p_dst, 0, 0 );
+    p_dst += i_dst_stride;
+    __lasx_xvstelm_w( src2, p_dst, 0, 0 );
+    p_dst += i_dst_stride;
+    __lasx_xvstelm_w( src3, p_dst, 0, 0 );
+    p_dst += i_dst_stride;
+}
+
+static inline void intra_predict_vert_16x16_lasx( uint8_t *p_src,
+                                                  uint8_t *p_dst,
+                                                  int32_t i_dst_stride )
+{
+    __m256i src;
+    int32_t i_dst_stride_x2 = i_dst_stride << 1;
+    int32_t i_dst_stride_x3 = i_dst_stride_x2 + i_dst_stride;
+    src  = __lasx_xvld( p_src, 0 );
+
+    __lasx_xvstelm_d( src, p_dst, 0, 0 );
+    __lasx_xvstelm_d( src, p_dst, 8, 1 );
+    __lasx_xvstelm_d( src, p_dst + i_dst_stride, 0, 0 );
+    __lasx_xvstelm_d( src, p_dst + i_dst_stride, 8, 1 );
+    __lasx_xvstelm_d( src, p_dst + i_dst_stride_x2, 0, 0 );
+    __lasx_xvstelm_d( src, p_dst + i_dst_stride_x2, 8, 1 );
+    __lasx_xvstelm_d( src, p_dst + i_dst_stride_x3, 0, 0 );
+    __lasx_xvstelm_d( src, p_dst + i_dst_stride_x3, 8, 1 );
+    p_dst += ( i_dst_stride  << 2);
+    __lasx_xvstelm_d( src, p_dst, 0, 0 );
+    __lasx_xvstelm_d( src, p_dst, 8, 1 );
+    __lasx_xvstelm_d( src, p_dst + i_dst_stride, 0, 0 );
+    __lasx_xvstelm_d( src, p_dst + i_dst_stride, 8, 1 );
+    __lasx_xvstelm_d( src, p_dst + i_dst_stride_x2, 0, 0 );
+    __lasx_xvstelm_d( src, p_dst + i_dst_stride_x2, 8, 1 );
+    __lasx_xvstelm_d( src, p_dst + i_dst_stride_x3, 0, 0 );
+    __lasx_xvstelm_d( src, p_dst + i_dst_stride_x3, 8, 1 );
+    p_dst += ( i_dst_stride  << 2);
+    __lasx_xvstelm_d( src, p_dst, 0, 0 );
+    __lasx_xvstelm_d( src, p_dst, 8, 1 );
+    __lasx_xvstelm_d( src, p_dst + i_dst_stride, 0, 0 );
+    __lasx_xvstelm_d( src, p_dst + i_dst_stride, 8, 1 );
+    __lasx_xvstelm_d( src, p_dst + i_dst_stride_x2, 0, 0 );
+    __lasx_xvstelm_d( src, p_dst + i_dst_stride_x2, 8, 1 );
+    __lasx_xvstelm_d( src, p_dst + i_dst_stride_x3, 0, 0 );
+    __lasx_xvstelm_d( src, p_dst + i_dst_stride_x3, 8, 1 );
+    p_dst += ( i_dst_stride  << 2);
+    __lasx_xvstelm_d( src, p_dst, 0, 0 );
+    __lasx_xvstelm_d( src, p_dst, 8, 1 );
+    __lasx_xvstelm_d( src, p_dst + i_dst_stride, 0, 0 );
+    __lasx_xvstelm_d( src, p_dst + i_dst_stride, 8, 1 );
+    __lasx_xvstelm_d( src, p_dst + i_dst_stride_x2, 0, 0 );
+    __lasx_xvstelm_d( src, p_dst + i_dst_stride_x2, 8, 1 );
+    __lasx_xvstelm_d( src, p_dst + i_dst_stride_x3, 0, 0 );
+    __lasx_xvstelm_d( src, p_dst + i_dst_stride_x3, 8, 1 );
+}
+
+static inline void intra_predict_vert_8x8_lasx( uint8_t *p_src,
+                                                uint8_t *p_dst,
+                                                int32_t i_dst_stride )
+{
+    __m256i out;
+    int32_t i_dst_stride_x2 = i_dst_stride << 1;
+    int32_t i_dst_stride_x3 = i_dst_stride_x2 + i_dst_stride;
+
+    out = __lasx_xvldrepl_d( p_src, 0 );
+
+    __lasx_xvstelm_d( out, p_dst, 0, 0 );
+    __lasx_xvstelm_d( out, p_dst + i_dst_stride, 0, 0 );
+    __lasx_xvstelm_d( out, p_dst + i_dst_stride_x2, 0, 0 );
+    __lasx_xvstelm_d( out, p_dst + i_dst_stride_x3, 0, 0 );
+    p_dst += ( i_dst_stride << 2 );
+    __lasx_xvstelm_d( out, p_dst, 0, 0 );
+    __lasx_xvstelm_d( out, p_dst + i_dst_stride, 0, 0 );
+    __lasx_xvstelm_d( out, p_dst + i_dst_stride_x2, 0, 0 );
+    __lasx_xvstelm_d( out, p_dst + i_dst_stride_x3, 0, 0 );
+}
+
+static inline void intra_predict_vert_4x4_lasx( uint8_t *p_src,
+                                                uint8_t *p_dst,
+                                                int32_t i_dst_stride )
+{
+    __m256i out;
+    int32_t i_dst_stride_x2 = i_dst_stride << 1;
+    int32_t i_dst_stride_x3 = i_dst_stride_x2 + i_dst_stride;
+
+    out = __lasx_xvldrepl_w( p_src, 0 );
+
+    __lasx_xvstelm_w( out, p_dst, 0, 0 );
+    __lasx_xvstelm_w( out, p_dst + i_dst_stride, 0, 0 );
+    __lasx_xvstelm_w( out, p_dst + i_dst_stride_x2, 0, 0 );
+    __lasx_xvstelm_w( out, p_dst + i_dst_stride_x3, 0, 0 );
+}
+
+void x264_intra_predict_dc_4blk_8x8_lasx( uint8_t *p_src )
+{
+    intra_predict_dc_4blk_8x8_lasx( p_src, FDEC_STRIDE );
+}
+
+void x264_intra_predict_hor_8x8_lasx( uint8_t *p_src )
+{
+    intra_predict_horiz_8x8_lasx( ( p_src - 1 ), FDEC_STRIDE,
+                                  p_src, FDEC_STRIDE );
+}
+
+void x264_intra_predict_vert_8x8_lasx( uint8_t *p_src )
+{
+    intra_predict_vert_8x8_lasx( ( p_src - FDEC_STRIDE ), p_src, FDEC_STRIDE );
+}
+
+void x264_intra_predict_dc_4x4_lasx( uint8_t *p_src )
+{
+    intra_predict_dc_4x4_lasx( ( p_src - FDEC_STRIDE ), ( p_src - 1 ),
+                               FDEC_STRIDE, p_src, FDEC_STRIDE, 1, 1 );
+}
+
+void x264_intra_predict_hor_4x4_lasx( uint8_t *p_src )
+{
+    intra_predict_horiz_4x4_lasx( ( p_src - 1 ), FDEC_STRIDE,
+                                  p_src, FDEC_STRIDE );
+}
+
+void x264_intra_predict_vert_4x4_lasx( uint8_t *p_src )
+{
+    intra_predict_vert_4x4_lasx( ( p_src - FDEC_STRIDE ), p_src, FDEC_STRIDE );
+}
+
+void x264_intra_predict_hor_16x16_lasx( uint8_t *p_src )
+{
+    intra_predict_horiz_16x16_lasx( ( p_src - 1 ), FDEC_STRIDE,
+                                    p_src, FDEC_STRIDE );
+}
+
+void x264_intra_predict_vert_16x16_lasx( uint8_t *p_src )
+{
+    intra_predict_vert_16x16_lasx( ( p_src - FDEC_STRIDE ), p_src, FDEC_STRIDE );
+}
+
+void x264_intra_predict_dc_16x16_lasx( uint8_t *p_src )
+{
+    intra_predict_dc_16x16_lasx( ( p_src - FDEC_STRIDE ), ( p_src - 1 ),
+                                 FDEC_STRIDE, p_src, FDEC_STRIDE, 1, 1 );
+}
+
+void x264_intra_predict_dc_8x8_lasx( uint8_t *p_src, uint8_t pu_xyz[36] )
+{
+    intra_predict_dc_8x8_lasx( ( pu_xyz + 16 ), ( pu_xyz + 7 ),
+                               p_src, FDEC_STRIDE );
+}
+
+void x264_intra_predict_h_8x8_lasx( uint8_t *p_src, uint8_t pu_xyz[36] )
+{
+    intra_predict_horiz_8x8_lasx( ( pu_xyz + 14 ), -1, p_src, FDEC_STRIDE );
+}
+
+void x264_intra_predict_v_8x8_lasx( uint8_t *p_src, uint8_t pu_xyz[36] )
+{
+    intra_predict_vert_8x8_lasx( ( pu_xyz + 16 ), p_src, FDEC_STRIDE );
+}
+
+void x264_predict_16x16_init_lasx( int cpu, x264_predict_t pf[7] )
+{
+    if ( cpu&X264_CPU_LASX ) {
+#if !HIGH_BIT_DEPTH
+        pf[I_PRED_16x16_V]    = x264_intra_predict_vert_16x16_lasx;
+        pf[I_PRED_16x16_H]    = x264_intra_predict_hor_16x16_lasx;
+        pf[I_PRED_16x16_DC]   = x264_intra_predict_dc_16x16_lasx;
+#endif
+    }
+}
+
+#endif
diff --git a/common/loongarch/predict.h b/common/loongarch/predict.h
new file mode 100644
index 0000000..102ebc7
--- /dev/null
+++ b/common/loongarch/predict.h
@@ -0,0 +1,61 @@
+/*****************************************************************************
+ * predict.h: loongarch intra prediction
+ *****************************************************************************
+ * Copyright (C) 2015-2018 x264 project
+ * Copyright (C) 2020 Loongson Technology Corporation Limited
+ *
+ * Authors: zhou peng <zhoupeng@loongson.cn>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02111, USA.
+ *
+ * This program is also available under a commercial proprietary license.
+ * For more information, contact us at licensing@x264.com.
+ *****************************************************************************/
+
+#ifndef X264_LOONGARCH_PREDICT_H
+#define X264_LOONGARCH_PREDICT_H
+
+#define x264_intra_predict_dc_16x16_lasx x264_template(intra_predict_dc_16x16_lasx)
+void x264_intra_predict_dc_16x16_lasx( uint8_t *p_src );
+#define x264_intra_predict_hor_16x16_lasx x264_template(intra_predict_hor_16x16_lasx)
+void x264_intra_predict_hor_16x16_lasx( uint8_t *p_src );
+#define x264_intra_predict_vert_16x16_lasx x264_template(intra_predict_vert_16x16_lasx)
+void x264_intra_predict_vert_16x16_lasx( uint8_t *p_src );
+
+#define x264_intra_predict_dc_4blk_8x8_lasx x264_template(intra_predict_dc_4blk_8x8_lasx)
+void x264_intra_predict_dc_4blk_8x8_lasx( uint8_t *p_src );
+#define x264_intra_predict_hor_8x8_lasx x264_template(intra_predict_hor_8x8_lasx)
+void x264_intra_predict_hor_8x8_lasx( uint8_t *p_src );
+#define x264_intra_predict_vert_8x8_lasx x264_template(intra_predict_vert_8x8_lasx)
+void x264_intra_predict_vert_8x8_lasx( uint8_t *p_src );
+
+#define x264_intra_predict_dc_4x4_lasx x264_template(intra_predict_dc_4x4_lasx)
+void x264_intra_predict_dc_4x4_lasx( uint8_t *p_src );
+#define x264_intra_predict_hor_4x4_lasx x264_template(intra_predict_hor_4x4_lasx)
+void x264_intra_predict_hor_4x4_lasx( uint8_t *p_src );
+#define x264_intra_predict_vert_4x4_lasx x264_template(intra_predict_vert_4x4_lasx)
+void x264_intra_predict_vert_4x4_lasx( uint8_t *p_src );
+
+#define x264_intra_predict_dc_8x8_lasx x264_template(intra_predict_dc_8x8_lasx)
+void x264_intra_predict_dc_8x8_lasx( uint8_t *p_src, uint8_t pu_xyz[36] );
+#define x264_intra_predict_h_8x8_lasx x264_template(intra_predict_h_8x8_lasx)
+void x264_intra_predict_h_8x8_lasx( uint8_t *p_src, uint8_t pu_xyz[36] );
+#define x264_intra_predict_v_8x8_lasx x264_template(intra_predict_v_8x8_lasx)
+void x264_intra_predict_v_8x8_lasx( uint8_t *p_src, uint8_t pu_xyz[36] );
+
+#define x264_predict_16x16_init_lasx x264_template(predict_16x16_init_lasx)
+void x264_predict_16x16_init_lasx( int cpu, x264_predict_t pf[7] );
+
+#endif
diff --git a/common/loongarch/quant-c.c b/common/loongarch/quant-c.c
new file mode 100644
index 0000000..e590c71
--- /dev/null
+++ b/common/loongarch/quant-c.c
@@ -0,0 +1,280 @@
+/*****************************************************************************
+ * quant-c.c: loongarch quantization and level-run
+ *****************************************************************************
+ * Copyright (C) 2020 x264 project
+ * Copyright (C) 2020 Loongson Technology Corporation Limited
+ *
+ * Authors: zhou peng <zhoupeng@loongson.cn>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02111, USA.
+ *
+ * This program is also available under a commercial proprietary license.
+ * For more information, contact us at licensing@x264.com.
+ *****************************************************************************/
+
+#include "common/common.h"
+#include "loongson_intrinsics.h"
+#include "quant.h"
+
+#if !HIGH_BIT_DEPTH
+
+static inline int32_t avc_quant_4x4_lasx( int16_t *p_dct,
+                                          uint16_t *p_mf,
+                                          uint16_t *p_bias )
+{
+    int32_t non_zero = 0;
+    __m256i zero = __lasx_xvldi( 0 );
+    __m256i dct_mask;
+    __m256i dct, dct0, dct1;
+    __m256i mf, mf0, mf1;
+    __m256i bias, bias0, bias1;
+    __m256i tmp;
+
+    dct = __lasx_xvld( p_dct, 0 );
+    bias = __lasx_xvld( p_bias, 0 );
+    mf = __lasx_xvld( p_mf, 0 );
+
+    dct_mask = __lasx_xvslei_h( dct, 0 );
+
+    LASX_UNPCK_SH( dct, dct0, dct1 );
+    bias0 = __lasx_xvilvl_h( zero, bias );
+    bias1 = __lasx_xvilvh_h( zero, bias );
+    mf0 = __lasx_xvilvl_h( zero, mf );
+    mf1 = __lasx_xvilvh_h( zero, mf );
+
+    dct0 = __lasx_xvadda_w( dct0, bias0 );
+    dct1 = __lasx_xvadda_w( dct1, bias1 );
+    dct0 = __lasx_xvmul_w(dct0, mf0);
+    dct1 = __lasx_xvmul_w(dct1, mf1);
+
+    dct = __lasx_xvsrani_h_w(dct1, dct0, 16);
+
+    tmp = __lasx_xvhaddw_w_h( dct, dct );
+    tmp = __lasx_xvhaddw_d_w(tmp, tmp);
+    tmp = __lasx_xvhaddw_q_d(tmp, tmp);
+    non_zero = __lasx_xvpickve2gr_w(tmp, 0) + __lasx_xvpickve2gr_w(tmp, 4);
+
+    dct0 = __lasx_xvsub_h( zero, dct );
+    dct = __lasx_xvbitsel_v( dct, dct0, dct_mask );
+    __lasx_xvst( dct, p_dct, 0 );
+
+    return !!non_zero;
+}
+
+static inline int32_t avc_quant_8x8_lasx( int16_t *p_dct,
+                                          uint16_t *p_mf,
+                                          uint16_t *p_bias )
+{
+    int32_t non_zero = 0;
+    __m256i zero = __lasx_xvldi( 0 );
+    __m256i dct_mask0, dct_mask1;
+    __m256i dct0, dct1, dct0_0, dct0_1, dct1_0, dct1_1;
+    __m256i mf0, mf1, mf0_0, mf0_1, mf1_0, mf1_1;
+    __m256i bias0, bias1, bias0_0, bias0_1, bias1_0, bias1_1;
+    __m256i tmp;
+
+    dct0 = __lasx_xvld( p_dct, 0 );
+    dct1 = __lasx_xvld( p_dct, 32 );
+    bias0 = __lasx_xvld( p_bias, 0 );
+    bias1 = __lasx_xvld( p_bias, 32 );
+    mf0 = __lasx_xvld( p_mf, 0 );
+    mf1 = __lasx_xvld( p_mf, 32 );
+
+    dct_mask0 = __lasx_xvslei_h( dct0, 0 );
+    dct_mask1 = __lasx_xvslei_h( dct1, 0 );
+
+    LASX_UNPCK_SH( dct0, dct0_0, dct0_1 );
+    LASX_UNPCK_SH( dct1, dct1_0, dct1_1 );
+    bias0_0 = __lasx_xvilvl_h( zero, bias0 );
+    bias0_1 = __lasx_xvilvh_h( zero, bias0 );
+    bias1_0 = __lasx_xvilvl_h( zero, bias1 );
+    bias1_1 = __lasx_xvilvh_h( zero, bias1 );
+    mf0_0 = __lasx_xvilvl_h( zero, mf0 );
+    mf0_1 = __lasx_xvilvh_h( zero, mf0 );
+    mf1_0 = __lasx_xvilvl_h( zero, mf1 );
+    mf1_1 = __lasx_xvilvh_h( zero, mf1 );
+
+    dct0_0 = __lasx_xvadda_w( dct0_0, bias0_0 );
+    dct0_1 = __lasx_xvadda_w( dct0_1, bias0_1 );
+    dct1_0 = __lasx_xvadda_w( dct1_0, bias1_0 );
+    dct1_1 = __lasx_xvadda_w( dct1_1, bias1_1 );
+
+    dct0_0 = __lasx_xvmul_w( dct0_0, mf0_0 );
+    dct0_1 = __lasx_xvmul_w( dct0_1, mf0_1 );
+    dct1_0 = __lasx_xvmul_w( dct1_0, mf1_0 );
+    dct1_1 = __lasx_xvmul_w( dct1_1, mf1_1 );
+
+    dct0 = __lasx_xvsrani_h_w( dct0_1, dct0_0, 16 );
+    dct1 = __lasx_xvsrani_h_w( dct1_1, dct1_0, 16 );
+
+    tmp = __lasx_xvadd_h( dct0, dct1 );
+    tmp = __lasx_xvhaddw_w_h( tmp, tmp );
+    tmp = __lasx_xvhaddw_d_w(tmp, tmp);
+    tmp = __lasx_xvhaddw_q_d(tmp, tmp);
+    non_zero = __lasx_xvpickve2gr_w(tmp, 0) + __lasx_xvpickve2gr_w(tmp, 4);
+
+    dct0_0 = __lasx_xvsub_h( zero, dct0 );
+    dct1_0 = __lasx_xvsub_h( zero, dct1 );
+    dct0 = __lasx_xvbitsel_v( dct0, dct0_0, dct_mask0 );
+    dct1 = __lasx_xvbitsel_v( dct1, dct1_0, dct_mask1 );
+
+    __lasx_xvst( dct0, p_dct, 0 );
+    __lasx_xvst( dct1, p_dct, 32 );
+
+    /* next part */
+    dct0 = __lasx_xvld( p_dct, 64 );
+    dct1 = __lasx_xvld( p_dct, 96 );
+    bias0 = __lasx_xvld( p_bias, 64 );
+    bias1 = __lasx_xvld( p_bias, 96 );
+    mf0 = __lasx_xvld( p_mf, 64 );
+    mf1 = __lasx_xvld( p_mf, 96 );
+
+    dct_mask0 = __lasx_xvslei_h( dct0, 0 );
+    dct_mask1 = __lasx_xvslei_h( dct1, 0 );
+
+    LASX_UNPCK_SH( dct0, dct0_0, dct0_1 );
+    LASX_UNPCK_SH( dct1, dct1_0, dct1_1 );
+    bias0_0 = __lasx_xvilvl_h( zero, bias0 );
+    bias0_1 = __lasx_xvilvh_h( zero, bias0 );
+    bias1_0 = __lasx_xvilvl_h( zero, bias1 );
+    bias1_1 = __lasx_xvilvh_h( zero, bias1 );
+    mf0_0 = __lasx_xvilvl_h( zero, mf0 );
+    mf0_1 = __lasx_xvilvh_h( zero, mf0 );
+    mf1_0 = __lasx_xvilvl_h( zero, mf1 );
+    mf1_1 = __lasx_xvilvh_h( zero, mf1 );
+
+    dct0_0 = __lasx_xvadda_w( dct0_0, bias0_0 );
+    dct0_1 = __lasx_xvadda_w( dct0_1, bias0_1 );
+    dct1_0 = __lasx_xvadda_w( dct1_0, bias1_0 );
+    dct1_1 = __lasx_xvadda_w( dct1_1, bias1_1 );
+
+    dct0_0 = __lasx_xvmul_w( dct0_0, mf0_0 );
+    dct0_1 = __lasx_xvmul_w( dct0_1, mf0_1 );
+    dct1_0 = __lasx_xvmul_w( dct1_0, mf1_0 );
+    dct1_1 = __lasx_xvmul_w( dct1_1, mf1_1 );
+
+    dct0 = __lasx_xvsrani_h_w( dct0_1, dct0_0, 16 );
+    dct1 = __lasx_xvsrani_h_w( dct1_1, dct1_0, 16 );
+
+    tmp = __lasx_xvadd_h( dct0, dct1 );
+    tmp = __lasx_xvhaddw_w_h( tmp, tmp );
+    tmp = __lasx_xvhaddw_d_w(tmp, tmp);
+    tmp = __lasx_xvhaddw_q_d(tmp, tmp);
+    non_zero += __lasx_xvpickve2gr_w(tmp, 0) + __lasx_xvpickve2gr_w(tmp, 4);
+
+    dct0_0 = __lasx_xvsub_h( zero, dct0 );
+    dct1_0 = __lasx_xvsub_h( zero, dct1 );
+    dct0 = __lasx_xvbitsel_v( dct0, dct0_0, dct_mask0 );
+    dct1 = __lasx_xvbitsel_v( dct1, dct1_0, dct_mask1 );
+
+    __lasx_xvst( dct0, p_dct, 64 );
+    __lasx_xvst( dct1, p_dct, 96 );
+
+    return !!non_zero;
+}
+
+int32_t x264_coeff_last16_lasx( int16_t *p_src )
+{
+    __m256i src0, tmp0, tmp1;
+    __m256i one = __lasx_xvldi(1);
+    int32_t result;
+
+    src0 = __lasx_xvld( p_src, 0 );
+    tmp0 = __lasx_xvssrlni_bu_h(src0, src0, 0);
+    tmp0 = __lasx_xvpermi_d(tmp0, 0xD8);
+    tmp1 = __lasx_xvsle_bu(one, tmp0);
+    tmp0 = __lasx_xvssrlni_bu_h(tmp1, tmp1, 4);
+    tmp1 = __lasx_xvclz_d(tmp0);
+    result = __lasx_xvpickve2gr_w(tmp1, 0);
+
+    return 15 - (result >> 2);
+}
+
+int32_t x264_coeff_last15_lasx( int16_t *psrc )
+{
+    __m256i src0, tmp0, tmp1;
+    __m256i one = __lasx_xvldi(1);
+    int32_t result;
+
+    src0 = __lasx_xvld( psrc, -2 );
+    tmp0 = __lasx_xvssrlni_bu_h(src0, src0, 0);
+    tmp0 = __lasx_xvpermi_d(tmp0, 0xD8);
+    tmp1 = __lasx_xvsle_bu(one, tmp0);
+    tmp0 = __lasx_xvssrlni_bu_h(tmp1, tmp1, 4);
+    tmp1 = __lasx_xvclz_d(tmp0);
+    result = __lasx_xvpickve2gr_w(tmp1, 0);
+
+    return 14 - (result >> 2);
+}
+
+int32_t x264_coeff_last64_lasx( int16_t *p_src )
+{
+    int32_t result;
+    __m256i src0, src1, src2, src3;
+    __m256i tmp0, tmp1, tmp2, tmp3;
+    __m256i one = __lasx_xvldi(1);
+    __m256i const_8 = __lasx_xvldi(0x408);
+    __m256i const_1 = __lasx_xvldi(0x401);
+    __m256i shift = {0x0000000400000000, 0x0000000500000001,
+                     0x0000000600000002, 0x0000000700000003};
+    src0 = __lasx_xvld( p_src, 0 );
+    src1 = __lasx_xvld( p_src, 32);
+    src2 = __lasx_xvld( p_src, 64);
+    src3 = __lasx_xvld( p_src, 96);
+
+    tmp0 = __lasx_xvssrlni_bu_h(src1, src0, 0);
+    tmp1 = __lasx_xvssrlni_bu_h(src3, src2, 0);
+    tmp2 = __lasx_xvsle_bu(one, tmp0);
+    tmp3 = __lasx_xvsle_bu(one, tmp1);
+    tmp0 = __lasx_xvssrlni_bu_h(tmp3, tmp2, 4);
+    tmp0 = __lasx_xvperm_w(tmp0, shift);
+    tmp1 = __lasx_xvclz_w(tmp0);
+    tmp0 = __lasx_xvssrlni_hu_w(tmp1, tmp1, 2);
+    tmp0 = __lasx_xvpermi_d(tmp0, 0xD8);
+
+    tmp1 = __lasx_xvsub_h(const_8, tmp0);
+    tmp0 = __lasx_xvsll_h(const_1, tmp1);
+    tmp0 = __lasx_xvssrlni_bu_h(tmp0, tmp0, 1);
+    tmp1 = __lasx_xvclz_d(tmp0);
+    result = __lasx_xvpickve2gr_w(tmp1, 0);
+    return 63 - result;
+}
+
+int32_t x264_quant_4x4_lasx( int16_t *p_dct, uint16_t *p_mf, uint16_t *p_bias )
+{
+    return avc_quant_4x4_lasx( p_dct, p_mf, p_bias );
+}
+
+int32_t x264_quant_4x4x4_lasx( int16_t p_dct[4][16],
+                               uint16_t pu_mf[16], uint16_t pu_bias[16] )
+{
+    int32_t i_non_zero, i_non_zero_acc = 0;
+
+    for( int32_t j = 0; j < 4; j++  )
+    {
+        i_non_zero = x264_quant_4x4_lasx( p_dct[j], pu_mf, pu_bias );
+
+        i_non_zero_acc |= ( !!i_non_zero ) << j;
+    }
+
+    return i_non_zero_acc;
+}
+
+int32_t x264_quant_8x8_lasx( int16_t *p_dct, uint16_t *p_mf, uint16_t *p_bias )
+{
+    return avc_quant_8x8_lasx( p_dct, p_mf, p_bias );
+}
+
+#endif /* !HIGH_BIT_DEPTH */
diff --git a/common/loongarch/quant.h b/common/loongarch/quant.h
new file mode 100644
index 0000000..50a2fee
--- /dev/null
+++ b/common/loongarch/quant.h
@@ -0,0 +1,44 @@
+/*****************************************************************************
+ * quant.h: loongarch quantization and level-run
+ *****************************************************************************
+ * Copyright (C) 2020 x264 project
+ * Copyright (C) 2020 Loongson Technology Corporation Limited
+ *
+ * Authors: zhou peng <zhoupeng@loongson.cn>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02111, USA.
+ *
+ * This program is also available under a commercial proprietary license.
+ * For more information, contact us at licensing@x264.com.
+ *****************************************************************************/
+
+#ifndef X264_LOONGARCH_QUANT_H
+#define X264_LOONGARCH_QUANT_H
+
+#define x264_coeff_last64_lasx x264_template(coeff_last64_lasx)
+int32_t x264_coeff_last64_lasx( int16_t *p_src );
+#define x264_coeff_last16_lasx x264_template(coeff_last16_lasx)
+int32_t x264_coeff_last16_lasx( int16_t *p_src );
+#define x264_coeff_last15_lasx x264_template(coeff_last15_lasx)
+int32_t x264_coeff_last15_lasx( int16_t *p_src );
+#define x264_quant_4x4_lasx x264_template(quant_4x4_lasx)
+int32_t x264_quant_4x4_lasx( int16_t *p_dct, uint16_t *p_mf, uint16_t *p_bias );
+#define x264_quant_4x4x4_lasx x264_template(quant_4x4x4_lasx)
+int32_t x264_quant_4x4x4_lasx( int16_t p_dct[4][16],
+                               uint16_t pu_mf[16], uint16_t pu_bias[16] );
+#define x264_quant_8x8_lasx x264_template(quant_8x8_lasx)
+int32_t x264_quant_8x8_lasx( int16_t *p_dct, uint16_t *p_mf, uint16_t *p_bias );
+
+#endif/* X264_LOONGARCH_QUANT_H */
diff --git a/common/loongarch/sad-a.S b/common/loongarch/sad-a.S
new file mode 100644
index 0000000..fc4d329
--- /dev/null
+++ b/common/loongarch/sad-a.S
@@ -0,0 +1,322 @@
+/*****************************************************************************
+ * sad-a.S: loongarch sad functions
+ *****************************************************************************
+ * Copyright (C) 2015-2018 x264 project
+ * Copyright (C) 2022 Loongson Technology Corporation Limited
+ *
+ * Authors: gxw <guxiwei-hf@loongson.cn>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02111, USA.
+ *
+ * This program is also available under a commercial proprietary license.
+ * For more information, contact us at licensing@x264.com.
+ *****************************************************************************/
+
+#include "asm.S"
+
+#if !HIGH_BIT_DEPTH
+.macro LOAD_REF_DATA_16W_START x, y, z
+    LSX_LOADX_4   \x, \z, t0, t1, vr8, vr12, vr9, vr13
+    add.d         \x, \x, t2
+    LSX_LOADX_4   \x, \z, t0, t1, vr10, vr14, vr11, vr15
+    xvpermi.q     xr8,    xr12,    0x02
+    xvpermi.q     xr9,    xr13,    0x02
+    xvpermi.q     xr10,   xr14,    0x02
+    xvpermi.q     xr11,   xr15,    0x02
+    xvabsd.bu     xr16,   xr8,     xr0
+    xvhaddw.hu.bu xr16,   xr16,    xr16
+    xvabsd.bu     xr17,   xr9,     xr1
+    xvhaddw.hu.bu xr17,   xr17,    xr17
+    xvadd.h       \y,     xr16,    xr17
+    xvabsd.bu     xr16,   xr10,    xr2
+    xvhaddw.hu.bu xr16,   xr16,    xr16
+    xvabsd.bu     xr17,   xr11,    xr3
+    xvhaddw.hu.bu xr17,   xr17,    xr17
+    xvadd.h       \y,     \y,      xr16
+    xvadd.h       \y,     \y,      xr17
+.endm
+
+.macro LOAD_REF_DATA_16W_END x, y, z
+    LSX_LOADX_4   \x, \z, t0, t1, vr8, vr12, vr9, vr13
+    add.d         \x, \x, t2
+    LSX_LOADX_4   \x, \z, t0, t1, vr10, vr14, vr11, vr15
+    xvpermi.q     xr8,    xr12,    0x02
+    xvpermi.q     xr9,    xr13,    0x02
+    xvpermi.q     xr10,   xr14,    0x02
+    xvpermi.q     xr11,   xr15,    0x02
+    xvabsd.bu     xr16,   xr8,     xr0
+    xvhaddw.hu.bu xr16,   xr16,    xr16
+    xvabsd.bu     xr17,   xr9,     xr1
+    xvhaddw.hu.bu xr17,   xr17,    xr17
+    xvadd.h       \y,     \y,      xr16
+    xvadd.h       \y,     \y,      xr17
+    xvabsd.bu     xr16,   xr10,    xr2
+    xvhaddw.hu.bu xr16,   xr16,    xr16
+    xvabsd.bu     xr17,   xr11,    xr3
+    xvhaddw.hu.bu xr17,   xr17,    xr17
+    xvadd.h       \y,     \y,      xr16
+    xvadd.h       \y,     \y,      xr17
+.endm
+
+.macro ST_REF_DATA x  /* TODO: We should use native ins rather than macro */
+    xvhaddw.wu.hu   \x,    \x,    \x
+    xvhaddw.du.wu   \x,    \x,    \x
+    xvhaddw.qu.du   \x,    \x,    \x
+.endm
+
+
+/* int32_t x264_pixel_sad_WxH_lasx( uint8_t *p_src, intptr_t i_src_stride,
+ *                                  uint8_t *p_ref, intptr_t i_ref_stride )
+ */
+function pixel_sad_16x16_lasx
+    slli.d    t3,    a1,    1
+    add.d     t4,    a1,    t3
+
+    slli.d    t0,    a3,    1
+    add.d     t1,    a3,    t0
+    slli.d    t2,    a3,    2
+
+    LSX_LOADX_4 a0, a1, t3, t4, vr0, vr4, vr1, vr6
+    alsl.d    a0,   a1,     a0,    2
+    LSX_LOADX_4 a0, a1, t3, t4, vr2, vr5, vr3, vr7
+    xvpermi.q   xr0,  xr4,    0x02
+    xvpermi.q   xr1,  xr6,    0x02
+    xvpermi.q   xr2,  xr5,    0x02
+    xvpermi.q   xr3,  xr7,    0x02  /* Inefficient */
+    alsl.d    a0,    a1,    a0,    2
+    LOAD_REF_DATA_16W_START a2, xr18, a3
+    add.d     a2,    a2,    t2
+
+    LSX_LOADX_4 a0, a1, t3, t4, vr0, vr4, vr1, vr6
+    alsl.d    a0,    a1,    a0,    2
+    LSX_LOADX_4 a0, a1, t3, t4, vr2, vr5, vr3, vr7
+    xvpermi.q   xr0,  xr4,    0x02
+    xvpermi.q   xr1,  xr6,    0x02
+    xvpermi.q   xr2,  xr5,    0x02
+    xvpermi.q   xr3,  xr7,    0x02  /* Inefficient */
+    LOAD_REF_DATA_16W_END   a2, xr18, a3
+
+    ST_REF_DATA xr18
+    xvpickve2gr.wu  v0,   xr18,   0
+    xvpickve2gr.wu  t0,   xr18,   4
+    add.d     v0,   v0,     t0
+endfunc
+
+/* void x264_pixel_sad_x3_WxH_lasx( uint8_t *p_src, uint8_t *p_ref0,
+ *                                  uint8_t *p_ref1, uint8_t *p_ref2,
+ *                                  intptr_t i_ref_stride,
+ *                                  int32_t p_sad_array[3] )
+ */
+function pixel_sad_x3_16x16_lasx
+    slli.d    t0,    a4,    1
+    add.d     t1,    a4,    t0
+    slli.d    t2,    a4,    2
+    xvld      xr0,   a0,    0
+    xvld      xr1,   a0,    FENC_STRIDE << 1  /* assumes FENC_STRIDE == 16 */
+    xvld      xr2,   a0,    FENC_STRIDE << 2
+    xvld      xr3,   a0,    FENC_STRIDE * 6
+    LOAD_REF_DATA_16W_START a1, xr4, a4
+    LOAD_REF_DATA_16W_START a2, xr5, a4
+    LOAD_REF_DATA_16W_START a3, xr6, a4
+    xvld      xr0,   a0,   FENC_STRIDE << 3
+    xvld      xr1,   a0,   FENC_STRIDE * 10
+    xvld      xr2,   a0,   FENC_STRIDE * 12
+    xvld      xr3,   a0,   FENC_STRIDE * 14
+    add.d     a1,    a1,   t2
+    add.d     a2,    a2,   t2
+    add.d     a3,    a3,   t2
+    LOAD_REF_DATA_16W_END a1, xr4, a4
+    LOAD_REF_DATA_16W_END a2, xr5, a4
+    LOAD_REF_DATA_16W_END a3, xr6, a4
+
+    xvpermi.q xr0,   xr4,   0x01
+    xvpermi.q xr1,   xr5,   0x01
+    xvpermi.q xr2,   xr6,   0x01
+    xvadd.h   xr0,   xr0,   xr4
+    xvadd.h   xr1,   xr1,   xr5
+    xvadd.h   xr2,   xr2,   xr6
+    xvpermi.q    xr0,   xr1,   0x02  /* Inefficient */
+    ST_REF_DATA xr0
+    ST_REF_DATA xr2
+    vstelm.w  vr0,   a5,    0,    0
+    xvstelm.w xr0,   a5,    4,    4
+    vstelm.w  vr2,   a5,    8,    0
+endfunc
+
+/* void x264_pixel_sad_x4_WxH_lasx( uint8_t *p_src, uint8_t *p_ref0,
+ *                                  uint8_t *p_ref1, uint8_t *p_ref2,
+ *                                  uint8_t *p_ref3, intptr_t i_ref_stride,
+ *                                  int32_t p_sad_array[4] )
+ */
+function pixel_sad_x4_16x16_lasx
+    slli.d    t0,    a5,    1
+    add.d     t1,    a5,    t0
+    slli.d    t2,    a5,    2
+    xvld      xr0,   a0,    0
+    xvld      xr1,   a0,    FENC_STRIDE << 1
+    xvld      xr2,   a0,    FENC_STRIDE << 2
+    xvld      xr3,   a0,    FENC_STRIDE * 6
+
+    LOAD_REF_DATA_16W_START a1, xr4, a5
+    LOAD_REF_DATA_16W_START a2, xr5, a5
+    LOAD_REF_DATA_16W_START a3, xr6, a5
+    LOAD_REF_DATA_16W_START a4, xr7, a5
+
+    xvld      xr0,   a0,   FENC_STRIDE << 3
+    xvld      xr1,   a0,   FENC_STRIDE * 10
+    xvld      xr2,   a0,   FENC_STRIDE * 12
+    xvld      xr3,   a0,   FENC_STRIDE * 14
+
+    add.d    a1,    a1,   t2
+    add.d    a2,    a2,   t2
+    add.d    a3,    a3,   t2
+    add.d    a4,    a4,   t2
+
+    LOAD_REF_DATA_16W_END a1, xr4, a5
+    LOAD_REF_DATA_16W_END a2, xr5, a5
+    LOAD_REF_DATA_16W_END a3, xr6, a5
+    LOAD_REF_DATA_16W_END a4, xr7, a5
+
+    xvori.b     xr0,   xr4,   0
+    xvori.b     xr1,   xr5,   0
+    xvpermi.q   xr4,   xr6,   0x02
+    xvpermi.q   xr6,   xr0,   0x31
+    xvpermi.q   xr5,   xr7,   0x02
+    xvpermi.q   xr7,   xr1,   0x31
+    xvadd.h     xr0,   xr4,   xr6
+    xvadd.h     xr1,   xr5,   xr7
+    ST_REF_DATA xr0
+    ST_REF_DATA xr1
+    xvpackev.w  xr0,   xr1,   xr0
+    xvstelm.d   xr0,   a6,    0,   0
+    xvstelm.d   xr0,   a6,    8,   2
+endfunc /* end of pixel_sad_x4_16x16_lasx */
+function pixel_sad_x4_8x8_lasx
+#define ot0  xr20
+#define ot1  xr21
+#define ot2  xr22
+#define ot3  xr23
+#define ot4  xr6
+#define ot5  xr7
+#define ot6  xr8
+#define ot7  xr9
+    xvldrepl.d  xr4, a0,  0
+    vld         vr0, a1,  0
+    vld         vr1, a3,  0
+    vld         vr2, a2,  0
+    vld         vr3, a4,  0
+    vpackev.d   vr0, vr2, vr0
+    vpackev.d   vr1, vr3, vr1
+    xvpermi.q   xr0, xr1, 0x02
+    slli.d      t1,  a5,  1
+    xvldrepl.d  xr9, a0,  FENC_STRIDE
+    vldx        vr5, a1,  a5
+    vldx        vr6, a3,  a5
+    vldx        vr7, a2,  a5
+    vldx        vr8, a4,  a5
+    xvabsd.bu   xr0, xr4, xr0
+    vpackev.d   vr5, vr7, vr5
+    vpackev.d   vr6, vr8, vr6
+    xvpermi.q   xr5, xr6, 0x02
+
+    xvldrepl.d  xr14,  a0,   FENC_STRIDE << 1
+    vldx        vr10,  a1,   t1
+    vldx        vr11,  a3,   t1
+    vldx        vr12,  a2,   t1
+    vldx        vr13,  a4,   t1
+    xvabsd.bu   xr5,   xr9,  xr5
+    vpackev.d   vr10,  vr12, vr10
+    vpackev.d   vr11,  vr13, vr11
+    xvpermi.q   xr10,  xr11, 0x02
+    add.d       t1,    t1,   a5
+    xvldrepl.d  xr19,  a0,   FENC_STRIDE * 3
+    vldx        vr15,  a1,   t1
+    vldx        vr16,  a3,   t1
+    vldx        vr17,  a2,   t1
+    vldx        vr18,  a4,   t1
+    xvabsd.bu   xr10,  xr14, xr10
+    vpackev.d   vr15,  vr17, vr15
+    vpackev.d   vr16,  vr18, vr16
+    xvpermi.q   xr15,  xr16, 0x02
+    add.d       t1,    t1,   a5
+    xvaddwev.h.bu  ot0,  xr0,  xr5
+    xvaddwod.h.bu  ot1,  xr0,  xr5
+
+    xvldrepl.d  xr4, a0,  FENC_STRIDE << 2
+    vldx        vr0, a1,  t1
+    vldx        vr1, a3,  t1
+    vldx        vr2, a2,  t1
+    vldx        vr3, a4,  t1
+    xvabsd.bu   xr15,  xr19,  xr15
+    vpackev.d   vr0,   vr2,   vr0
+    vpackev.d   vr1,   vr3,   vr1
+    xvpermi.q   xr0,   xr1,   0x02
+    add.d       t1,    t1,    a5
+    xvldrepl.d  xr9,   a0,    FENC_STRIDE * 5
+    vldx        vr5,   a1,    t1
+    vldx        vr6,   a3,    t1
+    vldx        vr7,   a2,    t1
+    vldx        vr8,   a4,    t1
+    xvabsd.bu   xr0,   xr4,   xr0
+    vpackev.d   vr5,   vr7,   vr5
+    vpackev.d   vr6,   vr8,   vr6
+    xvpermi.q   xr5,   xr6,   0x02
+    add.d       t1,    t1,    a5
+    xvaddwev.h.bu  ot2,  xr10,  xr15
+    xvaddwod.h.bu  ot3,  xr10,  xr15
+
+    xvldrepl.d  xr14,  a0,  FENC_STRIDE * 6
+    vldx        vr10,  a1,  t1
+    vldx        vr11,  a3,  t1
+    vldx        vr12,  a2,  t1
+    vldx        vr13,  a4,  t1
+    xvabsd.bu   xr5,   xr9, xr5
+    vpackev.d   vr10,  vr12, vr10
+    vpackev.d   vr11,  vr13, vr11
+    xvpermi.q   xr10,  xr11, 0x02
+    add.d       t1,    t1,   a5
+    xvldrepl.d  xr19,  a0,   FENC_STRIDE * 7
+    vldx        vr15,  a1,   t1
+    vldx        vr16,  a3,   t1
+    vldx        vr17,  a2,   t1
+    vldx        vr18,  a4,   t1
+    xvabsd.bu   xr10,  xr14, xr10
+    vpackev.d   vr15,  vr17,  vr15
+    vpackev.d   vr16,  vr18,  vr16
+    xvpermi.q   xr15,  xr16,  0x02
+    xvaddwev.h.bu  ot4,  xr0,  xr5
+    xvaddwod.h.bu  ot5,  xr0,  xr5
+    xvabsd.bu      xr15, xr19,  xr15
+    xvaddwev.h.bu  ot6,  xr10,  xr15
+    xvaddwod.h.bu  ot7,  xr10,  xr15
+
+    xvadd.h    ot0,  ot0,  ot2
+    xvadd.h    ot1,  ot1,  ot3
+    xvadd.h    ot0,  ot0,  ot4
+    xvadd.h    ot1,  ot1,  ot5
+    xvadd.h    ot0,  ot0,  ot6
+    xvadd.h    ot1,  ot1,  ot7
+    xvadd.h    ot0,  ot0,  ot1
+
+    xvhaddw.wu.hu  ot0,  ot0,  ot0
+    xvhaddw.du.wu  ot0,  ot0,  ot0
+    xvpermi.q      ot1,  ot0,  0X01
+    xvpickev.w     ot0,  ot1,  ot0
+    vst            vr20, a6,   0
+endfunc
+
+.end     sad-a.S
+
+#endif /* !HIGH_BIT_DEPTH */
diff --git a/common/mc.c b/common/mc.c
index 2a39513..d884d27 100644
--- a/common/mc.c
+++ b/common/mc.c
@@ -41,6 +41,9 @@
 #if HAVE_MSA
 #include "mips/mc.h"
 #endif
+#if ARCH_LOONGARCH
+#   include "loongarch/mc.h"
+#endif
 
 
 static inline void pixel_avg( pixel *dst,  intptr_t i_dst_stride,
@@ -687,6 +690,9 @@ void x264_mc_init( uint32_t cpu, x264_mc_functions_t *pf, int cpu_independent )
     if( cpu&X264_CPU_MSA )
         x264_mc_init_mips( cpu, pf );
 #endif
+#if HAVE_LASX
+    x264_mc_init_loongarch( cpu, pf );
+#endif
 
     if( cpu_independent )
     {
diff --git a/common/mips/dct-c.c b/common/mips/dct-c.c
index 2aa4d57..7e19d10 100644
--- a/common/mips/dct-c.c
+++ b/common/mips/dct-c.c
@@ -523,4 +523,97 @@ void x264_zigzag_scan_4x4_frame_msa( int16_t pi_level[16], int16_t pi_dct[16] )
 {
     avc_zigzag_scan_4x4_frame_msa( pi_dct, pi_level );
 }
+
+/****************************************************************************
+ * 8x8 transform:
+ ****************************************************************************/
+
+void x264_sub8x8_dct8_msa( int16_t pi_dct[64], uint8_t *p_pix1,
+                           uint8_t *p_pix2 )
+{
+    v16i8 src0, src1, src2, src3;
+    v8i16 tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7, temp;
+    v16i8 zero = {0};
+    v8i16 s07, s16, s25, s34, d07, d16, d25, d34;
+    v8i16 a0, a1, a2, a3, a4, a5, a6, a7;
+    v8i16 shift0, shift1;
+
+    shift0 = (v8i16)__msa_fill_h(1);
+    shift1 = (v8i16)__msa_fill_h(2);
+
+#define LOAD_PIX_DATA_2(data1, data2) \
+    LD_B2( v16i8, p_pix1, FENC_STRIDE, src0, src1 ); \
+    LD_B2( v16i8, p_pix2, FDEC_STRIDE, src2, src3 ); \
+    src0 = (v16i8) __msa_ilvr_b( zero, src0); \
+    src1 = (v16i8) __msa_ilvr_b( zero, src1); \
+    src2 = (v16i8) __msa_ilvr_b( zero, src2); \
+    src3 = (v16i8) __msa_ilvr_b( zero, src3); \
+    data1 = (v8i16) src0 - (v8i16) src2; \
+    data2 = (v8i16) src1 - (v8i16) src3; \
+    p_pix1 += ( FENC_STRIDE << 1 ); \
+    p_pix2 += ( FDEC_STRIDE << 1 );
+
+    LOAD_PIX_DATA_2(tmp0, tmp1);
+    LOAD_PIX_DATA_2(tmp2, tmp3);
+    LOAD_PIX_DATA_2(tmp4, tmp5);
+    LOAD_PIX_DATA_2(tmp6, tmp7);
+
+#undef LOAD_PIX_DATA_2
+
+#define DCT8_1D_MSA \
+    s07 = tmp0 + tmp7; \
+    s16 = tmp1 + tmp6; \
+    s25 = tmp2 + tmp5; \
+    s34 = tmp3 + tmp4; \
+    a0  = s07 + s34; \
+    a1  = s16 + s25; \
+    a2  = s07 - s34; \
+    a3  = s16 - s25; \
+ \
+    d07  = tmp0 - tmp7; \
+    d16  = tmp1 - tmp6; \
+    d25  = tmp2 - tmp5; \
+    d34  = tmp3 - tmp4; \
+    temp = __msa_sra_h( d07, shift0 ); \
+    a4   = d16 + d25 + d07 + temp; \
+    temp = __msa_sra_h( d25, shift0 ); \
+    a5   = d07 - d34 - d25 - temp;\
+    temp = __msa_sra_h( d16, shift0 ); \
+    a6   = d07 + d34 - d16 - temp; \
+    temp = __msa_sra_h( d34, shift0 ); \
+    a7   = d16 - d25 + d34 + temp; \
+ \
+    tmp0 = a0 + a1; \
+    temp = __msa_sra_h( a7, shift1 ); \
+    tmp1 = a4 + temp; \
+    temp = __msa_sra_h( a3, shift0 ); \
+    tmp2 = a2 + temp; \
+    temp = __msa_sra_h( a6, shift1 ); \
+    tmp3 = a5 + temp; \
+    tmp4 = a0 -a1; \
+    temp = __msa_sra_h( a5, shift1 ); \
+    tmp5 = a6 - temp; \
+    temp = __msa_sra_h( a2, shift0 ); \
+    tmp6 = temp - a3; \
+    temp = __msa_sra_h( a4, shift1 ); \
+    tmp7 = temp - a7;
+
+    DCT8_1D_MSA;
+    TRANSPOSE8x8_H( v8i16, tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7,
+                     tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7);
+    DCT8_1D_MSA;
+
+#undef DCT8_1D_MSA
+
+    ST_H8( v8i16,  tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7, &pi_dct[0], 8);
+}
+
+void x264_sub16x16_dct8_msa( int16_t pi_dct[4][64], uint8_t *p_pix1,
+                             uint8_t *p_pix2 )
+{
+    x264_sub8x8_dct8_msa( pi_dct[0], &p_pix1[0],               &p_pix2[0] );
+    x264_sub8x8_dct8_msa( pi_dct[1], &p_pix1[8],               &p_pix2[8] );
+    x264_sub8x8_dct8_msa( pi_dct[2], &p_pix1[8*FENC_STRIDE+0], &p_pix2[8*FDEC_STRIDE+0] );
+    x264_sub8x8_dct8_msa( pi_dct[3], &p_pix1[8*FENC_STRIDE+8], &p_pix2[8*FDEC_STRIDE+8] );
+}
 #endif
diff --git a/common/mips/dct.h b/common/mips/dct.h
index 03b3c6d..f3293f5 100644
--- a/common/mips/dct.h
+++ b/common/mips/dct.h
@@ -40,6 +40,12 @@ void x264_add16x16_idct_msa( uint8_t *p_dst, int16_t pi_dct[16][16] );
 void x264_add8x8_idct8_msa( uint8_t *p_dst, int16_t pi_dct[64] );
 #define x264_add16x16_idct8_msa x264_template(add16x16_idct8_msa)
 void x264_add16x16_idct8_msa( uint8_t *p_dst, int16_t pi_dct[4][64] );
+#define x264_sub8x8_idct8_msa x264_template(sub8x8_idct8_msa)
+void x264_sub8x8_dct8_msa( int16_t pi_dct[64], uint8_t *p_pix1,
+                           uint8_t *p_pix2 );
+#define x264_sub16x16_idct8_msa x264_template(sub16x16_idct8_msa)
+void x264_sub16x16_dct8_msa( int16_t pi_dct[4][64], uint8_t *p_pix1,
+                             uint8_t *p_pix2 );
 #define x264_add8x8_idct_dc_msa x264_template(add8x8_idct_dc_msa)
 void x264_add8x8_idct_dc_msa( uint8_t *p_dst, int16_t pi_dct[4] );
 #define x264_add16x16_idct_dc_msa x264_template(add16x16_idct_dc_msa)
diff --git a/common/mips/mc-c.c b/common/mips/mc-c.c
index 61499c6..7053c8d 100644
--- a/common/mips/mc-c.c
+++ b/common/mips/mc-c.c
@@ -972,7 +972,7 @@ static void avc_biwgt_opscale_4x2_nw_msa( uint8_t *p_src1_in,
     v16u8 in0, in1, in2, in3;
     v8i16 temp0, temp1, temp2, temp3;
     v16i8 zero = { 0 };
-    v8i16 denom = __msa_ldi_h( i_log2_denom + 1 );
+    v8i16 denom = __msa_fill_h( i_log2_denom + 1 );
 
     src1_wgt = __msa_fill_h( i_src1_weight );
     src2_wgt = __msa_fill_h( i_src2_weight );
@@ -1016,7 +1016,7 @@ static void avc_biwgt_opscale_4x4multiple_nw_msa( uint8_t *p_src1_in,
     v16u8 src0, src1, src2, src3, src4, src5, src6, src7;
     v8i16 temp0, temp1, temp2, temp3, temp4, temp5, temp6, temp7;
     v16i8 zero = { 0 };
-    v8i16 denom = __msa_ldi_h( i_log2_denom + 1 );
+    v8i16 denom = __msa_fill_h( i_log2_denom + 1 );
 
     src1_wgt = __msa_fill_h( i_src1_weight );
     src2_wgt = __msa_fill_h( i_src2_weight );
@@ -1099,7 +1099,7 @@ static void avc_biwgt_opscale_8width_nw_msa( uint8_t *p_src1_in,
     v8i16 temp0, temp1, temp2, temp3;
     v8i16 res0, res1, res2, res3;
     v16i8 zero = { 0 };
-    v8i16 denom = __msa_ldi_h( i_log2_denom + 1 );
+    v8i16 denom = __msa_fill_h( i_log2_denom + 1 );
 
     src1_wgt = __msa_fill_h( i_src1_weight );
     src2_wgt = __msa_fill_h( i_src2_weight );
@@ -1152,7 +1152,7 @@ static void avc_biwgt_opscale_16width_nw_msa( uint8_t *p_src1_in,
     v8i16 temp0, temp1, temp2, temp3, temp4, temp5, temp6, temp7;
     v8i16 res0, res1, res2, res3, res4, res5, res6, res7;
     v16i8 zero = { 0 };
-    v8i16 denom = __msa_ldi_h( i_log2_denom + 1 );
+    v8i16 denom = __msa_fill_h( i_log2_denom + 1 );
 
     src1_wgt = __msa_fill_h( i_src1_weight );
     src2_wgt = __msa_fill_h( i_src2_weight );
diff --git a/common/mips/pixel-c.c b/common/mips/pixel-c.c
index b22527f..8515b09 100644
--- a/common/mips/pixel-c.c
+++ b/common/mips/pixel-c.c
@@ -711,7 +711,6 @@ static int32_t pixel_satd_4width_msa( uint8_t *p_src, int32_t i_src_stride,
     uint32_t u_sum = 0;
     v16i8 src0, src1, src2, src3;
     v16i8 ref0, ref1, ref2, ref3;
-    v8i16 zero = { 0 };
     v8i16 diff0, diff1, diff2, diff3;
     v8i16 temp0, temp1, temp2, temp3;
 
@@ -734,11 +733,9 @@ static int32_t pixel_satd_4width_msa( uint8_t *p_src, int32_t i_src_stride,
         BUTTERFLY_4( diff0, diff2, diff3, diff1, temp0, temp2, temp3, temp1 );
         BUTTERFLY_4( temp0, temp1, temp3, temp2, diff0, diff1, diff3, diff2 );
 
-        diff0 = __msa_add_a_h( diff0, zero );
-        diff1 = __msa_add_a_h( diff1, zero );
-        diff2 = __msa_add_a_h( diff2, zero );
-        diff3 = __msa_add_a_h( diff3, zero );
-        diff0 = ( diff0 + diff1 + diff2 + diff3 );
+        diff0 = __msa_add_a_h( diff0, diff1 );
+        diff2 = __msa_add_a_h( diff2, diff3 );
+        diff0 += diff2;
         diff0 = ( v8i16 ) __msa_hadd_u_w( ( v8u16 ) diff0, ( v8u16 ) diff0 );
         diff0 = ( v8i16 ) __msa_hadd_u_d( ( v4u32 ) diff0, ( v4u32 ) diff0 );
         u_sum += __msa_copy_u_w( ( v4i32 ) diff0, 0 );
@@ -753,18 +750,20 @@ static int32_t pixel_satd_8width_msa( uint8_t *p_src, int32_t i_src_stride,
 {
     int32_t cnt;
     uint32_t u_sum = 0;
+    int32_t src_stride_4x = i_src_stride << 2;
+    int32_t ref_stride_4x = i_ref_stride << 2;
     v16i8 src0, src1, src2, src3;
     v16i8 ref0, ref1, ref2, ref3;
-    v8i16 zero = { 0 };
+    v8i16 sum = { 0 };
     v8i16 diff0, diff1, diff2, diff3, diff4, diff5, diff6, diff7;
     v8i16 temp0, temp1, temp2, temp3;
 
     for( cnt = i_height >> 2; cnt--; )
     {
         LD_SB4( p_src, i_src_stride, src0, src1, src2, src3 );
-        p_src += 4 * i_src_stride;
+        p_src += src_stride_4x;
         LD_SB4( p_ref, i_ref_stride, ref0, ref1, ref2, ref3 );
-        p_ref += 4 * i_ref_stride;
+        p_ref += ref_stride_4x;
 
         ILVR_B4_SH( src0, ref0, src1, ref1, src2, ref2, src3, ref3,
                     diff0, diff1, diff2, diff3 );
@@ -772,29 +771,28 @@ static int32_t pixel_satd_8width_msa( uint8_t *p_src, int32_t i_src_stride,
         TRANSPOSE8X4_SH_SH( diff0, diff1, diff2, diff3,
                             diff0, diff2, diff4, diff6 );
 
-        diff1 = ( v8i16 ) __msa_splati_d( ( v2i64 ) diff0, 1 );
-        diff3 = ( v8i16 ) __msa_splati_d( ( v2i64 ) diff2, 1 );
-        diff5 = ( v8i16 ) __msa_splati_d( ( v2i64 ) diff4, 1 );
-        diff7 = ( v8i16 ) __msa_splati_d( ( v2i64 ) diff6, 1 );
-
-        BUTTERFLY_4( diff0, diff2, diff3, diff1, temp0, temp2, temp3, temp1 );
+        diff1 = ( v8i16 ) __msa_ilvr_d( ( v2i64 ) diff4,  ( v2i64 ) diff0 );
+        diff3 = ( v8i16 ) __msa_ilvl_d( ( v2i64 ) diff4,  ( v2i64 ) diff0 );
+        diff5 = ( v8i16 ) __msa_ilvr_d( ( v2i64 ) diff6,  ( v2i64 ) diff2 );
+        diff7 = ( v8i16 ) __msa_ilvl_d( ( v2i64 ) diff6,  ( v2i64 ) diff2 );
+        BUTTERFLY_4( diff1, diff5, diff7, diff3, temp0, temp2, temp3, temp1 );
         BUTTERFLY_4( temp0, temp1, temp3, temp2, diff0, diff1, diff3, diff2 );
-        BUTTERFLY_4( diff4, diff6, diff7, diff5, temp0, temp2, temp3, temp1 );
-        BUTTERFLY_4( temp0, temp1, temp3, temp2, diff4, diff5, diff7, diff6 );
-        TRANSPOSE4X8_SH_SH( diff0, diff1, diff2, diff3, diff4, diff5, diff6,
-                            diff7, diff0, diff1, diff2, diff3, diff4, diff5,
-                            diff6, diff7 );
+        TRANSPOSE8X4_SH_SH( diff0, diff1, diff2, diff3,
+                            diff4, diff5, diff6, diff7 );
+        diff0 = ( v8i16 ) __msa_ilvr_d( ( v2i64 ) diff6,  ( v2i64 ) diff4 );
+        diff1 = ( v8i16 ) __msa_ilvl_d( ( v2i64 ) diff6,  ( v2i64 ) diff4 );
+        diff2 = ( v8i16 ) __msa_ilvr_d( ( v2i64 ) diff7,  ( v2i64 ) diff5 );
+        diff3 = ( v8i16 ) __msa_ilvl_d( ( v2i64 ) diff7,  ( v2i64 ) diff5 );
+
         BUTTERFLY_4( diff0, diff2, diff3, diff1, temp0, temp2, temp3, temp1 );
         BUTTERFLY_4( temp0, temp1, temp3, temp2, diff0, diff1, diff3, diff2 );
 
-        diff0 = __msa_add_a_h( diff0, zero );
-        diff1 = __msa_add_a_h( diff1, zero );
-        diff2 = __msa_add_a_h( diff2, zero );
-        diff3 = __msa_add_a_h( diff3, zero );
-        diff0 = ( diff0 + diff1 + diff2 + diff3 );
-        u_sum += HADD_UH_U32( diff0 );
+        diff0 = __msa_add_a_h( diff0, diff1 );
+        diff2 = __msa_add_a_h( diff2, diff3 );
+        sum += ( diff0 + diff2 );
     }
 
+    u_sum = HADD_UH_U32( sum );
     return ( u_sum >> 1 );
 }
 
@@ -804,7 +802,6 @@ static int32_t sa8d_8x8_msa( uint8_t *p_src, int32_t i_src_stride,
     uint32_t u_sum = 0;
     v16i8 src0, src1, src2, src3, src4, src5, src6, src7;
     v16i8 ref0, ref1, ref2, ref3, ref4, ref5, ref6, ref7;
-    v8i16 zero = { 0 };
     v8i16 diff0, diff1, diff2, diff3, diff4, diff5, diff6, diff7;
     v8i16 sub0, sub1, sub2, sub3, sub4, sub5, sub6, sub7;
     v8i16 temp0, temp1, temp2, temp3;
@@ -837,15 +834,15 @@ static int32_t sa8d_8x8_msa( uint8_t *p_src, int32_t i_src_stride,
     temp2 = diff2 + diff6;
     temp3 = diff3 + diff7;
 
-    temp0 = __msa_add_a_h( temp0, zero );
-    temp1 = __msa_add_a_h( temp1, zero );
-    temp2 = __msa_add_a_h( temp2, zero );
-    temp3 = __msa_add_a_h( temp3, zero );
+    diff0 = __msa_asub_s_h( diff0, diff4 );
+    diff1 = __msa_asub_s_h( diff1, diff5 );
+    diff2 = __msa_asub_s_h( diff2, diff6 );
+    diff3 = __msa_asub_s_h( diff3, diff7 );
+    diff0 = __msa_add_a_h( diff0, temp0 );
+    diff1 = __msa_add_a_h( diff1, temp1 );
+    diff2 = __msa_add_a_h( diff2, temp2 );
+    diff3 = __msa_add_a_h( diff3, temp3 );
 
-    diff0 = temp0 + __msa_asub_s_h( diff0, diff4 );
-    diff1 = temp1 + __msa_asub_s_h( diff1, diff5 );
-    diff2 = temp2 + __msa_asub_s_h( diff2, diff6 );
-    diff3 = temp3 + __msa_asub_s_h( diff3, diff7 );
     diff0 = ( diff0 + diff1 + diff2 + diff3 );
 
     u_sum = HADD_UH_U32( diff0 );
@@ -895,19 +892,12 @@ static uint64_t pixel_hadamard_ac_8x8_msa( uint8_t *p_pix, int32_t i_stride )
     tmp2 = diff4[0];
     tmp3 = diff4[4];
 
-    sub0 = __msa_add_a_h( diff0, zero );
-    sub1 = __msa_add_a_h( diff1, zero );
-    sub2 = __msa_add_a_h( diff2, zero );
-    sub3 = __msa_add_a_h( diff3, zero );
-    sub4 = __msa_add_a_h( diff4, zero );
-    sub5 = __msa_add_a_h( diff5, zero );
-    sub6 = __msa_add_a_h( diff6, zero );
-    sub7 = __msa_add_a_h( diff7, zero );
-
-    sub0 = ( sub0 + sub1 + sub2 + sub3 );
-    sub1 = ( sub4 + sub5 + sub6 + sub7 );
-    sub0 += sub1;
+    sub0 = __msa_add_a_h( diff0, diff1 );
+    sub2 = __msa_add_a_h( diff2, diff3 );
+    sub4 = __msa_add_a_h( diff4, diff5 );
+    sub6 = __msa_add_a_h( diff6, diff7 );
 
+    sub0 = ( sub0 + sub2 + sub4 + sub6 );
     u_sum4 += HADD_UH_U32( sub0 );
 
     TRANSPOSE8x8_SH_SH( diff0, diff1, diff2, diff3, diff4, diff5, diff6, diff7,
@@ -926,19 +916,12 @@ static uint64_t pixel_hadamard_ac_8x8_msa( uint8_t *p_pix, int32_t i_stride )
     BUTTERFLY_4( diff4, diff6, diff7, diff5, temp0, temp2, temp3, temp1 );
     BUTTERFLY_4( temp0, temp1, temp3, temp2, diff4, diff5, diff7, diff6 );
 
-    sub0 = __msa_add_a_h( diff0, zero );
-    sub1 = __msa_add_a_h( diff1, zero );
-    sub2 = __msa_add_a_h( diff2, zero );
-    sub3 = __msa_add_a_h( diff3, zero );
-    sub4 = __msa_add_a_h( diff4, zero );
-    sub5 = __msa_add_a_h( diff5, zero );
-    sub6 = __msa_add_a_h( diff6, zero );
-    sub7 = __msa_add_a_h( diff7, zero );
-
-    sub0 = ( sub0 + sub1 + sub2 + sub3 );
-    sub1 = ( sub4 + sub5 + sub6 + sub7 );
-    sub0 += sub1;
+    sub0 = __msa_add_a_h( diff0, diff1 );
+    sub2 = __msa_add_a_h( diff2, diff3 );
+    sub4 = __msa_add_a_h( diff4, diff5 );
+    sub6 = __msa_add_a_h( diff6, diff7 );
 
+    sub0 = ( sub0 + sub2 + sub4 + sub6 );
     u_sum8 += HADD_UH_U32( sub0 );
 
     u_dc = ( uint16_t ) ( tmp0 + tmp1 + tmp2 + tmp3 );
@@ -1461,31 +1444,44 @@ uint64_t x264_pixel_var_8x8_msa( uint8_t *p_pix, intptr_t i_stride )
     return avc_pixel_var8width_msa( p_pix, i_stride, 8 );
 }
 
-int32_t x264_pixel_var2_8x16_msa( uint8_t *p_pix1, intptr_t i_stride1,
-                                  uint8_t *p_pix2, intptr_t i_stride2,
-                                  int32_t *p_ssd )
+int32_t x264_pixel_var2_8x16_msa( uint8_t *p_pix1, uint8_t *p_pix2,
+                                  int32_t ssd[2] )
 {
-    int32_t i_var = 0, i_diff = 0, i_sqr = 0;
+    int32_t i_var = 0, i_diff_u = 0, i_sqr_u = 0;
+    int32_t i_diff_v = 0, i_sqr_v = 0;
 
-    i_sqr = sse_diff_8width_msa( p_pix1, i_stride1, p_pix2, i_stride2, 16,
-                                 &i_diff );
-    i_var = VARIANCE_WxH( i_sqr, i_diff, 7 );
-    *p_ssd = i_sqr;
+    i_sqr_u = sse_diff_8width_msa( p_pix1, FENC_STRIDE,
+                                   p_pix2, FDEC_STRIDE, 16, &i_diff_u );
+    i_sqr_v = sse_diff_8width_msa( p_pix1 + (FENC_STRIDE >> 1),
+                                   FENC_STRIDE,
+                                   p_pix2 + (FDEC_STRIDE >> 1),
+                                   FDEC_STRIDE, 16, &i_diff_v );
+    i_var = VARIANCE_WxH( i_sqr_u, i_diff_u, 7 ) +
+            VARIANCE_WxH( i_sqr_v, i_diff_v, 7 );
+    ssd[0] = i_sqr_u;
+    ssd[1] = i_sqr_v;
 
     return i_var;
 }
 
-int32_t x264_pixel_var2_8x8_msa( uint8_t *p_pix1, intptr_t i_stride1,
-                                 uint8_t *p_pix2, intptr_t i_stride2,
-                                 int32_t *p_ssd )
+int32_t x264_pixel_var2_8x8_msa( uint8_t *p_pix1, uint8_t *p_pix2,
+                                 int32_t ssd[2] )
 {
-    int32_t i_var = 0, i_diff = 0, i_sqr = 0;
+    int32_t i_var = 0, i_diff_u = 0, i_sqr_u = 0;
+    int32_t i_diff_v = 0, i_sqr_v = 0;
 
-    i_sqr = sse_diff_8width_msa( p_pix1, i_stride1,
-                                 p_pix2, i_stride2, 8, &i_diff );
-    i_var = VARIANCE_WxH( i_sqr, i_diff, 6 );
-    *p_ssd = i_sqr;
+    i_sqr_u = sse_diff_8width_msa( p_pix1, FENC_STRIDE,
+                                   p_pix2, FDEC_STRIDE, 8, &i_diff_u );
+    i_sqr_v = sse_diff_8width_msa( p_pix1 + (FENC_STRIDE >> 1),
+                                   FENC_STRIDE,
+                                   p_pix2 + (FDEC_STRIDE >> 1),
+                                   FDEC_STRIDE, 8, &i_diff_v );
+    i_var = VARIANCE_WxH( i_sqr_u, i_diff_u, 6 ) +
+            VARIANCE_WxH( i_sqr_v, i_diff_v, 6 );
+    ssd[0] = i_sqr_u;
+    ssd[1] = i_sqr_v;
 
     return i_var;
 }
+
 #endif
diff --git a/common/mips/pixel.h b/common/mips/pixel.h
index 5e7c813..33987de 100644
--- a/common/mips/pixel.h
+++ b/common/mips/pixel.h
@@ -217,12 +217,10 @@ uint64_t x264_pixel_var_8x16_msa( uint8_t *p_pix, intptr_t i_stride );
 #define x264_pixel_var_8x8_msa x264_template(pixel_var_8x8_msa)
 uint64_t x264_pixel_var_8x8_msa( uint8_t *p_pix, intptr_t i_stride );
 #define x264_pixel_var2_8x16_msa x264_template(pixel_var2_8x16_msa)
-int32_t x264_pixel_var2_8x16_msa( uint8_t *p_pix1, intptr_t i_stride1,
-                                  uint8_t *p_pix2, intptr_t i_stride2,
-                                  int32_t *p_ssd );
+int32_t x264_pixel_var2_8x16_msa( uint8_t *p_pix1, uint8_t *p_pix2,
+                                  int32_t ssd[2] );
 #define x264_pixel_var2_8x8_msa x264_template(pixel_var2_8x8_msa)
-int32_t x264_pixel_var2_8x8_msa( uint8_t *p_pix1, intptr_t i_stride1,
-                                 uint8_t *p_pix2, intptr_t i_stride2,
-                                 int32_t *p_ssd );
+int32_t x264_pixel_var2_8x8_msa( uint8_t *p_pix1, uint8_t *p_pix2,
+                                 int32_t ssd[2] );
 
 #endif
diff --git a/common/pixel.c b/common/pixel.c
index 504848c..5d28b2d 100644
--- a/common/pixel.c
+++ b/common/pixel.c
@@ -45,7 +45,9 @@
 #if HAVE_MSA
 #   include "mips/pixel.h"
 #endif
-
+#if ARCH_LOONGARCH
+#   include "loongarch/pixel.h"
+#endif
 
 /****************************************************************************
  * pixel_sad_WxH
@@ -1502,13 +1504,43 @@ void x264_pixel_init( uint32_t cpu, x264_pixel_function_t *pixf )
         pixf->var[PIXEL_16x16] = x264_pixel_var_16x16_msa;
         pixf->var[PIXEL_8x16]  = x264_pixel_var_8x16_msa;
         pixf->var[PIXEL_8x8]   = x264_pixel_var_8x8_msa;
-      //pixf->var2[PIXEL_8x16]  = x264_pixel_var2_8x16_msa;
-      //pixf->var2[PIXEL_8x8]   = x264_pixel_var2_8x8_msa;
+        pixf->var2[PIXEL_8x16]  = x264_pixel_var2_8x16_msa;
+        pixf->var2[PIXEL_8x8]   = x264_pixel_var2_8x8_msa;
         pixf->sa8d[PIXEL_16x16] = x264_pixel_sa8d_16x16_msa;
         pixf->sa8d[PIXEL_8x8]   = x264_pixel_sa8d_8x8_msa;
     }
 #endif // HAVE_MSA
 
+#if HAVE_LASX
+    if( cpu&X264_CPU_LASX )
+    {
+        INIT8( sad, _lasx );
+        INIT8_NAME( sad_aligned, sad, _lasx );
+        INIT8( ssd, _lasx );
+        INIT7( sad_x3, _lasx );
+        INIT7( sad_x4, _lasx );
+        INIT8( satd, _lasx );
+        INIT4( hadamard_ac, _lasx );
+
+        pixf->intra_sad_x3_4x4   = x264_intra_sad_x3_4x4_lasx;
+        pixf->intra_sad_x3_8x8   = x264_intra_sad_x3_8x8_lasx;
+        pixf->intra_sad_x3_8x8c  = x264_intra_sad_x3_8x8c_lasx;
+        pixf->intra_sad_x3_16x16 = x264_intra_sad_x3_16x16_lasx;
+        pixf->intra_satd_x3_4x4   = x264_intra_satd_x3_4x4_lasx;
+        pixf->intra_satd_x3_16x16 = x264_intra_satd_x3_16x16_lasx;
+        pixf->intra_satd_x3_8x8c  = x264_intra_satd_x3_8x8c_lasx;
+        pixf->intra_sa8d_x3_8x8   = x264_intra_sa8d_x3_8x8_lasx;
+
+        pixf->var[PIXEL_16x16] = x264_pixel_var_16x16_lasx;
+        pixf->var[PIXEL_8x16]  = x264_pixel_var_8x16_lasx;
+        pixf->var[PIXEL_8x8]   = x264_pixel_var_8x8_lasx;
+        pixf->var2[PIXEL_8x16]  = x264_pixel_var2_8x16_lasx;
+        pixf->var2[PIXEL_8x8]   = x264_pixel_var2_8x8_lasx;
+        pixf->sa8d[PIXEL_16x16] = x264_pixel_sa8d_16x16_lasx;
+        pixf->sa8d[PIXEL_8x8]   = x264_pixel_sa8d_8x8_lasx;
+    }
+#endif // HAVE_LASX
+
 #endif // HIGH_BIT_DEPTH
 #if HAVE_ALTIVEC
     if( cpu&X264_CPU_ALTIVEC )
diff --git a/common/predict.c b/common/predict.c
index ce70960..95ff56c 100644
--- a/common/predict.c
+++ b/common/predict.c
@@ -46,7 +46,9 @@
 #if HAVE_MSA
 #   include "mips/predict.h"
 #endif
-
+#if ARCH_LOONGARCH
+#   include "loongarch/predict.h"
+#endif
 /****************************************************************************
  * 16x16 prediction for intra luma block
  ****************************************************************************/
@@ -924,6 +926,10 @@ void x264_predict_16x16_init( uint32_t cpu, x264_predict_t pf[7] )
     }
 #endif
 #endif
+
+#if ARCH_LOONGARCH64
+    x264_predict_16x16_init_lasx( cpu, pf );
+#endif
 }
 
 void x264_predict_8x8c_init( uint32_t cpu, x264_predict_t pf[7] )
diff --git a/common/quant.c b/common/quant.c
index 8017706..cd911f0 100644
--- a/common/quant.c
+++ b/common/quant.c
@@ -43,6 +43,9 @@
 #if HAVE_MSA
 #   include "mips/quant.h"
 #endif
+#if ARCH_LOONGARCH
+#   include "loongarch/quant.h"
+#endif
 
 #define QUANT_ONE( coef, mf, f ) \
 { \
@@ -805,6 +808,18 @@ void x264_quant_init( x264_t *h, uint32_t cpu, x264_quant_function_t *pf )
         pf->coeff_last[DCT_LUMA_8x8] = x264_coeff_last64_msa;
     }
 #endif
+
+#if HAVE_LASX
+    if( cpu&X264_CPU_LASX )
+    {
+        pf->quant_4x4      = x264_quant_4x4_lasx;
+        pf->quant_4x4x4    = x264_quant_4x4x4_lasx;
+        pf->quant_8x8      = x264_quant_8x8_lasx;
+        pf->coeff_last[ DCT_LUMA_AC] = x264_coeff_last15_lasx;
+        pf->coeff_last[DCT_LUMA_4x4] = x264_coeff_last16_lasx;
+        pf->coeff_last[DCT_LUMA_8x8] = x264_coeff_last64_lasx;
+    }
+#endif
 #endif // HIGH_BIT_DEPTH
     pf->coeff_last[DCT_LUMA_DC]     = pf->coeff_last[DCT_CHROMAU_DC]  = pf->coeff_last[DCT_CHROMAV_DC] =
     pf->coeff_last[DCT_CHROMAU_4x4] = pf->coeff_last[DCT_CHROMAV_4x4] = pf->coeff_last[DCT_LUMA_4x4];
diff --git a/config.guess b/config.guess
index f50dcdb..015efd1 100755
--- a/config.guess
+++ b/config.guess
@@ -964,6 +964,9 @@ EOF
     k1om:Linux:*:*)
 	echo "$UNAME_MACHINE"-unknown-linux-"$LIBC"
 	exit ;;
+    loongarch32:Linux:*:* | loongarch64:Linux:*:*)
+	echo "$UNAME_MACHINE"-unknown-linux-"$LIBC"
+	exit ;;
     m32r*:Linux:*:*)
 	echo "$UNAME_MACHINE"-unknown-linux-"$LIBC"
 	exit ;;
diff --git a/config.sub b/config.sub
index 1d8e98b..682f597 100755
--- a/config.sub
+++ b/config.sub
@@ -148,7 +148,7 @@ case $os in
 	-convergent* | -ncr* | -news | -32* | -3600* | -3100* | -hitachi* |\
 	-c[123]* | -convex* | -sun | -crds | -omron* | -dg | -ultra | -tti* | \
 	-harris | -dolphin | -highlevel | -gould | -cbm | -ns | -masscomp | \
-	-apple | -axis | -knuth | -cray | -microblaze*)
+	-apple | -axis | -knuth | -cray | -microblaze* | loongson)
 		os=
 		basic_machine=$1
 		;;
@@ -292,6 +292,7 @@ case $basic_machine in
 	| moxie \
 	| mt \
 	| msp430 \
+	| loongarch32 | loongarch64 \
 	| nds32 | nds32le | nds32be \
 	| nios | nios2 | nios2eb | nios2el \
 	| ns16k | ns32k \
@@ -389,6 +390,7 @@ case $basic_machine in
 	| ip2k-* | iq2000-* \
 	| k1om-* \
 	| le32-* | le64-* \
+	| loongarch32-* | loongarch64-* \
 	| lm32-* \
 	| m32c-* | m32r-* | m32rle-* \
 	| m68000-* | m680[012346]0-* | m68360-* | m683?2-* | m68k-* \
diff --git a/configure b/configure
index d6c45a6..e310812 100755
--- a/configure
+++ b/configure
@@ -405,7 +405,7 @@ NL="
 # list of all preprocessor HAVE values we can define
 CONFIG_HAVE="MALLOC_H ALTIVEC ALTIVEC_H MMX ARMV6 ARMV6T2 NEON AARCH64 BEOSTHREAD POSIXTHREAD WIN32THREAD THREAD LOG2F SWSCALE \
              LAVF FFMS GPAC AVS GPL VECTOREXT INTERLACED CPU_COUNT OPENCL THP LSMASH X86_INLINE_ASM AS_FUNC INTEL_DISPATCHER \
-             MSA MMAP WINRT VSX ARM_INLINE_ASM STRTOK_R CLOCK_GETTIME BITDEPTH8 BITDEPTH10"
+             MSA LSX LASX MMAP WINRT VSX ARM_INLINE_ASM STRTOK_R CLOCK_GETTIME BITDEPTH8 BITDEPTH10"
 
 # parse options
 
@@ -807,6 +807,12 @@ case $host_cpu in
         AS="${AS-${CC}}"
         AS_EXT=".c"
         ;;
+    loongarch*)
+        ARCH="LOONGARCH"
+        AS="${AS-${CC}}"
+        ASFLAGS="$ASFLAGS -c"
+        AS_EXT=".c"
+        ;;
     arm*)
         ARCH="ARM"
         if [ "$SYS" = MACOSX ] ; then
@@ -901,7 +907,7 @@ if [ $compiler_style = GNU ]; then
     fi
 fi
 
-if [ $shared = yes -a \( $ARCH = "X86_64" -o $ARCH = "PPC" -o $ARCH = "ALPHA" -o $ARCH = "ARM" -o $ARCH = "IA64" -o $ARCH = "PARISC" -o $ARCH = "MIPS" -o $ARCH = "AARCH64" \) ] ; then
+if [ $shared = yes -a \( $ARCH = "X86_64" -o $ARCH = "PPC" -o $ARCH = "ALPHA" -o $ARCH = "ARM" -o $ARCH = "IA64" -o $ARCH = "PARISC" -o $ARCH = "MIPS" -o $ARCH = "AARCH64" -o $ARCH = "LOONGARCH" \) ] ; then
     pic="yes"
 fi
 
@@ -990,17 +996,21 @@ if [ $asm = auto -a \( $ARCH = ARM -o $ARCH = AARCH64 \) ] ; then
     as_check ".func test${NL}.endfunc" && define HAVE_AS_FUNC 1
 fi
 
-if [ $asm = auto -a $ARCH = MIPS ] ; then
-    if ! echo $CFLAGS | grep -Eq '(-march|-mmsa|-mno-msa)' ; then
-        cc_check '' '-mmsa -mfp64 -mhard-float' && CFLAGS="-mmsa -mfp64 -mhard-float $CFLAGS"
+if [ $asm = auto -a $ARCH = LOONGARCH ] ; then
+    if cc_check '' '-mlsx' '__asm__("vadd.b $vr0, $vr1, $vr2");' ; then
+        LSX_CFLAGS="-mlsx"
+        define HAVE_LSX
+    fi
+    if cc_check '' '-mlasx' '__asm__("xvadd.b $xr0, $xr1, $xr2");' ; then
+        LASX_CFLAGS="-mlasx"
+        define HAVE_LASX
     fi
+fi
 
-    if cc_check '' '' '__asm__("addvi.b $w0, $w1, 1");' ; then
+if [ $asm = auto -a $ARCH = MIPS ] ; then
+    if cc_check 'msa.h' '-mmsa -mfp64 -mhard-float' '__asm__("addvi.b $w0, $w1, 1");' ; then
+        MSA_CFLAGS="-mmsa -mfp64 -mhard-float"
         define HAVE_MSA
-    else
-        echo "You specified a pre-MSA CPU in your CFLAGS."
-        echo "If you really want to run on such a CPU, configure with --disable-asm."
-        exit 1
     fi
 fi
 
@@ -1490,6 +1500,9 @@ SYS_ARCH=$ARCH
 SYS=$SYS
 CC=$CC
 CFLAGS=$CFLAGS
+MSA_CFLAGS=$MSA_CFLAGS
+LSX_CFLAGS=$LSX_CFLAGS
+LASX_CFLAGS=$LASX_CFLAGS
 CFLAGSSO=$CFLAGSSO
 CFLAGSCLI=$CFLAGSCLI
 COMPILER=$compiler
@@ -1612,7 +1625,7 @@ cat conftest.log >> config.log
 cat conftest.log
 
 [ "$SRCPATH" != "." ] && ln -sf ${SRCPATH}/Makefile ./Makefile
-mkdir -p common/{aarch64,arm,mips,ppc,x86} encoder extras filters/video input output tools
+mkdir -p common/{aarch64,arm,mips,ppc,x86,loongarch} encoder extras filters/video input output tools
 
 echo
 echo "You can run 'make' or 'make fprofiled' now."
diff --git a/tools/checkasm.c b/tools/checkasm.c
index 0361e92..90a5982 100644
--- a/tools/checkasm.c
+++ b/tools/checkasm.c
@@ -106,6 +106,9 @@ static inline uint32_t read_time(void)
     a = b;
 #elif ARCH_MIPS
     asm volatile( "rdhwr %0, $2" : "=r"(a) :: "memory" );
+#elif ARCH_LOONGARCH
+    uint32_t id = 0;
+    asm volatile ( "rdtimel.w  %0, %1" : "=r"(a), "=r"(id) :: "memory" );
 #endif
     return a;
 }
@@ -204,6 +207,8 @@ static void print_bench(void)
                     b->cpu&X264_CPU_ARMV8 ? "armv8" :
 #elif ARCH_MIPS
                     b->cpu&X264_CPU_MSA ? "msa" :
+#elif ARCH_LOONGARCH
+                    b->cpu&X264_CPU_LASX ? "lasx" :
 #endif
                     "c",
 #if ARCH_X86 || ARCH_X86_64
@@ -2910,6 +2915,9 @@ static int check_all_flags( void )
 #elif ARCH_MIPS
     if( cpu_detect & X264_CPU_MSA )
         ret |= add_flags( &cpu0, &cpu1, X264_CPU_MSA, "MSA" );
+#elif ARCH_LOONGARCH
+    if( cpu_detect & X264_CPU_LASX )
+        ret |= add_flags( &cpu0, &cpu1, X264_CPU_LASX, "LASX" );
 #endif
     return ret;
 }
@@ -2923,7 +2931,7 @@ REALIGN_STACK int main( int argc, char **argv )
 
     if( argc > 1 && !strncmp( argv[1], "--bench", 7 ) )
     {
-#if !ARCH_X86 && !ARCH_X86_64 && !ARCH_PPC && !ARCH_ARM && !ARCH_AARCH64 && !ARCH_MIPS
+#if !ARCH_X86 && !ARCH_X86_64 && !ARCH_PPC && !ARCH_ARM && !ARCH_AARCH64 && !ARCH_MIPS && !ARCH_LOONGARCH
         fprintf( stderr, "no --bench for your cpu until you port rdtsc\n" );
         return 1;
 #endif
diff --git a/x264.h b/x264.h
index 3a50f1c..bde98d9 100644
--- a/x264.h
+++ b/x264.h
@@ -181,6 +181,10 @@ typedef struct x264_nal_t
 /* MIPS */
 #define X264_CPU_MSA             0x0000001U  /* MIPS MSA */
 
+/* LOONGARCH */
+#define X264_CPU_LSX             0x0000001  /* LOONGARCH LSX */
+#define X264_CPU_LASX            0x0000002  /* LOONGARCH LASX */
+
 /* Analyse flags */
 #define X264_ANALYSE_I4x4       0x0001U  /* Analyse i4x4 */
 #define X264_ANALYSE_I8x8       0x0002U  /* Analyse i8x8 (requires 8x8 transform) */
-- 
2.20.1

